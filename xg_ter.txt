In [27]: run final_xg.py
load data...
done

check df2 NaN...
False

select PAY as df3, delete PAY as df4...
done

add PAY column as df4...
done

delete ID...
done

one hot encodding 'SEX','EDUCATION','MARRIAGE' as X, and default as Y...
done

describe X Y...
            LIMIT_BAL           AGE      BILL_AMT1      BILL_AMT2  \
count    30000.000000  30000.000000   30000.000000   30000.000000   
mean    167484.322667     35.485500   51223.330900   49179.075167   
std     129747.661567      9.217904   73635.860576   71173.768783   
min      10000.000000     21.000000 -165580.000000  -69777.000000   
25%      50000.000000     28.000000    3558.750000    2984.750000   
50%     140000.000000     34.000000   22381.500000   21200.000000   
75%     240000.000000     41.000000   67091.000000   64006.250000   
max    1000000.000000     79.000000  964511.000000  983931.000000   

          BILL_AMT3      BILL_AMT4      BILL_AMT5      BILL_AMT6  \
count  3.000000e+04   30000.000000   30000.000000   30000.000000   
mean   4.701315e+04   43262.948967   40311.400967   38871.760400   
std    6.934939e+04   64332.856134   60797.155770   59554.107537   
min   -1.572640e+05 -170000.000000  -81334.000000 -339603.000000   
25%    2.666250e+03    2326.750000    1763.000000    1256.000000   
50%    2.008850e+04   19052.000000   18104.500000   17071.000000   
75%    6.016475e+04   54506.000000   50190.500000   49198.250000   
max    1.664089e+06  891586.000000  927171.000000  961664.000000   

            PAY_AMT1      PAY_AMT2      ...        EDUCATION_1   EDUCATION_2  \
count   30000.000000  3.000000e+04      ...       30000.000000  30000.000000   
mean     5663.580500  5.921163e+03      ...           0.352833      0.467667   
std     16563.280354  2.304087e+04      ...           0.477859      0.498962   
min         0.000000  0.000000e+00      ...           0.000000      0.000000   
25%      1000.000000  8.330000e+02      ...           0.000000      0.000000   
50%      2100.000000  2.009000e+03      ...           0.000000      0.000000   
75%      5006.000000  5.000000e+03      ...           1.000000      1.000000   
max    873552.000000  1.684259e+06      ...           1.000000      1.000000   

        EDUCATION_3   EDUCATION_4   EDUCATION_5   EDUCATION_6    MARRIAGE_0  \
count  30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   
mean       0.163900      0.004100      0.009333      0.001700      0.001800   
std        0.370191      0.063901      0.096159      0.041197      0.042389   
min        0.000000      0.000000      0.000000      0.000000      0.000000   
25%        0.000000      0.000000      0.000000      0.000000      0.000000   
50%        0.000000      0.000000      0.000000      0.000000      0.000000   
75%        0.000000      0.000000      0.000000      0.000000      0.000000   
max        1.000000      1.000000      1.000000      1.000000      1.000000   

         MARRIAGE_1    MARRIAGE_2    MARRIAGE_3  
count  30000.000000  30000.000000  30000.000000  
mean       0.455300      0.532133      0.010767  
std        0.498006      0.498975      0.103204  
min        0.000000      0.000000      0.000000  
25%        0.000000      0.000000      0.000000  
50%        0.000000      1.000000      0.000000  
75%        1.000000      1.000000      0.000000  
max        1.000000      1.000000      1.000000  

[8 rows x 57 columns]
count     30000
unique        2
top           0
freq      23364
Name: default.payment.next.month, dtype: int64
done

splitting training and testing...
done

[0]	train-auc:0.522446+0.00849511	test-auc:0.494676+0.00573482
[1]	train-auc:0.53769+0.00659504	test-auc:0.488101+0.00284092
[2]	train-auc:0.55537+0.0047635	test-auc:0.487425+0.00560346
[3]	train-auc:0.566099+0.00825657	test-auc:0.487692+0.00310338
[4]	train-auc:0.575048+0.0106705	test-auc:0.490019+0.00293376
[5]	train-auc:0.581598+0.0100076	test-auc:0.491505+0.00392232
[6]	train-auc:0.587811+0.00743735	test-auc:0.490053+0.00478925
[7]	train-auc:0.594298+0.00508188	test-auc:0.490613+0.00622496
[8]	train-auc:0.598821+0.00715926	test-auc:0.491214+0.00741729
[9]	train-auc:0.605408+0.00559223	test-auc:0.492835+0.00860558
[10]	train-auc:0.6106+0.00596824	test-auc:0.491369+0.00761779
[11]	train-auc:0.61379+0.00563856	test-auc:0.492081+0.00747363
[12]	train-auc:0.617927+0.00647027	test-auc:0.491056+0.00784089
[13]	train-auc:0.621304+0.0075645	test-auc:0.490703+0.00801689
[14]	train-auc:0.626612+0.00783394	test-auc:0.491543+0.00848554
[15]	train-auc:0.630302+0.00739231	test-auc:0.491657+0.00863632
[16]	train-auc:0.635558+0.00488999	test-auc:0.492557+0.00944608
[17]	train-auc:0.638465+0.00405563	test-auc:0.493815+0.00904261
[18]	train-auc:0.641526+0.00479708	test-auc:0.49435+0.00801451
[19]	train-auc:0.644228+0.00497994	test-auc:0.494196+0.00803613
[20]	train-auc:0.648614+0.00466135	test-auc:0.494963+0.008093
[21]	train-auc:0.651317+0.0044687	test-auc:0.495091+0.00781075
[22]	train-auc:0.654241+0.00453781	test-auc:0.495527+0.00672216
[23]	train-auc:0.656928+0.00462329	test-auc:0.496021+0.006095
[24]	train-auc:0.659708+0.00502737	test-auc:0.495909+0.00688206
[25]	train-auc:0.662117+0.0049788	test-auc:0.496606+0.00650018
[26]	train-auc:0.663118+0.00510971	test-auc:0.496875+0.00704354
[27]	train-auc:0.66438+0.00523704	test-auc:0.497136+0.00623225
[28]	train-auc:0.667977+0.00585308	test-auc:0.497072+0.00567927
[29]	train-auc:0.670506+0.00641972	test-auc:0.496928+0.00602028
[30]	train-auc:0.673858+0.00657046	test-auc:0.495951+0.00521706
[31]	train-auc:0.675594+0.00587214	test-auc:0.495808+0.0051002
[32]	train-auc:0.677719+0.00531752	test-auc:0.495352+0.00518583
[33]	train-auc:0.68011+0.00502855	test-auc:0.494792+0.00536782
[34]	train-auc:0.682232+0.0043149	test-auc:0.494741+0.00502726
[35]	train-auc:0.683976+0.00526326	test-auc:0.495177+0.00543955
[36]	train-auc:0.685925+0.00567337	test-auc:0.494166+0.00465179
[37]	train-auc:0.687319+0.00561614	test-auc:0.493597+0.00473097
[38]	train-auc:0.689211+0.00573037	test-auc:0.493503+0.00467753
[39]	train-auc:0.691866+0.00576247	test-auc:0.493019+0.00492327
[40]	train-auc:0.693807+0.00511174	test-auc:0.492682+0.00465432
[41]	train-auc:0.695048+0.00581027	test-auc:0.492428+0.0042427
[42]	train-auc:0.697561+0.00562154	test-auc:0.492927+0.00503531
[43]	train-auc:0.699236+0.00569142	test-auc:0.493078+0.00452667
[44]	train-auc:0.700376+0.00556087	test-auc:0.492913+0.00487403
[45]	train-auc:0.702418+0.00546812	test-auc:0.492435+0.00465905
[46]	train-auc:0.703544+0.00540718	test-auc:0.492324+0.00499032
[47]	train-auc:0.705918+0.00527224	test-auc:0.492673+0.00500221
[48]	train-auc:0.707389+0.00586724	test-auc:0.492217+0.00534171
[49]	train-auc:0.708834+0.00649171	test-auc:0.491986+0.00468332
[50]	train-auc:0.710636+0.00607502	test-auc:0.491368+0.00439065
[51]	train-auc:0.71249+0.00574487	test-auc:0.491717+0.00449807
[52]	train-auc:0.714282+0.0056891	test-auc:0.491453+0.00437593
[53]	train-auc:0.715459+0.00624451	test-auc:0.491217+0.00427975
[54]	train-auc:0.716945+0.00623475	test-auc:0.491262+0.0045195
[55]	train-auc:0.71807+0.00656707	test-auc:0.490887+0.0044677
[56]	train-auc:0.720418+0.00678438	test-auc:0.491376+0.00535104
[57]	train-auc:0.721235+0.00666299	test-auc:0.491859+0.00525466
[58]	train-auc:0.723179+0.00678935	test-auc:0.491049+0.00665547
[59]	train-auc:0.725497+0.00669993	test-auc:0.491329+0.00632672
[60]	train-auc:0.727157+0.00626086	test-auc:0.491316+0.00694196
[61]	train-auc:0.729283+0.00656844	test-auc:0.491259+0.00677229
[62]	train-auc:0.731017+0.00616279	test-auc:0.491164+0.00648148
[63]	train-auc:0.731726+0.00591721	test-auc:0.49126+0.00647587
[64]	train-auc:0.732853+0.00576784	test-auc:0.490864+0.00680236
[65]	train-auc:0.734314+0.0056304	test-auc:0.491038+0.00672678
[66]	train-auc:0.735419+0.00548661	test-auc:0.491171+0.00673437
[67]	train-auc:0.737111+0.00483592	test-auc:0.491408+0.00651896
[68]	train-auc:0.738536+0.0049512	test-auc:0.491468+0.00624023
[69]	train-auc:0.740047+0.0046025	test-auc:0.491195+0.00633619
[70]	train-auc:0.741367+0.00481106	test-auc:0.491116+0.00651038
[71]	train-auc:0.74346+0.00482855	test-auc:0.490942+0.00638648
[72]	train-auc:0.744526+0.00448828	test-auc:0.491169+0.00639687
[73]	train-auc:0.746169+0.00426992	test-auc:0.490626+0.00584153
[74]	train-auc:0.747598+0.00465056	test-auc:0.490718+0.00596286
[75]	train-auc:0.74891+0.00444725	test-auc:0.490913+0.00611517
[76]	train-auc:0.750225+0.00433762	test-auc:0.490663+0.00551597
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/Desktop/風管碩一/bigdata/bigdata-final/final_xg.py in <module>()
    142  scale_pos_weight=1,
    143  seed=27)
--> 144 modelfit(xgb1, X_train, y_train, predictors)
    145 
    146 '''

~/Desktop/風管碩一/bigdata/bigdata-final/final_xg.py in modelfit(alg, X_train, y_train, predictors, useTrainCV, cv_folds, early_stopping_rounds)
    114 
    115     #Fit the algorithm on the data
--> 116     alg.fit(X_train, y_train,eval_metric='auc')
    117 
    118     #Predict training set:

~/xgboost/python-package/xgboost/sklearn.py in fit(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model)
    476                 xgb_options.update({"eval_metric": eval_metric})
    477 
--> 478         self._le = XGBLabelEncoder().fit(y)
    479         training_labels = self._le.transform(y)
    480 

~/miniconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py in fit(self, y)
     93         self : returns an instance of self.
     94         """
---> 95         y = column_or_1d(y, warn=True)
     96         self.classes_ = np.unique(y)
     97         return self

~/miniconda3/lib/python3.6/site-packages/sklearn/utils/validation.py in column_or_1d(y, warn)
    612         return np.ravel(y)
    613 
--> 614     raise ValueError("bad input shape {0}".format(shape))
    615 
    616 

ValueError: bad input shape (24300, 2)

In [28]: np.ravel(y_train)
Out[28]: array([ 1.,  0.,  0., ...,  0.,  1.,  0.])

In [29]: np.ravel(y_train)
Out[29]: array([ 1.,  0.,  0., ...,  0.,  1.,  0.])

In [30]: run final_xg.py
load data...
done

check df2 NaN...
False

select PAY as df3, delete PAY as df4...
done

add PAY column as df4...
done

delete ID...
done

one hot encodding 'SEX','EDUCATION','MARRIAGE' as X, and default as Y...
done

describe X Y...
            LIMIT_BAL           AGE      BILL_AMT1      BILL_AMT2  \
count    30000.000000  30000.000000   30000.000000   30000.000000   
mean    167484.322667     35.485500   51223.330900   49179.075167   
std     129747.661567      9.217904   73635.860576   71173.768783   
min      10000.000000     21.000000 -165580.000000  -69777.000000   
25%      50000.000000     28.000000    3558.750000    2984.750000   
50%     140000.000000     34.000000   22381.500000   21200.000000   
75%     240000.000000     41.000000   67091.000000   64006.250000   
max    1000000.000000     79.000000  964511.000000  983931.000000   

          BILL_AMT3      BILL_AMT4      BILL_AMT5      BILL_AMT6  \
count  3.000000e+04   30000.000000   30000.000000   30000.000000   
mean   4.701315e+04   43262.948967   40311.400967   38871.760400   
std    6.934939e+04   64332.856134   60797.155770   59554.107537   
min   -1.572640e+05 -170000.000000  -81334.000000 -339603.000000   
25%    2.666250e+03    2326.750000    1763.000000    1256.000000   
50%    2.008850e+04   19052.000000   18104.500000   17071.000000   
75%    6.016475e+04   54506.000000   50190.500000   49198.250000   
max    1.664089e+06  891586.000000  927171.000000  961664.000000   

            PAY_AMT1      PAY_AMT2      ...        EDUCATION_1   EDUCATION_2  \
count   30000.000000  3.000000e+04      ...       30000.000000  30000.000000   
mean     5663.580500  5.921163e+03      ...           0.352833      0.467667   
std     16563.280354  2.304087e+04      ...           0.477859      0.498962   
min         0.000000  0.000000e+00      ...           0.000000      0.000000   
25%      1000.000000  8.330000e+02      ...           0.000000      0.000000   
50%      2100.000000  2.009000e+03      ...           0.000000      0.000000   
75%      5006.000000  5.000000e+03      ...           1.000000      1.000000   
max    873552.000000  1.684259e+06      ...           1.000000      1.000000   

        EDUCATION_3   EDUCATION_4   EDUCATION_5   EDUCATION_6    MARRIAGE_0  \
count  30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   
mean       0.163900      0.004100      0.009333      0.001700      0.001800   
std        0.370191      0.063901      0.096159      0.041197      0.042389   
min        0.000000      0.000000      0.000000      0.000000      0.000000   
25%        0.000000      0.000000      0.000000      0.000000      0.000000   
50%        0.000000      0.000000      0.000000      0.000000      0.000000   
75%        0.000000      0.000000      0.000000      0.000000      0.000000   
max        1.000000      1.000000      1.000000      1.000000      1.000000   

         MARRIAGE_1    MARRIAGE_2    MARRIAGE_3  
count  30000.000000  30000.000000  30000.000000  
mean       0.455300      0.532133      0.010767  
std        0.498006      0.498975      0.103204  
min        0.000000      0.000000      0.000000  
25%        0.000000      0.000000      0.000000  
50%        0.000000      1.000000      0.000000  
75%        1.000000      1.000000      0.000000  
max        1.000000      1.000000      1.000000  

[8 rows x 57 columns]
count     30000
unique        2
top           0
freq      23364
Name: default.payment.next.month, dtype: int64
done

splitting training and testing...
done

---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
~/Desktop/風管碩一/bigdata/bigdata-final/final_xg.py in <module>()
    139  scale_pos_weight=1,
    140  seed=27)
--> 141 modelfit(xgb1, X_train, y_train, predictors)
    142 
    143 '''

NameError: name 'y_train' is not defined

In [31]: run final_xg.py
load data...
done

check df2 NaN...
False

select PAY as df3, delete PAY as df4...
done

add PAY column as df4...
done

delete ID...
done

one hot encodding 'SEX','EDUCATION','MARRIAGE' as X, and default as Y...
done

describe X Y...
            LIMIT_BAL           AGE      BILL_AMT1      BILL_AMT2  \
count    30000.000000  30000.000000   30000.000000   30000.000000   
mean    167484.322667     35.485500   51223.330900   49179.075167   
std     129747.661567      9.217904   73635.860576   71173.768783   
min      10000.000000     21.000000 -165580.000000  -69777.000000   
25%      50000.000000     28.000000    3558.750000    2984.750000   
50%     140000.000000     34.000000   22381.500000   21200.000000   
75%     240000.000000     41.000000   67091.000000   64006.250000   
max    1000000.000000     79.000000  964511.000000  983931.000000   

          BILL_AMT3      BILL_AMT4      BILL_AMT5      BILL_AMT6  \
count  3.000000e+04   30000.000000   30000.000000   30000.000000   
mean   4.701315e+04   43262.948967   40311.400967   38871.760400   
std    6.934939e+04   64332.856134   60797.155770   59554.107537   
min   -1.572640e+05 -170000.000000  -81334.000000 -339603.000000   
25%    2.666250e+03    2326.750000    1763.000000    1256.000000   
50%    2.008850e+04   19052.000000   18104.500000   17071.000000   
75%    6.016475e+04   54506.000000   50190.500000   49198.250000   
max    1.664089e+06  891586.000000  927171.000000  961664.000000   

            PAY_AMT1      PAY_AMT2      ...        EDUCATION_1   EDUCATION_2  \
count   30000.000000  3.000000e+04      ...       30000.000000  30000.000000   
mean     5663.580500  5.921163e+03      ...           0.352833      0.467667   
std     16563.280354  2.304087e+04      ...           0.477859      0.498962   
min         0.000000  0.000000e+00      ...           0.000000      0.000000   
25%      1000.000000  8.330000e+02      ...           0.000000      0.000000   
50%      2100.000000  2.009000e+03      ...           0.000000      0.000000   
75%      5006.000000  5.000000e+03      ...           1.000000      1.000000   
max    873552.000000  1.684259e+06      ...           1.000000      1.000000   

        EDUCATION_3   EDUCATION_4   EDUCATION_5   EDUCATION_6    MARRIAGE_0  \
count  30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   
mean       0.163900      0.004100      0.009333      0.001700      0.001800   
std        0.370191      0.063901      0.096159      0.041197      0.042389   
min        0.000000      0.000000      0.000000      0.000000      0.000000   
25%        0.000000      0.000000      0.000000      0.000000      0.000000   
50%        0.000000      0.000000      0.000000      0.000000      0.000000   
75%        0.000000      0.000000      0.000000      0.000000      0.000000   
max        1.000000      1.000000      1.000000      1.000000      1.000000   

         MARRIAGE_1    MARRIAGE_2    MARRIAGE_3  
count  30000.000000  30000.000000  30000.000000  
mean       0.455300      0.532133      0.010767  
std        0.498006      0.498975      0.103204  
min        0.000000      0.000000      0.000000  
25%        0.000000      0.000000      0.000000  
50%        0.000000      1.000000      0.000000  
75%        1.000000      1.000000      0.000000  
max        1.000000      1.000000      1.000000  

[8 rows x 57 columns]
count     30000
unique        2
top           0
freq      23364
Name: default.payment.next.month, dtype: int64
done

splitting training and testing...
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
~/Desktop/風管碩一/bigdata/bigdata-final/final_xg.py in <module>()
     86 print("splitting training and testing...")
     87 X_train,X_test,y_train,y_test=train_test_split(X, Y, test_size=0.1, random_state=10)
---> 88 X_train,X_val, y_train, y_val=train_test_split(X_train, Y_train, test_size=0.1, random_state=10)
     89 print("done")
     90 print()

NameError: name 'Y_train' is not defined

In [32]: run final_xg.py
load data...
done

check df2 NaN...
False

select PAY as df3, delete PAY as df4...
done

add PAY column as df4...
done

delete ID...
done

one hot encodding 'SEX','EDUCATION','MARRIAGE' as X, and default as Y...
done

describe X Y...
            LIMIT_BAL           AGE      BILL_AMT1      BILL_AMT2  \
count    30000.000000  30000.000000   30000.000000   30000.000000   
mean    167484.322667     35.485500   51223.330900   49179.075167   
std     129747.661567      9.217904   73635.860576   71173.768783   
min      10000.000000     21.000000 -165580.000000  -69777.000000   
25%      50000.000000     28.000000    3558.750000    2984.750000   
50%     140000.000000     34.000000   22381.500000   21200.000000   
75%     240000.000000     41.000000   67091.000000   64006.250000   
max    1000000.000000     79.000000  964511.000000  983931.000000   

          BILL_AMT3      BILL_AMT4      BILL_AMT5      BILL_AMT6  \
count  3.000000e+04   30000.000000   30000.000000   30000.000000   
mean   4.701315e+04   43262.948967   40311.400967   38871.760400   
std    6.934939e+04   64332.856134   60797.155770   59554.107537   
min   -1.572640e+05 -170000.000000  -81334.000000 -339603.000000   
25%    2.666250e+03    2326.750000    1763.000000    1256.000000   
50%    2.008850e+04   19052.000000   18104.500000   17071.000000   
75%    6.016475e+04   54506.000000   50190.500000   49198.250000   
max    1.664089e+06  891586.000000  927171.000000  961664.000000   

            PAY_AMT1      PAY_AMT2      ...        EDUCATION_1   EDUCATION_2  \
count   30000.000000  3.000000e+04      ...       30000.000000  30000.000000   
mean     5663.580500  5.921163e+03      ...           0.352833      0.467667   
std     16563.280354  2.304087e+04      ...           0.477859      0.498962   
min         0.000000  0.000000e+00      ...           0.000000      0.000000   
25%      1000.000000  8.330000e+02      ...           0.000000      0.000000   
50%      2100.000000  2.009000e+03      ...           0.000000      0.000000   
75%      5006.000000  5.000000e+03      ...           1.000000      1.000000   
max    873552.000000  1.684259e+06      ...           1.000000      1.000000   

        EDUCATION_3   EDUCATION_4   EDUCATION_5   EDUCATION_6    MARRIAGE_0  \
count  30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   
mean       0.163900      0.004100      0.009333      0.001700      0.001800   
std        0.370191      0.063901      0.096159      0.041197      0.042389   
min        0.000000      0.000000      0.000000      0.000000      0.000000   
25%        0.000000      0.000000      0.000000      0.000000      0.000000   
50%        0.000000      0.000000      0.000000      0.000000      0.000000   
75%        0.000000      0.000000      0.000000      0.000000      0.000000   
max        1.000000      1.000000      1.000000      1.000000      1.000000   

         MARRIAGE_1    MARRIAGE_2    MARRIAGE_3  
count  30000.000000  30000.000000  30000.000000  
mean       0.455300      0.532133      0.010767  
std        0.498006      0.498975      0.103204  
min        0.000000      0.000000      0.000000  
25%        0.000000      0.000000      0.000000  
50%        0.000000      1.000000      0.000000  
75%        1.000000      1.000000      0.000000  
max        1.000000      1.000000      1.000000  

[8 rows x 57 columns]
count     30000
unique        2
top           0
freq      23364
Name: default.payment.next.month, dtype: int64
done

splitting training and testing...
done

[0]	train-auc:0.759334+0.00272849	test-auc:0.748887+0.00709675
[1]	train-auc:0.768543+0.00161094	test-auc:0.758754+0.00806896
[2]	train-auc:0.773863+0.0027218	test-auc:0.76483+0.00631158
[3]	train-auc:0.776407+0.0022595	test-auc:0.767858+0.00694358
[4]	train-auc:0.778873+0.00119062	test-auc:0.769252+0.00793906
[5]	train-auc:0.780094+0.0010522	test-auc:0.769669+0.00811983
[6]	train-auc:0.78207+0.00154336	test-auc:0.770748+0.00795232
[7]	train-auc:0.783774+0.000840055	test-auc:0.771693+0.00781643
[8]	train-auc:0.785674+0.00101097	test-auc:0.772556+0.00733826
[9]	train-auc:0.786778+0.00104759	test-auc:0.772952+0.00676074
[10]	train-auc:0.788159+0.000717564	test-auc:0.773763+0.00712959
[11]	train-auc:0.789136+0.000708357	test-auc:0.773945+0.0069569
[12]	train-auc:0.790415+0.000890692	test-auc:0.773721+0.00671831
[13]	train-auc:0.791696+0.000981611	test-auc:0.774363+0.00619504
[14]	train-auc:0.793058+0.000576965	test-auc:0.774704+0.0057819
[15]	train-auc:0.79443+0.000640972	test-auc:0.77456+0.00619703
[16]	train-auc:0.795562+0.000757791	test-auc:0.774737+0.00607317
[17]	train-auc:0.796581+0.000561769	test-auc:0.774704+0.00579782
[18]	train-auc:0.797695+0.000305415	test-auc:0.775076+0.00548019
[19]	train-auc:0.798559+0.000684969	test-auc:0.774993+0.00573201
[20]	train-auc:0.799764+0.000547606	test-auc:0.775357+0.00587411
[21]	train-auc:0.800764+0.000551152	test-auc:0.775642+0.00646811
[22]	train-auc:0.801707+0.00063416	test-auc:0.775553+0.00622372
[23]	train-auc:0.80263+0.000825049	test-auc:0.77606+0.00592172
[24]	train-auc:0.803287+0.000672984	test-auc:0.77647+0.00610413
[25]	train-auc:0.803951+0.000827618	test-auc:0.776655+0.00583223
[26]	train-auc:0.804875+0.000878628	test-auc:0.77674+0.00557334
[27]	train-auc:0.80581+0.000805191	test-auc:0.77694+0.00552208
[28]	train-auc:0.806901+0.00113037	test-auc:0.77702+0.00554712
[29]	train-auc:0.80772+0.000891984	test-auc:0.777165+0.00549023
[30]	train-auc:0.808562+0.000809134	test-auc:0.777095+0.00543939
[31]	train-auc:0.809492+0.000955624	test-auc:0.776982+0.00542944
[32]	train-auc:0.810788+0.000954595	test-auc:0.777333+0.0053896
[33]	train-auc:0.81145+0.000914203	test-auc:0.777307+0.00533338
[34]	train-auc:0.812323+0.00109478	test-auc:0.777526+0.00521875
[35]	train-auc:0.813143+0.000945588	test-auc:0.777515+0.00543174
[36]	train-auc:0.813876+0.00084127	test-auc:0.777502+0.00545537
[37]	train-auc:0.814842+0.00096977	test-auc:0.777582+0.00520186
[38]	train-auc:0.815783+0.00107162	test-auc:0.777555+0.00514065
[39]	train-auc:0.816463+0.000987733	test-auc:0.777688+0.00507977
[40]	train-auc:0.817589+0.000977627	test-auc:0.777865+0.0050121
[41]	train-auc:0.818493+0.000968393	test-auc:0.777908+0.00489943
[42]	train-auc:0.81944+0.000905872	test-auc:0.77778+0.00492366
[43]	train-auc:0.820127+0.000921584	test-auc:0.777814+0.00476715
[44]	train-auc:0.820769+0.00106005	test-auc:0.777979+0.004689
[45]	train-auc:0.821524+0.00125589	test-auc:0.778108+0.00449675
[46]	train-auc:0.822158+0.00142479	test-auc:0.7781+0.00433372
[47]	train-auc:0.82285+0.00159003	test-auc:0.778271+0.00420032
[48]	train-auc:0.823704+0.00164596	test-auc:0.778267+0.00415346
[49]	train-auc:0.824424+0.00153935	test-auc:0.778318+0.00392203
[50]	train-auc:0.825059+0.00138552	test-auc:0.778309+0.00388656
[51]	train-auc:0.825784+0.00116642	test-auc:0.778422+0.00385053
[52]	train-auc:0.826384+0.001152	test-auc:0.778391+0.00399591
[53]	train-auc:0.827047+0.00119159	test-auc:0.77851+0.00382205
[54]	train-auc:0.827855+0.00137683	test-auc:0.778357+0.0035334
[55]	train-auc:0.828375+0.0012669	test-auc:0.778424+0.00354557
[56]	train-auc:0.829068+0.00128158	test-auc:0.778634+0.00341993
[57]	train-auc:0.829889+0.00115551	test-auc:0.778576+0.003466
[58]	train-auc:0.830442+0.00106489	test-auc:0.778744+0.00335842
[59]	train-auc:0.831106+0.00116188	test-auc:0.778744+0.00336973
[60]	train-auc:0.831814+0.00114795	test-auc:0.778584+0.0034504
[61]	train-auc:0.832348+0.00122456	test-auc:0.778643+0.00339602
[62]	train-auc:0.833164+0.00145265	test-auc:0.778687+0.00337614
[63]	train-auc:0.833621+0.0015331	test-auc:0.778589+0.00338348
[64]	train-auc:0.83461+0.00156765	test-auc:0.778564+0.00341228
[65]	train-auc:0.835064+0.00164837	test-auc:0.778576+0.00349108
[66]	train-auc:0.835495+0.00182939	test-auc:0.778475+0.00354619
[67]	train-auc:0.836382+0.0019456	test-auc:0.778421+0.00349119
[68]	train-auc:0.836974+0.00186463	test-auc:0.778451+0.00359067
[69]	train-auc:0.837537+0.00177428	test-auc:0.778366+0.00335901
[70]	train-auc:0.838246+0.00169093	test-auc:0.778138+0.00329163
[71]	train-auc:0.838636+0.00174968	test-auc:0.778135+0.00335435
[72]	train-auc:0.839279+0.00148441	test-auc:0.778047+0.0033079
[73]	train-auc:0.840191+0.00167617	test-auc:0.777817+0.00323999
[74]	train-auc:0.840753+0.00188516	test-auc:0.777971+0.00323491
[75]	train-auc:0.841332+0.00214604	test-auc:0.777857+0.00325394
[76]	train-auc:0.842138+0.00207046	test-auc:0.777877+0.00326743
[77]	train-auc:0.842578+0.00214968	test-auc:0.777864+0.00327015
[78]	train-auc:0.843158+0.00206894	test-auc:0.777923+0.00338506
[79]	train-auc:0.843585+0.00208784	test-auc:0.777909+0.00332257
[80]	train-auc:0.844405+0.0020312	test-auc:0.777728+0.00351414
[81]	train-auc:0.844768+0.00216423	test-auc:0.777732+0.00348095
[82]	train-auc:0.845477+0.00233307	test-auc:0.777703+0.00330624
[83]	train-auc:0.846075+0.00210258	test-auc:0.777654+0.0032864
[84]	train-auc:0.846862+0.0020238	test-auc:0.777574+0.00316344
[85]	train-auc:0.847462+0.00212318	test-auc:0.77757+0.0029613
[86]	train-auc:0.847809+0.00222994	test-auc:0.777534+0.00311827
[87]	train-auc:0.848604+0.00237151	test-auc:0.777599+0.00310175
[88]	train-auc:0.849118+0.0020373	test-auc:0.777615+0.00312699
[89]	train-auc:0.849528+0.00177205	test-auc:0.777618+0.00300771
[90]	train-auc:0.850037+0.00183731	test-auc:0.777492+0.00303402
[91]	train-auc:0.850613+0.00175215	test-auc:0.777429+0.0033906
[92]	train-auc:0.851053+0.00178865	test-auc:0.77754+0.00331414
[93]	train-auc:0.851655+0.0017836	test-auc:0.777276+0.00332201
[94]	train-auc:0.851956+0.00183327	test-auc:0.777297+0.00332536
[95]	train-auc:0.852586+0.00194058	test-auc:0.777231+0.00343754
[96]	train-auc:0.853277+0.00185219	test-auc:0.777131+0.00344822
[97]	train-auc:0.853569+0.00176881	test-auc:0.777073+0.00332564
[98]	train-auc:0.854072+0.00183298	test-auc:0.777148+0.00326558
[99]	train-auc:0.854524+0.00185992	test-auc:0.777034+0.00344456
[100]	train-auc:0.854731+0.00187851	test-auc:0.777071+0.00347537
[101]	train-auc:0.855219+0.00172954	test-auc:0.777135+0.00342863
[102]	train-auc:0.856062+0.00180392	test-auc:0.776968+0.00347993
[103]	train-auc:0.856358+0.00199726	test-auc:0.776891+0.00342109
[104]	train-auc:0.856874+0.00210876	test-auc:0.776961+0.00339272
[105]	train-auc:0.857366+0.00223464	test-auc:0.776991+0.00342363
[106]	train-auc:0.857852+0.00216924	test-auc:0.776967+0.0035472
[107]	train-auc:0.858271+0.00228618	test-auc:0.77693+0.00338744
[108]	train-auc:0.858824+0.00227052	test-auc:0.776787+0.0033846

Model Report
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
~/Desktop/風管碩一/bigdata/bigdata-final/final_xg.py in <module>()
    139  scale_pos_weight=1,
    140  seed=27)
--> 141 modelfit(xgb1, X_train, y_train, predictors)
    142 
    143 '''

~/Desktop/風管碩一/bigdata/bigdata-final/final_xg.py in modelfit(alg, X_train, y_train, predictors, useTrainCV, cv_folds, early_stopping_rounds)
    119     #Print model report:
    120     print( "\nModel Report")
--> 121     print( "Accuracy : %.4g" % metrics.accuracy_score(y_train, X_pred))
    122     print( "AUC Score (Train): %f" % metrics.roc_auc_score(y_train, X_predprob))
    123 

NameError: name 'metrics' is not defined

In [33]: run final_xg.py
/Users/tingyun/miniconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)
/Users/tingyun/miniconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.
  DeprecationWarning)
load data...
done

check df2 NaN...
False

select PAY as df3, delete PAY as df4...
done

add PAY column as df4...
done

delete ID...
done

one hot encodding 'SEX','EDUCATION','MARRIAGE' as X, and default as Y...
done

describe X Y...
            LIMIT_BAL           AGE      BILL_AMT1      BILL_AMT2  \
count    30000.000000  30000.000000   30000.000000   30000.000000   
mean    167484.322667     35.485500   51223.330900   49179.075167   
std     129747.661567      9.217904   73635.860576   71173.768783   
min      10000.000000     21.000000 -165580.000000  -69777.000000   
25%      50000.000000     28.000000    3558.750000    2984.750000   
50%     140000.000000     34.000000   22381.500000   21200.000000   
75%     240000.000000     41.000000   67091.000000   64006.250000   
max    1000000.000000     79.000000  964511.000000  983931.000000   

          BILL_AMT3      BILL_AMT4      BILL_AMT5      BILL_AMT6  \
count  3.000000e+04   30000.000000   30000.000000   30000.000000   
mean   4.701315e+04   43262.948967   40311.400967   38871.760400   
std    6.934939e+04   64332.856134   60797.155770   59554.107537   
min   -1.572640e+05 -170000.000000  -81334.000000 -339603.000000   
25%    2.666250e+03    2326.750000    1763.000000    1256.000000   
50%    2.008850e+04   19052.000000   18104.500000   17071.000000   
75%    6.016475e+04   54506.000000   50190.500000   49198.250000   
max    1.664089e+06  891586.000000  927171.000000  961664.000000   

            PAY_AMT1      PAY_AMT2      ...        EDUCATION_1   EDUCATION_2  \
count   30000.000000  3.000000e+04      ...       30000.000000  30000.000000   
mean     5663.580500  5.921163e+03      ...           0.352833      0.467667   
std     16563.280354  2.304087e+04      ...           0.477859      0.498962   
min         0.000000  0.000000e+00      ...           0.000000      0.000000   
25%      1000.000000  8.330000e+02      ...           0.000000      0.000000   
50%      2100.000000  2.009000e+03      ...           0.000000      0.000000   
75%      5006.000000  5.000000e+03      ...           1.000000      1.000000   
max    873552.000000  1.684259e+06      ...           1.000000      1.000000   

        EDUCATION_3   EDUCATION_4   EDUCATION_5   EDUCATION_6    MARRIAGE_0  \
count  30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   
mean       0.163900      0.004100      0.009333      0.001700      0.001800   
std        0.370191      0.063901      0.096159      0.041197      0.042389   
min        0.000000      0.000000      0.000000      0.000000      0.000000   
25%        0.000000      0.000000      0.000000      0.000000      0.000000   
50%        0.000000      0.000000      0.000000      0.000000      0.000000   
75%        0.000000      0.000000      0.000000      0.000000      0.000000   
max        1.000000      1.000000      1.000000      1.000000      1.000000   

         MARRIAGE_1    MARRIAGE_2    MARRIAGE_3  
count  30000.000000  30000.000000  30000.000000  
mean       0.455300      0.532133      0.010767  
std        0.498006      0.498975      0.103204  
min        0.000000      0.000000      0.000000  
25%        0.000000      0.000000      0.000000  
50%        0.000000      1.000000      0.000000  
75%        1.000000      1.000000      0.000000  
max        1.000000      1.000000      1.000000  

[8 rows x 57 columns]
count     30000
unique        2
top           0
freq      23364
Name: default.payment.next.month, dtype: int64
done

splitting training and testing...
done

[0]	train-auc:0.761262+0.00106989	test-auc:0.751539+0.00488465
[1]	train-auc:0.770783+0.00206975	test-auc:0.759387+0.00152567
[2]	train-auc:0.773895+0.00305062	test-auc:0.76316+0.00437927
[3]	train-auc:0.775803+0.00220552	test-auc:0.764238+0.00394304
[4]	train-auc:0.777806+0.00263082	test-auc:0.765849+0.00316605
[5]	train-auc:0.779525+0.00227685	test-auc:0.766868+0.00351199
[6]	train-auc:0.781069+0.00227583	test-auc:0.767847+0.00302836
[7]	train-auc:0.782682+0.00142218	test-auc:0.768569+0.0034779
[8]	train-auc:0.783864+0.000739323	test-auc:0.769221+0.00314476
[9]	train-auc:0.785049+0.000629276	test-auc:0.770094+0.00291014
[10]	train-auc:0.786154+0.00080555	test-auc:0.770351+0.00336168
[11]	train-auc:0.787056+0.0011125	test-auc:0.770876+0.00307864
[12]	train-auc:0.788273+0.00114136	test-auc:0.771344+0.00291937
[13]	train-auc:0.789312+0.00128807	test-auc:0.77231+0.00264051
[14]	train-auc:0.790567+0.00142937	test-auc:0.772615+0.00245878
[15]	train-auc:0.791408+0.00156728	test-auc:0.773343+0.00246872
[16]	train-auc:0.792363+0.0016541	test-auc:0.773809+0.00239282
[17]	train-auc:0.793848+0.00143286	test-auc:0.774176+0.00258822
[18]	train-auc:0.795019+0.00137596	test-auc:0.775134+0.00194994
[19]	train-auc:0.796072+0.00158455	test-auc:0.775529+0.00223022
[20]	train-auc:0.797197+0.00141963	test-auc:0.775661+0.00219797
[21]	train-auc:0.798159+0.00145115	test-auc:0.775775+0.00221312
[22]	train-auc:0.799023+0.00141249	test-auc:0.775882+0.00193393
[23]	train-auc:0.800218+0.00157093	test-auc:0.776024+0.00195479
[24]	train-auc:0.801049+0.00153497	test-auc:0.776037+0.0016422
[25]	train-auc:0.802179+0.00160955	test-auc:0.776599+0.00143563
[26]	train-auc:0.803228+0.00152747	test-auc:0.776946+0.00142234
[27]	train-auc:0.80396+0.00140612	test-auc:0.777039+0.00155865
[28]	train-auc:0.804969+0.0015291	test-auc:0.777583+0.00161361
[29]	train-auc:0.805845+0.00175052	test-auc:0.777892+0.00151713
[30]	train-auc:0.806575+0.00174683	test-auc:0.778128+0.00168899
[31]	train-auc:0.80721+0.00187187	test-auc:0.778104+0.00171786
[32]	train-auc:0.807983+0.00179214	test-auc:0.778235+0.00171789
[33]	train-auc:0.808832+0.00175105	test-auc:0.778345+0.00186704
[34]	train-auc:0.80947+0.00177729	test-auc:0.778436+0.00182133
[35]	train-auc:0.810158+0.00178461	test-auc:0.778304+0.00174599
[36]	train-auc:0.811054+0.00159091	test-auc:0.7784+0.00204284
[37]	train-auc:0.811694+0.00161277	test-auc:0.778594+0.00195597
[38]	train-auc:0.812537+0.00142557	test-auc:0.778731+0.00194524
[39]	train-auc:0.813361+0.00126146	test-auc:0.778659+0.00203557
[40]	train-auc:0.814335+0.0014083	test-auc:0.778676+0.00214683
[41]	train-auc:0.815512+0.00133198	test-auc:0.778536+0.00216922
[42]	train-auc:0.816316+0.00145516	test-auc:0.778601+0.00193018
[43]	train-auc:0.816997+0.00140368	test-auc:0.778736+0.00189167
[44]	train-auc:0.817899+0.00143274	test-auc:0.778736+0.00203304
[45]	train-auc:0.818514+0.00138761	test-auc:0.778804+0.00205366
[46]	train-auc:0.819284+0.00138352	test-auc:0.77889+0.00210676
[47]	train-auc:0.819727+0.00152921	test-auc:0.778947+0.00205395
[48]	train-auc:0.820479+0.00164942	test-auc:0.778965+0.00204255
[49]	train-auc:0.821288+0.001524	test-auc:0.778877+0.00204992
[50]	train-auc:0.821823+0.0015929	test-auc:0.77878+0.00204238
[51]	train-auc:0.822567+0.00150191	test-auc:0.77879+0.00212663
[52]	train-auc:0.823213+0.001872	test-auc:0.778779+0.00212511
[53]	train-auc:0.824+0.00167865	test-auc:0.77884+0.00218131
[54]	train-auc:0.824956+0.00178633	test-auc:0.778922+0.0024454
[55]	train-auc:0.82578+0.00188081	test-auc:0.779061+0.00266808
[56]	train-auc:0.826406+0.00195413	test-auc:0.77887+0.0027048
[57]	train-auc:0.826936+0.00162659	test-auc:0.778933+0.00282151
[58]	train-auc:0.827672+0.00148391	test-auc:0.778866+0.00289792
[59]	train-auc:0.828333+0.00137178	test-auc:0.778905+0.00291433
[60]	train-auc:0.828825+0.00143994	test-auc:0.778983+0.00294541
[61]	train-auc:0.829593+0.0013901	test-auc:0.778866+0.00295491
[62]	train-auc:0.830305+0.00132812	test-auc:0.778771+0.00299225
[63]	train-auc:0.830997+0.00125101	test-auc:0.778885+0.0031049
[64]	train-auc:0.831452+0.00130515	test-auc:0.778806+0.00310219
[65]	train-auc:0.831836+0.00123044	test-auc:0.778793+0.00315949
[66]	train-auc:0.832621+0.00112367	test-auc:0.778684+0.00310211
[67]	train-auc:0.832996+0.00110757	test-auc:0.778702+0.00323297
[68]	train-auc:0.833575+0.000960643	test-auc:0.778702+0.00323203
[69]	train-auc:0.833895+0.000966823	test-auc:0.778735+0.00323925
[70]	train-auc:0.834699+0.0011599	test-auc:0.778551+0.00330918
[71]	train-auc:0.835269+0.00107954	test-auc:0.778413+0.0031884
[72]	train-auc:0.835691+0.00114368	test-auc:0.778316+0.00315295
[73]	train-auc:0.836369+0.00128383	test-auc:0.77842+0.00326904
[74]	train-auc:0.837001+0.00144632	test-auc:0.778363+0.00324334
[75]	train-auc:0.837539+0.00131352	test-auc:0.7783+0.0033539
[76]	train-auc:0.838264+0.00162557	test-auc:0.778279+0.00327432
[77]	train-auc:0.838731+0.00160169	test-auc:0.778211+0.00340767
[78]	train-auc:0.839381+0.00162952	test-auc:0.778153+0.00344408
[79]	train-auc:0.839822+0.0018486	test-auc:0.778026+0.0033926
[80]	train-auc:0.840205+0.00199002	test-auc:0.778067+0.00338368
[81]	train-auc:0.840723+0.00183274	test-auc:0.777985+0.00353599
[82]	train-auc:0.841089+0.00182368	test-auc:0.777811+0.00352084
[83]	train-auc:0.841646+0.00168209	test-auc:0.777841+0.00349584
[84]	train-auc:0.842326+0.00133117	test-auc:0.778022+0.00365084
[85]	train-auc:0.842852+0.00122308	test-auc:0.77801+0.00368378
[86]	train-auc:0.843302+0.00106225	test-auc:0.777949+0.00368893
[87]	train-auc:0.843652+0.00118812	test-auc:0.777852+0.00362811
[88]	train-auc:0.844339+0.00127051	test-auc:0.777905+0.00366329
[89]	train-auc:0.844754+0.00127039	test-auc:0.777785+0.00364886
[90]	train-auc:0.845169+0.00116638	test-auc:0.777763+0.00381634
[91]	train-auc:0.845463+0.00116496	test-auc:0.777736+0.00398018
[92]	train-auc:0.846026+0.00102376	test-auc:0.777691+0.00383508
[93]	train-auc:0.846694+0.00105315	test-auc:0.77767+0.00385901
[94]	train-auc:0.847285+0.00104779	test-auc:0.777693+0.00401928
[95]	train-auc:0.847809+0.00112035	test-auc:0.777552+0.00412698
[96]	train-auc:0.848214+0.001106	test-auc:0.777579+0.00405618
[97]	train-auc:0.848662+0.00128039	test-auc:0.777475+0.00419132
[98]	train-auc:0.84919+0.00134332	test-auc:0.777383+0.00406308
[99]	train-auc:0.849388+0.00132764	test-auc:0.777347+0.00408546
[100]	train-auc:0.84994+0.00138758	test-auc:0.777345+0.00416411
[101]	train-auc:0.85043+0.00137346	test-auc:0.777306+0.0042056
[102]	train-auc:0.851009+0.00148223	test-auc:0.777348+0.00410304
[103]	train-auc:0.851627+0.00151012	test-auc:0.777072+0.00412988
[104]	train-auc:0.852287+0.00149796	test-auc:0.776973+0.00409716

Model Report
Accuracy : 0.8304
AUC Score (Train): 0.818528
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~/Desktop/風管碩一/bigdata/bigdata-final/final_xg.py in <module>()
    140  scale_pos_weight=1,
    141  seed=27)
--> 142 modelfit(xgb1, X_train, y_train, predictors)
    143 
    144 '''

~/Desktop/風管碩一/bigdata/bigdata-final/final_xg.py in modelfit(alg, X_train, y_train, predictors, useTrainCV, cv_folds, early_stopping_rounds)
    123     print( "AUC Score (Train): %f" % metrics.roc_auc_score(y_train, X_predprob))
    124 
--> 125     feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)
    126     feat_imp.plot(kind='bar', title='Feature Importances')
    127     plt.ylabel('Feature Importance Score')

TypeError: 'str' object is not callable

In [34]: run final_xg.py
load data...
done

check df2 NaN...
False

select PAY as df3, delete PAY as df4...
done

add PAY column as df4...
done

delete ID...
done

one hot encodding 'SEX','EDUCATION','MARRIAGE' as X, and default as Y...
done

describe X Y...
            LIMIT_BAL           AGE      BILL_AMT1      BILL_AMT2  \
count    30000.000000  30000.000000   30000.000000   30000.000000   
mean    167484.322667     35.485500   51223.330900   49179.075167   
std     129747.661567      9.217904   73635.860576   71173.768783   
min      10000.000000     21.000000 -165580.000000  -69777.000000   
25%      50000.000000     28.000000    3558.750000    2984.750000   
50%     140000.000000     34.000000   22381.500000   21200.000000   
75%     240000.000000     41.000000   67091.000000   64006.250000   
max    1000000.000000     79.000000  964511.000000  983931.000000   

          BILL_AMT3      BILL_AMT4      BILL_AMT5      BILL_AMT6  \
count  3.000000e+04   30000.000000   30000.000000   30000.000000   
mean   4.701315e+04   43262.948967   40311.400967   38871.760400   
std    6.934939e+04   64332.856134   60797.155770   59554.107537   
min   -1.572640e+05 -170000.000000  -81334.000000 -339603.000000   
25%    2.666250e+03    2326.750000    1763.000000    1256.000000   
50%    2.008850e+04   19052.000000   18104.500000   17071.000000   
75%    6.016475e+04   54506.000000   50190.500000   49198.250000   
max    1.664089e+06  891586.000000  927171.000000  961664.000000   

            PAY_AMT1      PAY_AMT2      ...        EDUCATION_1   EDUCATION_2  \
count   30000.000000  3.000000e+04      ...       30000.000000  30000.000000   
mean     5663.580500  5.921163e+03      ...           0.352833      0.467667   
std     16563.280354  2.304087e+04      ...           0.477859      0.498962   
min         0.000000  0.000000e+00      ...           0.000000      0.000000   
25%      1000.000000  8.330000e+02      ...           0.000000      0.000000   
50%      2100.000000  2.009000e+03      ...           0.000000      0.000000   
75%      5006.000000  5.000000e+03      ...           1.000000      1.000000   
max    873552.000000  1.684259e+06      ...           1.000000      1.000000   

        EDUCATION_3   EDUCATION_4   EDUCATION_5   EDUCATION_6    MARRIAGE_0  \
count  30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   
mean       0.163900      0.004100      0.009333      0.001700      0.001800   
std        0.370191      0.063901      0.096159      0.041197      0.042389   
min        0.000000      0.000000      0.000000      0.000000      0.000000   
25%        0.000000      0.000000      0.000000      0.000000      0.000000   
50%        0.000000      0.000000      0.000000      0.000000      0.000000   
75%        0.000000      0.000000      0.000000      0.000000      0.000000   
max        1.000000      1.000000      1.000000      1.000000      1.000000   

         MARRIAGE_1    MARRIAGE_2    MARRIAGE_3  
count  30000.000000  30000.000000  30000.000000  
mean       0.455300      0.532133      0.010767  
std        0.498006      0.498975      0.103204  
min        0.000000      0.000000      0.000000  
25%        0.000000      0.000000      0.000000  
50%        0.000000      1.000000      0.000000  
75%        1.000000      1.000000      0.000000  
max        1.000000      1.000000      1.000000  

[8 rows x 57 columns]
count     30000
unique        2
top           0
freq      23364
Name: default.payment.next.month, dtype: int64
done

splitting training and testing...
done

[0]	train-auc:0.761262+0.00106989	test-auc:0.751539+0.00488465
[1]	train-auc:0.770783+0.00206975	test-auc:0.759387+0.00152567
[2]	train-auc:0.773895+0.00305062	test-auc:0.76316+0.00437927
[3]	train-auc:0.775803+0.00220552	test-auc:0.764238+0.00394304
[4]	train-auc:0.777806+0.00263082	test-auc:0.765849+0.00316605
[5]	train-auc:0.779525+0.00227685	test-auc:0.766868+0.00351199
[6]	train-auc:0.781069+0.00227583	test-auc:0.767847+0.00302836
[7]	train-auc:0.782682+0.00142218	test-auc:0.768569+0.0034779
[8]	train-auc:0.783864+0.000739323	test-auc:0.769221+0.00314476
[9]	train-auc:0.785049+0.000629276	test-auc:0.770094+0.00291014
[10]	train-auc:0.786154+0.00080555	test-auc:0.770351+0.00336168
[11]	train-auc:0.787056+0.0011125	test-auc:0.770876+0.00307864
[12]	train-auc:0.788273+0.00114136	test-auc:0.771344+0.00291937
[13]	train-auc:0.789312+0.00128807	test-auc:0.77231+0.00264051
[14]	train-auc:0.790567+0.00142937	test-auc:0.772615+0.00245878
[15]	train-auc:0.791408+0.00156728	test-auc:0.773343+0.00246872
[16]	train-auc:0.792363+0.0016541	test-auc:0.773809+0.00239282
[17]	train-auc:0.793848+0.00143286	test-auc:0.774176+0.00258822
[18]	train-auc:0.795019+0.00137596	test-auc:0.775134+0.00194994
[19]	train-auc:0.796072+0.00158455	test-auc:0.775529+0.00223022
[20]	train-auc:0.797197+0.00141963	test-auc:0.775661+0.00219797
[21]	train-auc:0.798159+0.00145115	test-auc:0.775775+0.00221312
[22]	train-auc:0.799023+0.00141249	test-auc:0.775882+0.00193393
[23]	train-auc:0.800218+0.00157093	test-auc:0.776024+0.00195479
[24]	train-auc:0.801049+0.00153497	test-auc:0.776037+0.0016422
[25]	train-auc:0.802179+0.00160955	test-auc:0.776599+0.00143563
[26]	train-auc:0.803228+0.00152747	test-auc:0.776946+0.00142234
[27]	train-auc:0.80396+0.00140612	test-auc:0.777039+0.00155865
[28]	train-auc:0.804969+0.0015291	test-auc:0.777583+0.00161361
[29]	train-auc:0.805845+0.00175052	test-auc:0.777892+0.00151713
[30]	train-auc:0.806575+0.00174683	test-auc:0.778128+0.00168899
[31]	train-auc:0.80721+0.00187187	test-auc:0.778104+0.00171786
[32]	train-auc:0.807983+0.00179214	test-auc:0.778235+0.00171789
[33]	train-auc:0.808832+0.00175105	test-auc:0.778345+0.00186704
[34]	train-auc:0.80947+0.00177729	test-auc:0.778436+0.00182133
[35]	train-auc:0.810158+0.00178461	test-auc:0.778304+0.00174599
[36]	train-auc:0.811054+0.00159091	test-auc:0.7784+0.00204284
[37]	train-auc:0.811694+0.00161277	test-auc:0.778594+0.00195597
[38]	train-auc:0.812537+0.00142557	test-auc:0.778731+0.00194524
[39]	train-auc:0.813361+0.00126146	test-auc:0.778659+0.00203557
[40]	train-auc:0.814335+0.0014083	test-auc:0.778676+0.00214683
[41]	train-auc:0.815512+0.00133198	test-auc:0.778536+0.00216922
[42]	train-auc:0.816316+0.00145516	test-auc:0.778601+0.00193018
[43]	train-auc:0.816997+0.00140368	test-auc:0.778736+0.00189167
[44]	train-auc:0.817899+0.00143274	test-auc:0.778736+0.00203304
[45]	train-auc:0.818514+0.00138761	test-auc:0.778804+0.00205366
[46]	train-auc:0.819284+0.00138352	test-auc:0.77889+0.00210676
[47]	train-auc:0.819727+0.00152921	test-auc:0.778947+0.00205395
[48]	train-auc:0.820479+0.00164942	test-auc:0.778965+0.00204255
[49]	train-auc:0.821288+0.001524	test-auc:0.778877+0.00204992
[50]	train-auc:0.821823+0.0015929	test-auc:0.77878+0.00204238
[51]	train-auc:0.822567+0.00150191	test-auc:0.77879+0.00212663
[52]	train-auc:0.823213+0.001872	test-auc:0.778779+0.00212511
[53]	train-auc:0.824+0.00167865	test-auc:0.77884+0.00218131
[54]	train-auc:0.824956+0.00178633	test-auc:0.778922+0.0024454
[55]	train-auc:0.82578+0.00188081	test-auc:0.779061+0.00266808
[56]	train-auc:0.826406+0.00195413	test-auc:0.77887+0.0027048
[57]	train-auc:0.826936+0.00162659	test-auc:0.778933+0.00282151
[58]	train-auc:0.827672+0.00148391	test-auc:0.778866+0.00289792
[59]	train-auc:0.828333+0.00137178	test-auc:0.778905+0.00291433
[60]	train-auc:0.828825+0.00143994	test-auc:0.778983+0.00294541
[61]	train-auc:0.829593+0.0013901	test-auc:0.778866+0.00295491
[62]	train-auc:0.830305+0.00132812	test-auc:0.778771+0.00299225
[63]	train-auc:0.830997+0.00125101	test-auc:0.778885+0.0031049
[64]	train-auc:0.831452+0.00130515	test-auc:0.778806+0.00310219
[65]	train-auc:0.831836+0.00123044	test-auc:0.778793+0.00315949
[66]	train-auc:0.832621+0.00112367	test-auc:0.778684+0.00310211
[67]	train-auc:0.832996+0.00110757	test-auc:0.778702+0.00323297
[68]	train-auc:0.833575+0.000960643	test-auc:0.778702+0.00323203
[69]	train-auc:0.833895+0.000966823	test-auc:0.778735+0.00323925
[70]	train-auc:0.834699+0.0011599	test-auc:0.778551+0.00330918
[71]	train-auc:0.835269+0.00107954	test-auc:0.778413+0.0031884
[72]	train-auc:0.835691+0.00114368	test-auc:0.778316+0.00315295
[73]	train-auc:0.836369+0.00128383	test-auc:0.77842+0.00326904
[74]	train-auc:0.837001+0.00144632	test-auc:0.778363+0.00324334
[75]	train-auc:0.837539+0.00131352	test-auc:0.7783+0.0033539
[76]	train-auc:0.838264+0.00162557	test-auc:0.778279+0.00327432
[77]	train-auc:0.838731+0.00160169	test-auc:0.778211+0.00340767
[78]	train-auc:0.839381+0.00162952	test-auc:0.778153+0.00344408
[79]	train-auc:0.839822+0.0018486	test-auc:0.778026+0.0033926
[80]	train-auc:0.840205+0.00199002	test-auc:0.778067+0.00338368
[81]	train-auc:0.840723+0.00183274	test-auc:0.777985+0.00353599
[82]	train-auc:0.841089+0.00182368	test-auc:0.777811+0.00352084
[83]	train-auc:0.841646+0.00168209	test-auc:0.777841+0.00349584
[84]	train-auc:0.842326+0.00133117	test-auc:0.778022+0.00365084
[85]	train-auc:0.842852+0.00122308	test-auc:0.77801+0.00368378
[86]	train-auc:0.843302+0.00106225	test-auc:0.777949+0.00368893
[87]	train-auc:0.843652+0.00118812	test-auc:0.777852+0.00362811
[88]	train-auc:0.844339+0.00127051	test-auc:0.777905+0.00366329
[89]	train-auc:0.844754+0.00127039	test-auc:0.777785+0.00364886
[90]	train-auc:0.845169+0.00116638	test-auc:0.777763+0.00381634
[91]	train-auc:0.845463+0.00116496	test-auc:0.777736+0.00398018
[92]	train-auc:0.846026+0.00102376	test-auc:0.777691+0.00383508
[93]	train-auc:0.846694+0.00105315	test-auc:0.77767+0.00385901
[94]	train-auc:0.847285+0.00104779	test-auc:0.777693+0.00401928
[95]	train-auc:0.847809+0.00112035	test-auc:0.777552+0.00412698
[96]	train-auc:0.848214+0.001106	test-auc:0.777579+0.00405618
[97]	train-auc:0.848662+0.00128039	test-auc:0.777475+0.00419132
[98]	train-auc:0.84919+0.00134332	test-auc:0.777383+0.00406308
[99]	train-auc:0.849388+0.00132764	test-auc:0.777347+0.00408546
[100]	train-auc:0.84994+0.00138758	test-auc:0.777345+0.00416411
[101]	train-auc:0.85043+0.00137346	test-auc:0.777306+0.0042056
[102]	train-auc:0.851009+0.00148223	test-auc:0.777348+0.00410304
[103]	train-auc:0.851627+0.00151012	test-auc:0.777072+0.00412988
[104]	train-auc:0.852287+0.00149796	test-auc:0.776973+0.00409716

Model Report
Accuracy : 0.8304
AUC Score (Train): 0.818528
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
~/Desktop/風管碩一/bigdata/bigdata-final/final_xg.py in <module>()
    140  scale_pos_weight=1,
    141  seed=27)
--> 142 modelfit(xgb1, X_train, y_train, predictors)
    143 
    144 '''

~/Desktop/風管碩一/bigdata/bigdata-final/final_xg.py in modelfit(alg, X_train, y_train, predictors, useTrainCV, cv_folds, early_stopping_rounds)
    123     print( "AUC Score (Train): %f" % metrics.roc_auc_score(y_train, X_predprob))
    124 
--> 125     feat_imp = pd.Series(alg.booster.get_fscore()).sort_values(ascending=False)
    126     feat_imp.plot(kind='bar', title='Feature Importances')
    127     plt.ylabel('Feature Importance Score')

AttributeError: 'str' object has no attribute 'get_fscore'

In [35]: run final_xg.py
load data...
done

check df2 NaN...
False

select PAY as df3, delete PAY as df4...
done

add PAY column as df4...
done

delete ID...
done

one hot encodding 'SEX','EDUCATION','MARRIAGE' as X, and default as Y...
done

describe X Y...
            LIMIT_BAL           AGE      BILL_AMT1      BILL_AMT2  \
count    30000.000000  30000.000000   30000.000000   30000.000000   
mean    167484.322667     35.485500   51223.330900   49179.075167   
std     129747.661567      9.217904   73635.860576   71173.768783   
min      10000.000000     21.000000 -165580.000000  -69777.000000   
25%      50000.000000     28.000000    3558.750000    2984.750000   
50%     140000.000000     34.000000   22381.500000   21200.000000   
75%     240000.000000     41.000000   67091.000000   64006.250000   
max    1000000.000000     79.000000  964511.000000  983931.000000   

          BILL_AMT3      BILL_AMT4      BILL_AMT5      BILL_AMT6  \
count  3.000000e+04   30000.000000   30000.000000   30000.000000   
mean   4.701315e+04   43262.948967   40311.400967   38871.760400   
std    6.934939e+04   64332.856134   60797.155770   59554.107537   
min   -1.572640e+05 -170000.000000  -81334.000000 -339603.000000   
25%    2.666250e+03    2326.750000    1763.000000    1256.000000   
50%    2.008850e+04   19052.000000   18104.500000   17071.000000   
75%    6.016475e+04   54506.000000   50190.500000   49198.250000   
max    1.664089e+06  891586.000000  927171.000000  961664.000000   

            PAY_AMT1      PAY_AMT2      ...        EDUCATION_1   EDUCATION_2  \
count   30000.000000  3.000000e+04      ...       30000.000000  30000.000000   
mean     5663.580500  5.921163e+03      ...           0.352833      0.467667   
std     16563.280354  2.304087e+04      ...           0.477859      0.498962   
min         0.000000  0.000000e+00      ...           0.000000      0.000000   
25%      1000.000000  8.330000e+02      ...           0.000000      0.000000   
50%      2100.000000  2.009000e+03      ...           0.000000      0.000000   
75%      5006.000000  5.000000e+03      ...           1.000000      1.000000   
max    873552.000000  1.684259e+06      ...           1.000000      1.000000   

        EDUCATION_3   EDUCATION_4   EDUCATION_5   EDUCATION_6    MARRIAGE_0  \
count  30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   
mean       0.163900      0.004100      0.009333      0.001700      0.001800   
std        0.370191      0.063901      0.096159      0.041197      0.042389   
min        0.000000      0.000000      0.000000      0.000000      0.000000   
25%        0.000000      0.000000      0.000000      0.000000      0.000000   
50%        0.000000      0.000000      0.000000      0.000000      0.000000   
75%        0.000000      0.000000      0.000000      0.000000      0.000000   
max        1.000000      1.000000      1.000000      1.000000      1.000000   

         MARRIAGE_1    MARRIAGE_2    MARRIAGE_3  
count  30000.000000  30000.000000  30000.000000  
mean       0.455300      0.532133      0.010767  
std        0.498006      0.498975      0.103204  
min        0.000000      0.000000      0.000000  
25%        0.000000      0.000000      0.000000  
50%        0.000000      1.000000      0.000000  
75%        1.000000      1.000000      0.000000  
max        1.000000      1.000000      1.000000  

[8 rows x 57 columns]
count     30000
unique        2
top           0
freq      23364
Name: default.payment.next.month, dtype: int64
done

splitting training and testing...
done

[0]	train-auc:0.761262+0.00106989	test-auc:0.751539+0.00488465
[1]	train-auc:0.770783+0.00206975	test-auc:0.759387+0.00152567
[2]	train-auc:0.773895+0.00305062	test-auc:0.76316+0.00437927
[3]	train-auc:0.775803+0.00220552	test-auc:0.764238+0.00394304
[4]	train-auc:0.777806+0.00263082	test-auc:0.765849+0.00316605
[5]	train-auc:0.779525+0.00227685	test-auc:0.766868+0.00351199
[6]	train-auc:0.781069+0.00227583	test-auc:0.767847+0.00302836
[7]	train-auc:0.782682+0.00142218	test-auc:0.768569+0.0034779
[8]	train-auc:0.783864+0.000739323	test-auc:0.769221+0.00314476
[9]	train-auc:0.785049+0.000629276	test-auc:0.770094+0.00291014
[10]	train-auc:0.786154+0.00080555	test-auc:0.770351+0.00336168
[11]	train-auc:0.787056+0.0011125	test-auc:0.770876+0.00307864
[12]	train-auc:0.788273+0.00114136	test-auc:0.771344+0.00291937
[13]	train-auc:0.789312+0.00128807	test-auc:0.77231+0.00264051
[14]	train-auc:0.790567+0.00142937	test-auc:0.772615+0.00245878
[15]	train-auc:0.791408+0.00156728	test-auc:0.773343+0.00246872
[16]	train-auc:0.792363+0.0016541	test-auc:0.773809+0.00239282
[17]	train-auc:0.793848+0.00143286	test-auc:0.774176+0.00258822
[18]	train-auc:0.795019+0.00137596	test-auc:0.775134+0.00194994
[19]	train-auc:0.796072+0.00158455	test-auc:0.775529+0.00223022
[20]	train-auc:0.797197+0.00141963	test-auc:0.775661+0.00219797
[21]	train-auc:0.798159+0.00145115	test-auc:0.775775+0.00221312
[22]	train-auc:0.799023+0.00141249	test-auc:0.775882+0.00193393
[23]	train-auc:0.800218+0.00157093	test-auc:0.776024+0.00195479
[24]	train-auc:0.801049+0.00153497	test-auc:0.776037+0.0016422
[25]	train-auc:0.802179+0.00160955	test-auc:0.776599+0.00143563
[26]	train-auc:0.803228+0.00152747	test-auc:0.776946+0.00142234
[27]	train-auc:0.80396+0.00140612	test-auc:0.777039+0.00155865
[28]	train-auc:0.804969+0.0015291	test-auc:0.777583+0.00161361
[29]	train-auc:0.805845+0.00175052	test-auc:0.777892+0.00151713
[30]	train-auc:0.806575+0.00174683	test-auc:0.778128+0.00168899
[31]	train-auc:0.80721+0.00187187	test-auc:0.778104+0.00171786
[32]	train-auc:0.807983+0.00179214	test-auc:0.778235+0.00171789
[33]	train-auc:0.808832+0.00175105	test-auc:0.778345+0.00186704
[34]	train-auc:0.80947+0.00177729	test-auc:0.778436+0.00182133
[35]	train-auc:0.810158+0.00178461	test-auc:0.778304+0.00174599
[36]	train-auc:0.811054+0.00159091	test-auc:0.7784+0.00204284
[37]	train-auc:0.811694+0.00161277	test-auc:0.778594+0.00195597
[38]	train-auc:0.812537+0.00142557	test-auc:0.778731+0.00194524
[39]	train-auc:0.813361+0.00126146	test-auc:0.778659+0.00203557
[40]	train-auc:0.814335+0.0014083	test-auc:0.778676+0.00214683
[41]	train-auc:0.815512+0.00133198	test-auc:0.778536+0.00216922
[42]	train-auc:0.816316+0.00145516	test-auc:0.778601+0.00193018
[43]	train-auc:0.816997+0.00140368	test-auc:0.778736+0.00189167
[44]	train-auc:0.817899+0.00143274	test-auc:0.778736+0.00203304
[45]	train-auc:0.818514+0.00138761	test-auc:0.778804+0.00205366
[46]	train-auc:0.819284+0.00138352	test-auc:0.77889+0.00210676
[47]	train-auc:0.819727+0.00152921	test-auc:0.778947+0.00205395
[48]	train-auc:0.820479+0.00164942	test-auc:0.778965+0.00204255
[49]	train-auc:0.821288+0.001524	test-auc:0.778877+0.00204992
[50]	train-auc:0.821823+0.0015929	test-auc:0.77878+0.00204238
[51]	train-auc:0.822567+0.00150191	test-auc:0.77879+0.00212663
[52]	train-auc:0.823213+0.001872	test-auc:0.778779+0.00212511
[53]	train-auc:0.824+0.00167865	test-auc:0.77884+0.00218131
[54]	train-auc:0.824956+0.00178633	test-auc:0.778922+0.0024454
[55]	train-auc:0.82578+0.00188081	test-auc:0.779061+0.00266808
[56]	train-auc:0.826406+0.00195413	test-auc:0.77887+0.0027048
[57]	train-auc:0.826936+0.00162659	test-auc:0.778933+0.00282151
[58]	train-auc:0.827672+0.00148391	test-auc:0.778866+0.00289792
[59]	train-auc:0.828333+0.00137178	test-auc:0.778905+0.00291433
[60]	train-auc:0.828825+0.00143994	test-auc:0.778983+0.00294541
[61]	train-auc:0.829593+0.0013901	test-auc:0.778866+0.00295491
[62]	train-auc:0.830305+0.00132812	test-auc:0.778771+0.00299225
[63]	train-auc:0.830997+0.00125101	test-auc:0.778885+0.0031049
[64]	train-auc:0.831452+0.00130515	test-auc:0.778806+0.00310219
[65]	train-auc:0.831836+0.00123044	test-auc:0.778793+0.00315949
[66]	train-auc:0.832621+0.00112367	test-auc:0.778684+0.00310211
[67]	train-auc:0.832996+0.00110757	test-auc:0.778702+0.00323297
[68]	train-auc:0.833575+0.000960643	test-auc:0.778702+0.00323203
[69]	train-auc:0.833895+0.000966823	test-auc:0.778735+0.00323925
[70]	train-auc:0.834699+0.0011599	test-auc:0.778551+0.00330918
[71]	train-auc:0.835269+0.00107954	test-auc:0.778413+0.0031884
[72]	train-auc:0.835691+0.00114368	test-auc:0.778316+0.00315295
[73]	train-auc:0.836369+0.00128383	test-auc:0.77842+0.00326904
[74]	train-auc:0.837001+0.00144632	test-auc:0.778363+0.00324334
[75]	train-auc:0.837539+0.00131352	test-auc:0.7783+0.0033539
[76]	train-auc:0.838264+0.00162557	test-auc:0.778279+0.00327432
[77]	train-auc:0.838731+0.00160169	test-auc:0.778211+0.00340767
[78]	train-auc:0.839381+0.00162952	test-auc:0.778153+0.00344408
[79]	train-auc:0.839822+0.0018486	test-auc:0.778026+0.0033926
[80]	train-auc:0.840205+0.00199002	test-auc:0.778067+0.00338368
[81]	train-auc:0.840723+0.00183274	test-auc:0.777985+0.00353599
[82]	train-auc:0.841089+0.00182368	test-auc:0.777811+0.00352084
[83]	train-auc:0.841646+0.00168209	test-auc:0.777841+0.00349584
[84]	train-auc:0.842326+0.00133117	test-auc:0.778022+0.00365084
[85]	train-auc:0.842852+0.00122308	test-auc:0.77801+0.00368378
[86]	train-auc:0.843302+0.00106225	test-auc:0.777949+0.00368893
[87]	train-auc:0.843652+0.00118812	test-auc:0.777852+0.00362811
[88]	train-auc:0.844339+0.00127051	test-auc:0.777905+0.00366329
[89]	train-auc:0.844754+0.00127039	test-auc:0.777785+0.00364886
[90]	train-auc:0.845169+0.00116638	test-auc:0.777763+0.00381634
[91]	train-auc:0.845463+0.00116496	test-auc:0.777736+0.00398018
[92]	train-auc:0.846026+0.00102376	test-auc:0.777691+0.00383508
[93]	train-auc:0.846694+0.00105315	test-auc:0.77767+0.00385901
[94]	train-auc:0.847285+0.00104779	test-auc:0.777693+0.00401928
[95]	train-auc:0.847809+0.00112035	test-auc:0.777552+0.00412698
[96]	train-auc:0.848214+0.001106	test-auc:0.777579+0.00405618
[97]	train-auc:0.848662+0.00128039	test-auc:0.777475+0.00419132
[98]	train-auc:0.84919+0.00134332	test-auc:0.777383+0.00406308
[99]	train-auc:0.849388+0.00132764	test-auc:0.777347+0.00408546
[100]	train-auc:0.84994+0.00138758	test-auc:0.777345+0.00416411
[101]	train-auc:0.85043+0.00137346	test-auc:0.777306+0.0042056
[102]	train-auc:0.851009+0.00148223	test-auc:0.777348+0.00410304
[103]	train-auc:0.851627+0.00151012	test-auc:0.777072+0.00412988
[104]	train-auc:0.852287+0.00149796	test-auc:0.776973+0.00409716

Model Report
Accuracy : 0.8304
AUC Score (Train): 0.818528
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~/Desktop/風管碩一/bigdata/bigdata-final/final_xg.py in <module>()
    140  scale_pos_weight=1,
    141  seed=27)
--> 142 modelfit(xgb1, X_train, y_train, predictors)
    143 
    144 '''

~/Desktop/風管碩一/bigdata/bigdata-final/final_xg.py in modelfit(alg, X_train, y_train, predictors, useTrainCV, cv_folds, early_stopping_rounds)
    123     print( "AUC Score (Train): %f" % metrics.roc_auc_score(y_train, X_predprob))
    124 
--> 125     feat_imp = pd.Series(alg.booster().get_fscore).sort_values(ascending=False)
    126     feat_imp.plot(kind='bar', title='Feature Importances')
    127     plt.ylabel('Feature Importance Score')

TypeError: 'str' object is not callable

In [36]: run final_xg.py
load data...
done

check df2 NaN...
False

select PAY as df3, delete PAY as df4...
done

add PAY column as df4...
done

delete ID...
done

one hot encodding 'SEX','EDUCATION','MARRIAGE' as X, and default as Y...
done

describe X Y...
            LIMIT_BAL           AGE      BILL_AMT1      BILL_AMT2  \
count    30000.000000  30000.000000   30000.000000   30000.000000   
mean    167484.322667     35.485500   51223.330900   49179.075167   
std     129747.661567      9.217904   73635.860576   71173.768783   
min      10000.000000     21.000000 -165580.000000  -69777.000000   
25%      50000.000000     28.000000    3558.750000    2984.750000   
50%     140000.000000     34.000000   22381.500000   21200.000000   
75%     240000.000000     41.000000   67091.000000   64006.250000   
max    1000000.000000     79.000000  964511.000000  983931.000000   

          BILL_AMT3      BILL_AMT4      BILL_AMT5      BILL_AMT6  \
count  3.000000e+04   30000.000000   30000.000000   30000.000000   
mean   4.701315e+04   43262.948967   40311.400967   38871.760400   
std    6.934939e+04   64332.856134   60797.155770   59554.107537   
min   -1.572640e+05 -170000.000000  -81334.000000 -339603.000000   
25%    2.666250e+03    2326.750000    1763.000000    1256.000000   
50%    2.008850e+04   19052.000000   18104.500000   17071.000000   
75%    6.016475e+04   54506.000000   50190.500000   49198.250000   
max    1.664089e+06  891586.000000  927171.000000  961664.000000   

            PAY_AMT1      PAY_AMT2      ...        EDUCATION_1   EDUCATION_2  \
count   30000.000000  3.000000e+04      ...       30000.000000  30000.000000   
mean     5663.580500  5.921163e+03      ...           0.352833      0.467667   
std     16563.280354  2.304087e+04      ...           0.477859      0.498962   
min         0.000000  0.000000e+00      ...           0.000000      0.000000   
25%      1000.000000  8.330000e+02      ...           0.000000      0.000000   
50%      2100.000000  2.009000e+03      ...           0.000000      0.000000   
75%      5006.000000  5.000000e+03      ...           1.000000      1.000000   
max    873552.000000  1.684259e+06      ...           1.000000      1.000000   

        EDUCATION_3   EDUCATION_4   EDUCATION_5   EDUCATION_6    MARRIAGE_0  \
count  30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   
mean       0.163900      0.004100      0.009333      0.001700      0.001800   
std        0.370191      0.063901      0.096159      0.041197      0.042389   
min        0.000000      0.000000      0.000000      0.000000      0.000000   
25%        0.000000      0.000000      0.000000      0.000000      0.000000   
50%        0.000000      0.000000      0.000000      0.000000      0.000000   
75%        0.000000      0.000000      0.000000      0.000000      0.000000   
max        1.000000      1.000000      1.000000      1.000000      1.000000   

         MARRIAGE_1    MARRIAGE_2    MARRIAGE_3  
count  30000.000000  30000.000000  30000.000000  
mean       0.455300      0.532133      0.010767  
std        0.498006      0.498975      0.103204  
min        0.000000      0.000000      0.000000  
25%        0.000000      0.000000      0.000000  
50%        0.000000      1.000000      0.000000  
75%        1.000000      1.000000      0.000000  
max        1.000000      1.000000      1.000000  

[8 rows x 57 columns]
count     30000
unique        2
top           0
freq      23364
Name: default.payment.next.month, dtype: int64
done

splitting training and testing...
done

[0]	train-auc:0.761262+0.00106989	test-auc:0.751539+0.00488465
[1]	train-auc:0.771072+0.00185415	test-auc:0.759632+0.00185325
[2]	train-auc:0.777287+0.00265697	test-auc:0.764952+0.00216168
[3]	train-auc:0.779081+0.00219126	test-auc:0.765777+0.00269409
[4]	train-auc:0.782317+0.00176664	test-auc:0.768202+0.00276387
[5]	train-auc:0.785277+0.00103477	test-auc:0.769761+0.00221473
[6]	train-auc:0.788356+0.00125676	test-auc:0.771692+0.00206396
[7]	train-auc:0.790856+0.00188279	test-auc:0.772367+0.00186716
[8]	train-auc:0.792833+0.00165655	test-auc:0.772775+0.00216506
[9]	train-auc:0.794917+0.00164328	test-auc:0.77385+0.00233364
[10]	train-auc:0.796622+0.0015293	test-auc:0.77423+0.00305224
[11]	train-auc:0.798815+0.00142007	test-auc:0.775441+0.00311652
[12]	train-auc:0.800324+0.00145673	test-auc:0.775295+0.00323133
[13]	train-auc:0.801791+0.0014255	test-auc:0.775846+0.00385736
[14]	train-auc:0.80374+0.00187819	test-auc:0.776232+0.00383317
[15]	train-auc:0.805217+0.00195188	test-auc:0.776311+0.00424359
[16]	train-auc:0.807167+0.00183541	test-auc:0.77661+0.00472907
[17]	train-auc:0.808973+0.001942	test-auc:0.776781+0.00482842
[18]	train-auc:0.8104+0.00212596	test-auc:0.777076+0.00485489
[19]	train-auc:0.812423+0.00166502	test-auc:0.777443+0.00452554
[20]	train-auc:0.81388+0.0015861	test-auc:0.777732+0.00485978
[21]	train-auc:0.815005+0.00134057	test-auc:0.777879+0.00490432
[22]	train-auc:0.816509+0.00093758	test-auc:0.778064+0.00491148
[23]	train-auc:0.817611+0.000986015	test-auc:0.778214+0.00506704
[24]	train-auc:0.819164+0.000986998	test-auc:0.778547+0.00505034
[25]	train-auc:0.820369+0.00104845	test-auc:0.778307+0.00521259
[26]	train-auc:0.821224+0.00105315	test-auc:0.778494+0.00492913
[27]	train-auc:0.822384+0.00125479	test-auc:0.778787+0.00465943
[28]	train-auc:0.823657+0.00124557	test-auc:0.778863+0.00481394
[29]	train-auc:0.825105+0.00091974	test-auc:0.778497+0.00507161
[30]	train-auc:0.826058+0.000993684	test-auc:0.77869+0.00507767
[31]	train-auc:0.827133+0.000886206	test-auc:0.778643+0.00529385
[32]	train-auc:0.82831+0.00119047	test-auc:0.778559+0.00520448
[33]	train-auc:0.829652+0.001014	test-auc:0.778474+0.00536931
[34]	train-auc:0.830549+0.000928494	test-auc:0.778603+0.00538919
[35]	train-auc:0.831642+0.0007777	test-auc:0.77809+0.00534666
[36]	train-auc:0.832837+0.000802202	test-auc:0.777954+0.00537673
[37]	train-auc:0.834043+0.00114053	test-auc:0.777999+0.00542321
[38]	train-auc:0.834797+0.00119331	test-auc:0.778003+0.00542023
[39]	train-auc:0.835528+0.00137669	test-auc:0.77771+0.00514332
[40]	train-auc:0.836414+0.0015504	test-auc:0.777668+0.00540553
[41]	train-auc:0.837697+0.00166275	test-auc:0.777478+0.00564631
[42]	train-auc:0.838602+0.00137998	test-auc:0.777545+0.00545228
[43]	train-auc:0.839423+0.00150458	test-auc:0.77734+0.00540111
[44]	train-auc:0.840251+0.0016553	test-auc:0.777056+0.00541721
[45]	train-auc:0.841032+0.00151952	test-auc:0.776903+0.00552714
[46]	train-auc:0.842416+0.00123419	test-auc:0.776931+0.00547897
[47]	train-auc:0.843512+0.00141006	test-auc:0.777229+0.0055038
[48]	train-auc:0.844827+0.00115044	test-auc:0.777254+0.00529381
[49]	train-auc:0.845792+0.00120588	test-auc:0.777061+0.00517828
[50]	train-auc:0.846577+0.00112457	test-auc:0.776817+0.0052354
[51]	train-auc:0.847455+0.00112683	test-auc:0.776735+0.00513873
[52]	train-auc:0.848632+0.000907655	test-auc:0.776613+0.00523467
[53]	train-auc:0.849646+0.000912566	test-auc:0.776656+0.00556627
[54]	train-auc:0.851077+0.000519955	test-auc:0.776737+0.00533132
[55]	train-auc:0.852118+0.000797402	test-auc:0.77672+0.00518615
[56]	train-auc:0.853153+0.00109477	test-auc:0.776834+0.00524513
[57]	train-auc:0.85403+0.00150844	test-auc:0.776908+0.00549926
[58]	train-auc:0.854923+0.00194525	test-auc:0.776741+0.00543823
[59]	train-auc:0.855495+0.0019886	test-auc:0.776448+0.00548919
[60]	train-auc:0.856161+0.00207332	test-auc:0.776458+0.00526273
[61]	train-auc:0.857153+0.00217077	test-auc:0.776527+0.00519921
[62]	train-auc:0.857855+0.00229718	test-auc:0.776484+0.00505106
[63]	train-auc:0.858904+0.00221223	test-auc:0.776084+0.0051435
[64]	train-auc:0.859652+0.00179282	test-auc:0.775979+0.00524496
[65]	train-auc:0.860567+0.00165686	test-auc:0.775808+0.00548017
[66]	train-auc:0.861528+0.00175336	test-auc:0.775426+0.00546745
[67]	train-auc:0.862268+0.00203945	test-auc:0.775335+0.00570137
[68]	train-auc:0.863046+0.00193002	test-auc:0.7753+0.00564819
[69]	train-auc:0.864089+0.00204119	test-auc:0.775481+0.00616737
[70]	train-auc:0.864931+0.0018587	test-auc:0.775463+0.00606444
[71]	train-auc:0.865598+0.00154562	test-auc:0.775253+0.00612503
[72]	train-auc:0.86598+0.00170449	test-auc:0.775303+0.00612545
[73]	train-auc:0.866796+0.00172111	test-auc:0.775359+0.00614684
[74]	train-auc:0.867627+0.00152281	test-auc:0.775144+0.00618126
[75]	train-auc:0.868435+0.00133425	test-auc:0.774911+0.00630157
[76]	train-auc:0.869088+0.00142047	test-auc:0.774827+0.00619966
[77]	train-auc:0.869918+0.00132253	test-auc:0.774823+0.00637433

Model Report
Accuracy : 0.8313
AUC Score (Train): 0.817859
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
~/Desktop/風管碩一/bigdata/bigdata-final/final_xg.py in <module>()
    140  scale_pos_weight=1,
    141  seed=27)
--> 142 modelfit(xgb1, X_train, y_train, predictors)
    143 
    144 '''

~/Desktop/風管碩一/bigdata/bigdata-final/final_xg.py in modelfit(alg, X_train, y_train, predictors, useTrainCV, cv_folds, early_stopping_rounds)
    123     print( "AUC Score (Train): %f" % metrics.roc_auc_score(y_train, X_predprob))
    124 
--> 125     feat_imp = pd.Series(alg.booster.get_fscore).sort_values(ascending=False)
    126     feat_imp.plot(kind='bar', title='Feature Importances')
    127     plt.ylabel('Feature Importance Score')

AttributeError: 'str' object has no attribute 'get_fscore'

In [37]: run final_xg.py
load data...
done

check df2 NaN...
False

select PAY as df3, delete PAY as df4...
done

add PAY column as df4...
done

delete ID...
done

one hot encodding 'SEX','EDUCATION','MARRIAGE' as X, and default as Y...
done

describe X Y...
            LIMIT_BAL           AGE      BILL_AMT1      BILL_AMT2  \
count    30000.000000  30000.000000   30000.000000   30000.000000   
mean    167484.322667     35.485500   51223.330900   49179.075167   
std     129747.661567      9.217904   73635.860576   71173.768783   
min      10000.000000     21.000000 -165580.000000  -69777.000000   
25%      50000.000000     28.000000    3558.750000    2984.750000   
50%     140000.000000     34.000000   22381.500000   21200.000000   
75%     240000.000000     41.000000   67091.000000   64006.250000   
max    1000000.000000     79.000000  964511.000000  983931.000000   

          BILL_AMT3      BILL_AMT4      BILL_AMT5      BILL_AMT6  \
count  3.000000e+04   30000.000000   30000.000000   30000.000000   
mean   4.701315e+04   43262.948967   40311.400967   38871.760400   
std    6.934939e+04   64332.856134   60797.155770   59554.107537   
min   -1.572640e+05 -170000.000000  -81334.000000 -339603.000000   
25%    2.666250e+03    2326.750000    1763.000000    1256.000000   
50%    2.008850e+04   19052.000000   18104.500000   17071.000000   
75%    6.016475e+04   54506.000000   50190.500000   49198.250000   
max    1.664089e+06  891586.000000  927171.000000  961664.000000   

            PAY_AMT1      PAY_AMT2      ...        EDUCATION_1   EDUCATION_2  \
count   30000.000000  3.000000e+04      ...       30000.000000  30000.000000   
mean     5663.580500  5.921163e+03      ...           0.352833      0.467667   
std     16563.280354  2.304087e+04      ...           0.477859      0.498962   
min         0.000000  0.000000e+00      ...           0.000000      0.000000   
25%      1000.000000  8.330000e+02      ...           0.000000      0.000000   
50%      2100.000000  2.009000e+03      ...           0.000000      0.000000   
75%      5006.000000  5.000000e+03      ...           1.000000      1.000000   
max    873552.000000  1.684259e+06      ...           1.000000      1.000000   

        EDUCATION_3   EDUCATION_4   EDUCATION_5   EDUCATION_6    MARRIAGE_0  \
count  30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   
mean       0.163900      0.004100      0.009333      0.001700      0.001800   
std        0.370191      0.063901      0.096159      0.041197      0.042389   
min        0.000000      0.000000      0.000000      0.000000      0.000000   
25%        0.000000      0.000000      0.000000      0.000000      0.000000   
50%        0.000000      0.000000      0.000000      0.000000      0.000000   
75%        0.000000      0.000000      0.000000      0.000000      0.000000   
max        1.000000      1.000000      1.000000      1.000000      1.000000   

         MARRIAGE_1    MARRIAGE_2    MARRIAGE_3  
count  30000.000000  30000.000000  30000.000000  
mean       0.455300      0.532133      0.010767  
std        0.498006      0.498975      0.103204  
min        0.000000      0.000000      0.000000  
25%        0.000000      0.000000      0.000000  
50%        0.000000      1.000000      0.000000  
75%        1.000000      1.000000      0.000000  
max        1.000000      1.000000      1.000000  

[8 rows x 57 columns]
count     30000
unique        2
top           0
freq      23364
Name: default.payment.next.month, dtype: int64
done

splitting training and testing...
done

[0]	train-auc:0.761262+0.00106989	test-auc:0.751539+0.00488465
[1]	train-auc:0.771072+0.00185415	test-auc:0.759632+0.00185325
[2]	train-auc:0.777287+0.00265697	test-auc:0.764952+0.00216168
[3]	train-auc:0.779081+0.00219126	test-auc:0.765777+0.00269409
[4]	train-auc:0.782317+0.00176664	test-auc:0.768202+0.00276387
[5]	train-auc:0.785277+0.00103477	test-auc:0.769761+0.00221473
[6]	train-auc:0.788356+0.00125676	test-auc:0.771692+0.00206396
[7]	train-auc:0.790856+0.00188279	test-auc:0.772367+0.00186716
[8]	train-auc:0.792833+0.00165655	test-auc:0.772775+0.00216506
[9]	train-auc:0.794917+0.00164328	test-auc:0.77385+0.00233364
[10]	train-auc:0.796622+0.0015293	test-auc:0.77423+0.00305224
[11]	train-auc:0.798815+0.00142007	test-auc:0.775441+0.00311652
[12]	train-auc:0.800324+0.00145673	test-auc:0.775295+0.00323133
[13]	train-auc:0.801791+0.0014255	test-auc:0.775846+0.00385736
[14]	train-auc:0.80374+0.00187819	test-auc:0.776232+0.00383317
[15]	train-auc:0.805217+0.00195188	test-auc:0.776311+0.00424359
[16]	train-auc:0.807167+0.00183541	test-auc:0.77661+0.00472907
[17]	train-auc:0.808973+0.001942	test-auc:0.776781+0.00482842
[18]	train-auc:0.8104+0.00212596	test-auc:0.777076+0.00485489
[19]	train-auc:0.812423+0.00166502	test-auc:0.777443+0.00452554
[20]	train-auc:0.81388+0.0015861	test-auc:0.777732+0.00485978
[21]	train-auc:0.815005+0.00134057	test-auc:0.777879+0.00490432
[22]	train-auc:0.816509+0.00093758	test-auc:0.778064+0.00491148
[23]	train-auc:0.817611+0.000986015	test-auc:0.778214+0.00506704
[24]	train-auc:0.819164+0.000986998	test-auc:0.778547+0.00505034
[25]	train-auc:0.820369+0.00104845	test-auc:0.778307+0.00521259
[26]	train-auc:0.821224+0.00105315	test-auc:0.778494+0.00492913
[27]	train-auc:0.822384+0.00125479	test-auc:0.778787+0.00465943
[28]	train-auc:0.823657+0.00124557	test-auc:0.778863+0.00481394
[29]	train-auc:0.825105+0.00091974	test-auc:0.778497+0.00507161
[30]	train-auc:0.826058+0.000993684	test-auc:0.77869+0.00507767
[31]	train-auc:0.827133+0.000886206	test-auc:0.778643+0.00529385
[32]	train-auc:0.82831+0.00119047	test-auc:0.778559+0.00520448
[33]	train-auc:0.829652+0.001014	test-auc:0.778474+0.00536931
[34]	train-auc:0.830549+0.000928494	test-auc:0.778603+0.00538919
[35]	train-auc:0.831642+0.0007777	test-auc:0.77809+0.00534666
[36]	train-auc:0.832837+0.000802202	test-auc:0.777954+0.00537673
[37]	train-auc:0.834043+0.00114053	test-auc:0.777999+0.00542321
[38]	train-auc:0.834797+0.00119331	test-auc:0.778003+0.00542023
[39]	train-auc:0.835528+0.00137669	test-auc:0.77771+0.00514332
[40]	train-auc:0.836414+0.0015504	test-auc:0.777668+0.00540553
[41]	train-auc:0.837697+0.00166275	test-auc:0.777478+0.00564631
[42]	train-auc:0.838602+0.00137998	test-auc:0.777545+0.00545228
[43]	train-auc:0.839423+0.00150458	test-auc:0.77734+0.00540111
[44]	train-auc:0.840251+0.0016553	test-auc:0.777056+0.00541721
[45]	train-auc:0.841032+0.00151952	test-auc:0.776903+0.00552714
[46]	train-auc:0.842416+0.00123419	test-auc:0.776931+0.00547897
[47]	train-auc:0.843512+0.00141006	test-auc:0.777229+0.0055038
[48]	train-auc:0.844827+0.00115044	test-auc:0.777254+0.00529381
[49]	train-auc:0.845792+0.00120588	test-auc:0.777061+0.00517828
[50]	train-auc:0.846577+0.00112457	test-auc:0.776817+0.0052354
[51]	train-auc:0.847455+0.00112683	test-auc:0.776735+0.00513873
[52]	train-auc:0.848632+0.000907655	test-auc:0.776613+0.00523467
[53]	train-auc:0.849646+0.000912566	test-auc:0.776656+0.00556627
[54]	train-auc:0.851077+0.000519955	test-auc:0.776737+0.00533132
[55]	train-auc:0.852118+0.000797402	test-auc:0.77672+0.00518615
[56]	train-auc:0.853153+0.00109477	test-auc:0.776834+0.00524513
[57]	train-auc:0.85403+0.00150844	test-auc:0.776908+0.00549926
[58]	train-auc:0.854923+0.00194525	test-auc:0.776741+0.00543823
[59]	train-auc:0.855495+0.0019886	test-auc:0.776448+0.00548919
[60]	train-auc:0.856161+0.00207332	test-auc:0.776458+0.00526273
[61]	train-auc:0.857153+0.00217077	test-auc:0.776527+0.00519921
[62]	train-auc:0.857855+0.00229718	test-auc:0.776484+0.00505106
[63]	train-auc:0.858904+0.00221223	test-auc:0.776084+0.0051435
[64]	train-auc:0.859652+0.00179282	test-auc:0.775979+0.00524496
[65]	train-auc:0.860567+0.00165686	test-auc:0.775808+0.00548017
[66]	train-auc:0.861528+0.00175336	test-auc:0.775426+0.00546745
[67]	train-auc:0.862268+0.00203945	test-auc:0.775335+0.00570137
[68]	train-auc:0.863046+0.00193002	test-auc:0.7753+0.00564819
[69]	train-auc:0.864089+0.00204119	test-auc:0.775481+0.00616737
[70]	train-auc:0.864931+0.0018587	test-auc:0.775463+0.00606444
[71]	train-auc:0.865598+0.00154562	test-auc:0.775253+0.00612503
[72]	train-auc:0.86598+0.00170449	test-auc:0.775303+0.00612545
[73]	train-auc:0.866796+0.00172111	test-auc:0.775359+0.00614684
[74]	train-auc:0.867627+0.00152281	test-auc:0.775144+0.00618126
[75]	train-auc:0.868435+0.00133425	test-auc:0.774911+0.00630157
[76]	train-auc:0.869088+0.00142047	test-auc:0.774827+0.00619966
[77]	train-auc:0.869918+0.00132253	test-auc:0.774823+0.00637433

Model Report
 - train_acc : 0.8313
 - train auc score: 0.817859
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
~/Desktop/風管碩一/bigdata/bigdata-final/final_xg.py in <module>()
    141  scale_pos_weight=1,
    142  seed=27)
--> 143 modelfit(xgb1, X_train, y_train, X_test, y_test, predictors)
    144 
    145 '''

~/Desktop/風管碩一/bigdata/bigdata-final/final_xg.py in modelfit(alg, X_train, y_train, X_test, y_test, predictors, useTrainCV, cv_folds, early_stopping_rounds)
    125     print( " - train_acc : %.4g" % metrics.accuracy_score(y_train, X_pred))
    126     print( " - train auc score: %f" % metrics.roc_auc_score(y_train, X_predprob))
--> 127     print( " - text_acc : %.4g" % metrics.accuracy_score(y_text, X_textpred))
    128     print( " - text auc score: %f" % metrics.roc_auc_score(y_train, X_textpredprob))
    129 

NameError: name 'y_text' is not defined

In [38]: run final_xg.py
load data...
done

check df2 NaN...
False

select PAY as df3, delete PAY as df4...
done

add PAY column as df4...
done

delete ID...
done

one hot encodding 'SEX','EDUCATION','MARRIAGE' as X, and default as Y...
done

describe X Y...
            LIMIT_BAL           AGE      BILL_AMT1      BILL_AMT2  \
count    30000.000000  30000.000000   30000.000000   30000.000000   
mean    167484.322667     35.485500   51223.330900   49179.075167   
std     129747.661567      9.217904   73635.860576   71173.768783   
min      10000.000000     21.000000 -165580.000000  -69777.000000   
25%      50000.000000     28.000000    3558.750000    2984.750000   
50%     140000.000000     34.000000   22381.500000   21200.000000   
75%     240000.000000     41.000000   67091.000000   64006.250000   
max    1000000.000000     79.000000  964511.000000  983931.000000   

          BILL_AMT3      BILL_AMT4      BILL_AMT5      BILL_AMT6  \
count  3.000000e+04   30000.000000   30000.000000   30000.000000   
mean   4.701315e+04   43262.948967   40311.400967   38871.760400   
std    6.934939e+04   64332.856134   60797.155770   59554.107537   
min   -1.572640e+05 -170000.000000  -81334.000000 -339603.000000   
25%    2.666250e+03    2326.750000    1763.000000    1256.000000   
50%    2.008850e+04   19052.000000   18104.500000   17071.000000   
75%    6.016475e+04   54506.000000   50190.500000   49198.250000   
max    1.664089e+06  891586.000000  927171.000000  961664.000000   

            PAY_AMT1      PAY_AMT2      ...        EDUCATION_1   EDUCATION_2  \
count   30000.000000  3.000000e+04      ...       30000.000000  30000.000000   
mean     5663.580500  5.921163e+03      ...           0.352833      0.467667   
std     16563.280354  2.304087e+04      ...           0.477859      0.498962   
min         0.000000  0.000000e+00      ...           0.000000      0.000000   
25%      1000.000000  8.330000e+02      ...           0.000000      0.000000   
50%      2100.000000  2.009000e+03      ...           0.000000      0.000000   
75%      5006.000000  5.000000e+03      ...           1.000000      1.000000   
max    873552.000000  1.684259e+06      ...           1.000000      1.000000   

        EDUCATION_3   EDUCATION_4   EDUCATION_5   EDUCATION_6    MARRIAGE_0  \
count  30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   
mean       0.163900      0.004100      0.009333      0.001700      0.001800   
std        0.370191      0.063901      0.096159      0.041197      0.042389   
min        0.000000      0.000000      0.000000      0.000000      0.000000   
25%        0.000000      0.000000      0.000000      0.000000      0.000000   
50%        0.000000      0.000000      0.000000      0.000000      0.000000   
75%        0.000000      0.000000      0.000000      0.000000      0.000000   
max        1.000000      1.000000      1.000000      1.000000      1.000000   

         MARRIAGE_1    MARRIAGE_2    MARRIAGE_3  
count  30000.000000  30000.000000  30000.000000  
mean       0.455300      0.532133      0.010767  
std        0.498006      0.498975      0.103204  
min        0.000000      0.000000      0.000000  
25%        0.000000      0.000000      0.000000  
50%        0.000000      1.000000      0.000000  
75%        1.000000      1.000000      0.000000  
max        1.000000      1.000000      1.000000  

[8 rows x 57 columns]
count     30000
unique        2
top           0
freq      23364
Name: default.payment.next.month, dtype: int64
done

splitting training and testing...
done

[0]	train-auc:0.761262+0.00106989	test-auc:0.751539+0.00488465
[1]	train-auc:0.771072+0.00185415	test-auc:0.759632+0.00185325
[2]	train-auc:0.777287+0.00265697	test-auc:0.764952+0.00216168
[3]	train-auc:0.779081+0.00219126	test-auc:0.765777+0.00269409
[4]	train-auc:0.782317+0.00176664	test-auc:0.768202+0.00276387
[5]	train-auc:0.785277+0.00103477	test-auc:0.769761+0.00221473
[6]	train-auc:0.788356+0.00125676	test-auc:0.771692+0.00206396
[7]	train-auc:0.790856+0.00188279	test-auc:0.772367+0.00186716
[8]	train-auc:0.792833+0.00165655	test-auc:0.772775+0.00216506
[9]	train-auc:0.794917+0.00164328	test-auc:0.77385+0.00233364
[10]	train-auc:0.796622+0.0015293	test-auc:0.77423+0.00305224
[11]	train-auc:0.798815+0.00142007	test-auc:0.775441+0.00311652
[12]	train-auc:0.800324+0.00145673	test-auc:0.775295+0.00323133
[13]	train-auc:0.801791+0.0014255	test-auc:0.775846+0.00385736
[14]	train-auc:0.80374+0.00187819	test-auc:0.776232+0.00383317
[15]	train-auc:0.805217+0.00195188	test-auc:0.776311+0.00424359
[16]	train-auc:0.807167+0.00183541	test-auc:0.77661+0.00472907
[17]	train-auc:0.808973+0.001942	test-auc:0.776781+0.00482842
[18]	train-auc:0.8104+0.00212596	test-auc:0.777076+0.00485489
[19]	train-auc:0.812423+0.00166502	test-auc:0.777443+0.00452554
[20]	train-auc:0.81388+0.0015861	test-auc:0.777732+0.00485978
[21]	train-auc:0.815005+0.00134057	test-auc:0.777879+0.00490432
[22]	train-auc:0.816509+0.00093758	test-auc:0.778064+0.00491148
[23]	train-auc:0.817611+0.000986015	test-auc:0.778214+0.00506704
[24]	train-auc:0.819164+0.000986998	test-auc:0.778547+0.00505034
[25]	train-auc:0.820369+0.00104845	test-auc:0.778307+0.00521259
[26]	train-auc:0.821224+0.00105315	test-auc:0.778494+0.00492913
[27]	train-auc:0.822384+0.00125479	test-auc:0.778787+0.00465943
[28]	train-auc:0.823657+0.00124557	test-auc:0.778863+0.00481394
[29]	train-auc:0.825105+0.00091974	test-auc:0.778497+0.00507161
[30]	train-auc:0.826058+0.000993684	test-auc:0.77869+0.00507767
[31]	train-auc:0.827133+0.000886206	test-auc:0.778643+0.00529385
[32]	train-auc:0.82831+0.00119047	test-auc:0.778559+0.00520448
[33]	train-auc:0.829652+0.001014	test-auc:0.778474+0.00536931
[34]	train-auc:0.830549+0.000928494	test-auc:0.778603+0.00538919
[35]	train-auc:0.831642+0.0007777	test-auc:0.77809+0.00534666
[36]	train-auc:0.832837+0.000802202	test-auc:0.777954+0.00537673
[37]	train-auc:0.834043+0.00114053	test-auc:0.777999+0.00542321
[38]	train-auc:0.834797+0.00119331	test-auc:0.778003+0.00542023
[39]	train-auc:0.835528+0.00137669	test-auc:0.77771+0.00514332
[40]	train-auc:0.836414+0.0015504	test-auc:0.777668+0.00540553
[41]	train-auc:0.837697+0.00166275	test-auc:0.777478+0.00564631
[42]	train-auc:0.838602+0.00137998	test-auc:0.777545+0.00545228
[43]	train-auc:0.839423+0.00150458	test-auc:0.77734+0.00540111
[44]	train-auc:0.840251+0.0016553	test-auc:0.777056+0.00541721
[45]	train-auc:0.841032+0.00151952	test-auc:0.776903+0.00552714
[46]	train-auc:0.842416+0.00123419	test-auc:0.776931+0.00547897
[47]	train-auc:0.843512+0.00141006	test-auc:0.777229+0.0055038
[48]	train-auc:0.844827+0.00115044	test-auc:0.777254+0.00529381
[49]	train-auc:0.845792+0.00120588	test-auc:0.777061+0.00517828
[50]	train-auc:0.846577+0.00112457	test-auc:0.776817+0.0052354
[51]	train-auc:0.847455+0.00112683	test-auc:0.776735+0.00513873
[52]	train-auc:0.848632+0.000907655	test-auc:0.776613+0.00523467
[53]	train-auc:0.849646+0.000912566	test-auc:0.776656+0.00556627
[54]	train-auc:0.851077+0.000519955	test-auc:0.776737+0.00533132
[55]	train-auc:0.852118+0.000797402	test-auc:0.77672+0.00518615
[56]	train-auc:0.853153+0.00109477	test-auc:0.776834+0.00524513
[57]	train-auc:0.85403+0.00150844	test-auc:0.776908+0.00549926
[58]	train-auc:0.854923+0.00194525	test-auc:0.776741+0.00543823
[59]	train-auc:0.855495+0.0019886	test-auc:0.776448+0.00548919
[60]	train-auc:0.856161+0.00207332	test-auc:0.776458+0.00526273
[61]	train-auc:0.857153+0.00217077	test-auc:0.776527+0.00519921
[62]	train-auc:0.857855+0.00229718	test-auc:0.776484+0.00505106
[63]	train-auc:0.858904+0.00221223	test-auc:0.776084+0.0051435
[64]	train-auc:0.859652+0.00179282	test-auc:0.775979+0.00524496
[65]	train-auc:0.860567+0.00165686	test-auc:0.775808+0.00548017
[66]	train-auc:0.861528+0.00175336	test-auc:0.775426+0.00546745
[67]	train-auc:0.862268+0.00203945	test-auc:0.775335+0.00570137
[68]	train-auc:0.863046+0.00193002	test-auc:0.7753+0.00564819
[69]	train-auc:0.864089+0.00204119	test-auc:0.775481+0.00616737
[70]	train-auc:0.864931+0.0018587	test-auc:0.775463+0.00606444
[71]	train-auc:0.865598+0.00154562	test-auc:0.775253+0.00612503
[72]	train-auc:0.86598+0.00170449	test-auc:0.775303+0.00612545
[73]	train-auc:0.866796+0.00172111	test-auc:0.775359+0.00614684
[74]	train-auc:0.867627+0.00152281	test-auc:0.775144+0.00618126
[75]	train-auc:0.868435+0.00133425	test-auc:0.774911+0.00630157
[76]	train-auc:0.869088+0.00142047	test-auc:0.774827+0.00619966
[77]	train-auc:0.869918+0.00132253	test-auc:0.774823+0.00637433

Model Report
 - train_acc : 0.8313
 - train auc score: 0.817859
 - text_acc : 0.834
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/Desktop/風管碩一/bigdata/bigdata-final/final_xg.py in <module>()
    141  scale_pos_weight=1,
    142  seed=27)
--> 143 modelfit(xgb1, X_train, y_train, X_test, y_test, predictors)
    144 
    145 '''

~/Desktop/風管碩一/bigdata/bigdata-final/final_xg.py in modelfit(alg, X_train, y_train, X_test, y_test, predictors, useTrainCV, cv_folds, early_stopping_rounds)
    126     print( " - train auc score: %f" % metrics.roc_auc_score(y_train, X_predprob))
    127     print( " - text_acc : %.4g" % metrics.accuracy_score(y_test, X_testpred))
--> 128     print( " - text auc score: %f" % metrics.roc_auc_score(y_train, X_testpredprob))
    129 
    130 predictors = X_train.columns.values.tolist()

~/miniconda3/lib/python3.6/site-packages/sklearn/metrics/ranking.py in roc_auc_score(y_true, y_score, average, sample_weight)
    275     return _average_binary_score(
    276         _binary_roc_auc_score, y_true, y_score, average,
--> 277         sample_weight=sample_weight)
    278 
    279 

~/miniconda3/lib/python3.6/site-packages/sklearn/metrics/base.py in _average_binary_score(binary_metric, y_true, y_score, average, sample_weight)
     73 
     74     if y_type == "binary":
---> 75         return binary_metric(y_true, y_score, sample_weight=sample_weight)
     76 
     77     check_consistent_length(y_true, y_score, sample_weight)

~/miniconda3/lib/python3.6/site-packages/sklearn/metrics/ranking.py in _binary_roc_auc_score(y_true, y_score, sample_weight)
    270 
    271         fpr, tpr, tresholds = roc_curve(y_true, y_score,
--> 272                                         sample_weight=sample_weight)
    273         return auc(fpr, tpr, reorder=True)
    274 

~/miniconda3/lib/python3.6/site-packages/sklearn/metrics/ranking.py in roc_curve(y_true, y_score, pos_label, sample_weight, drop_intermediate)
    532     """
    533     fps, tps, thresholds = _binary_clf_curve(
--> 534         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)
    535 
    536     # Attempt to drop thresholds corresponding to points in between and

~/miniconda3/lib/python3.6/site-packages/sklearn/metrics/ranking.py in _binary_clf_curve(y_true, y_score, pos_label, sample_weight)
    318         raise ValueError("{0} format is not supported".format(y_type))
    319 
--> 320     check_consistent_length(y_true, y_score, sample_weight)
    321     y_true = column_or_1d(y_true)
    322     y_score = column_or_1d(y_score)

~/miniconda3/lib/python3.6/site-packages/sklearn/utils/validation.py in check_consistent_length(*arrays)
    202     if len(uniques) > 1:
    203         raise ValueError("Found input variables with inconsistent numbers of"
--> 204                          " samples: %r" % [int(l) for l in lengths])
    205 
    206 

ValueError: Found input variables with inconsistent numbers of samples: [27000, 3000]

In [39]: run final_xg.py
load data...
done

check df2 NaN...
False

select PAY as df3, delete PAY as df4...
done

add PAY column as df4...
done

delete ID...
done

one hot encodding 'SEX','EDUCATION','MARRIAGE' as X, and default as Y...
done

describe X Y...
            LIMIT_BAL           AGE      BILL_AMT1      BILL_AMT2  \
count    30000.000000  30000.000000   30000.000000   30000.000000   
mean    167484.322667     35.485500   51223.330900   49179.075167   
std     129747.661567      9.217904   73635.860576   71173.768783   
min      10000.000000     21.000000 -165580.000000  -69777.000000   
25%      50000.000000     28.000000    3558.750000    2984.750000   
50%     140000.000000     34.000000   22381.500000   21200.000000   
75%     240000.000000     41.000000   67091.000000   64006.250000   
max    1000000.000000     79.000000  964511.000000  983931.000000   

          BILL_AMT3      BILL_AMT4      BILL_AMT5      BILL_AMT6  \
count  3.000000e+04   30000.000000   30000.000000   30000.000000   
mean   4.701315e+04   43262.948967   40311.400967   38871.760400   
std    6.934939e+04   64332.856134   60797.155770   59554.107537   
min   -1.572640e+05 -170000.000000  -81334.000000 -339603.000000   
25%    2.666250e+03    2326.750000    1763.000000    1256.000000   
50%    2.008850e+04   19052.000000   18104.500000   17071.000000   
75%    6.016475e+04   54506.000000   50190.500000   49198.250000   
max    1.664089e+06  891586.000000  927171.000000  961664.000000   

            PAY_AMT1      PAY_AMT2      ...        EDUCATION_1   EDUCATION_2  \
count   30000.000000  3.000000e+04      ...       30000.000000  30000.000000   
mean     5663.580500  5.921163e+03      ...           0.352833      0.467667   
std     16563.280354  2.304087e+04      ...           0.477859      0.498962   
min         0.000000  0.000000e+00      ...           0.000000      0.000000   
25%      1000.000000  8.330000e+02      ...           0.000000      0.000000   
50%      2100.000000  2.009000e+03      ...           0.000000      0.000000   
75%      5006.000000  5.000000e+03      ...           1.000000      1.000000   
max    873552.000000  1.684259e+06      ...           1.000000      1.000000   

        EDUCATION_3   EDUCATION_4   EDUCATION_5   EDUCATION_6    MARRIAGE_0  \
count  30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   
mean       0.163900      0.004100      0.009333      0.001700      0.001800   
std        0.370191      0.063901      0.096159      0.041197      0.042389   
min        0.000000      0.000000      0.000000      0.000000      0.000000   
25%        0.000000      0.000000      0.000000      0.000000      0.000000   
50%        0.000000      0.000000      0.000000      0.000000      0.000000   
75%        0.000000      0.000000      0.000000      0.000000      0.000000   
max        1.000000      1.000000      1.000000      1.000000      1.000000   

         MARRIAGE_1    MARRIAGE_2    MARRIAGE_3  
count  30000.000000  30000.000000  30000.000000  
mean       0.455300      0.532133      0.010767  
std        0.498006      0.498975      0.103204  
min        0.000000      0.000000      0.000000  
25%        0.000000      0.000000      0.000000  
50%        0.000000      1.000000      0.000000  
75%        1.000000      1.000000      0.000000  
max        1.000000      1.000000      1.000000  

[8 rows x 57 columns]
count     30000
unique        2
top           0
freq      23364
Name: default.payment.next.month, dtype: int64
done

splitting training and testing...
done

[0]	train-auc:0.761262+0.00106989	test-auc:0.751539+0.00488465
[1]	train-auc:0.771072+0.00185415	test-auc:0.759632+0.00185325
[2]	train-auc:0.777287+0.00265697	test-auc:0.764952+0.00216168
[3]	train-auc:0.779081+0.00219126	test-auc:0.765777+0.00269409
[4]	train-auc:0.782317+0.00176664	test-auc:0.768202+0.00276387
[5]	train-auc:0.785277+0.00103477	test-auc:0.769761+0.00221473
[6]	train-auc:0.788356+0.00125676	test-auc:0.771692+0.00206396
[7]	train-auc:0.790856+0.00188279	test-auc:0.772367+0.00186716
[8]	train-auc:0.792833+0.00165655	test-auc:0.772775+0.00216506
[9]	train-auc:0.794917+0.00164328	test-auc:0.77385+0.00233364
[10]	train-auc:0.796622+0.0015293	test-auc:0.77423+0.00305224
[11]	train-auc:0.798815+0.00142007	test-auc:0.775441+0.00311652
[12]	train-auc:0.800324+0.00145673	test-auc:0.775295+0.00323133
[13]	train-auc:0.801791+0.0014255	test-auc:0.775846+0.00385736
[14]	train-auc:0.80374+0.00187819	test-auc:0.776232+0.00383317
[15]	train-auc:0.805217+0.00195188	test-auc:0.776311+0.00424359
[16]	train-auc:0.807167+0.00183541	test-auc:0.77661+0.00472907
[17]	train-auc:0.808973+0.001942	test-auc:0.776781+0.00482842
[18]	train-auc:0.8104+0.00212596	test-auc:0.777076+0.00485489
[19]	train-auc:0.812423+0.00166502	test-auc:0.777443+0.00452554
[20]	train-auc:0.81388+0.0015861	test-auc:0.777732+0.00485978
[21]	train-auc:0.815005+0.00134057	test-auc:0.777879+0.00490432
[22]	train-auc:0.816509+0.00093758	test-auc:0.778064+0.00491148
[23]	train-auc:0.817611+0.000986015	test-auc:0.778214+0.00506704
[24]	train-auc:0.819164+0.000986998	test-auc:0.778547+0.00505034
[25]	train-auc:0.820369+0.00104845	test-auc:0.778307+0.00521259
[26]	train-auc:0.821224+0.00105315	test-auc:0.778494+0.00492913
[27]	train-auc:0.822384+0.00125479	test-auc:0.778787+0.00465943
[28]	train-auc:0.823657+0.00124557	test-auc:0.778863+0.00481394
[29]	train-auc:0.825105+0.00091974	test-auc:0.778497+0.00507161
[30]	train-auc:0.826058+0.000993684	test-auc:0.77869+0.00507767
[31]	train-auc:0.827133+0.000886206	test-auc:0.778643+0.00529385
[32]	train-auc:0.82831+0.00119047	test-auc:0.778559+0.00520448
[33]	train-auc:0.829652+0.001014	test-auc:0.778474+0.00536931
[34]	train-auc:0.830549+0.000928494	test-auc:0.778603+0.00538919
[35]	train-auc:0.831642+0.0007777	test-auc:0.77809+0.00534666
[36]	train-auc:0.832837+0.000802202	test-auc:0.777954+0.00537673
[37]	train-auc:0.834043+0.00114053	test-auc:0.777999+0.00542321
[38]	train-auc:0.834797+0.00119331	test-auc:0.778003+0.00542023
[39]	train-auc:0.835528+0.00137669	test-auc:0.77771+0.00514332
[40]	train-auc:0.836414+0.0015504	test-auc:0.777668+0.00540553
[41]	train-auc:0.837697+0.00166275	test-auc:0.777478+0.00564631
[42]	train-auc:0.838602+0.00137998	test-auc:0.777545+0.00545228
[43]	train-auc:0.839423+0.00150458	test-auc:0.77734+0.00540111
[44]	train-auc:0.840251+0.0016553	test-auc:0.777056+0.00541721
[45]	train-auc:0.841032+0.00151952	test-auc:0.776903+0.00552714
[46]	train-auc:0.842416+0.00123419	test-auc:0.776931+0.00547897
[47]	train-auc:0.843512+0.00141006	test-auc:0.777229+0.0055038
[48]	train-auc:0.844827+0.00115044	test-auc:0.777254+0.00529381
[49]	train-auc:0.845792+0.00120588	test-auc:0.777061+0.00517828
[50]	train-auc:0.846577+0.00112457	test-auc:0.776817+0.0052354
[51]	train-auc:0.847455+0.00112683	test-auc:0.776735+0.00513873
[52]	train-auc:0.848632+0.000907655	test-auc:0.776613+0.00523467
[53]	train-auc:0.849646+0.000912566	test-auc:0.776656+0.00556627
[54]	train-auc:0.851077+0.000519955	test-auc:0.776737+0.00533132
[55]	train-auc:0.852118+0.000797402	test-auc:0.77672+0.00518615
[56]	train-auc:0.853153+0.00109477	test-auc:0.776834+0.00524513
[57]	train-auc:0.85403+0.00150844	test-auc:0.776908+0.00549926
[58]	train-auc:0.854923+0.00194525	test-auc:0.776741+0.00543823
[59]	train-auc:0.855495+0.0019886	test-auc:0.776448+0.00548919
[60]	train-auc:0.856161+0.00207332	test-auc:0.776458+0.00526273
[61]	train-auc:0.857153+0.00217077	test-auc:0.776527+0.00519921
[62]	train-auc:0.857855+0.00229718	test-auc:0.776484+0.00505106
[63]	train-auc:0.858904+0.00221223	test-auc:0.776084+0.0051435
[64]	train-auc:0.859652+0.00179282	test-auc:0.775979+0.00524496
[65]	train-auc:0.860567+0.00165686	test-auc:0.775808+0.00548017
[66]	train-auc:0.861528+0.00175336	test-auc:0.775426+0.00546745
[67]	train-auc:0.862268+0.00203945	test-auc:0.775335+0.00570137
[68]	train-auc:0.863046+0.00193002	test-auc:0.7753+0.00564819
[69]	train-auc:0.864089+0.00204119	test-auc:0.775481+0.00616737
[70]	train-auc:0.864931+0.0018587	test-auc:0.775463+0.00606444
[71]	train-auc:0.865598+0.00154562	test-auc:0.775253+0.00612503
[72]	train-auc:0.86598+0.00170449	test-auc:0.775303+0.00612545
[73]	train-auc:0.866796+0.00172111	test-auc:0.775359+0.00614684
[74]	train-auc:0.867627+0.00152281	test-auc:0.775144+0.00618126
[75]	train-auc:0.868435+0.00133425	test-auc:0.774911+0.00630157
[76]	train-auc:0.869088+0.00142047	test-auc:0.774827+0.00619966
[77]	train-auc:0.869918+0.00132253	test-auc:0.774823+0.00637433

Model Report
 - train_acc : 0.8313
 - train auc score: 0.817859
 - text_acc : 0.834
 - text auc score: 0.806029

In [40]: run final_xg.py
load data...
done

check df2 NaN...
False

select PAY as df3, delete PAY as df4...
done

add PAY column as df4...
done

delete ID...
done

one hot encodding 'SEX','EDUCATION','MARRIAGE' as X, and default as Y...
done

describe X Y...
            LIMIT_BAL           AGE      BILL_AMT1      BILL_AMT2  \
count    30000.000000  30000.000000   30000.000000   30000.000000   
mean    167484.322667     35.485500   51223.330900   49179.075167   
std     129747.661567      9.217904   73635.860576   71173.768783   
min      10000.000000     21.000000 -165580.000000  -69777.000000   
25%      50000.000000     28.000000    3558.750000    2984.750000   
50%     140000.000000     34.000000   22381.500000   21200.000000   
75%     240000.000000     41.000000   67091.000000   64006.250000   
max    1000000.000000     79.000000  964511.000000  983931.000000   

          BILL_AMT3      BILL_AMT4      BILL_AMT5      BILL_AMT6  \
count  3.000000e+04   30000.000000   30000.000000   30000.000000   
mean   4.701315e+04   43262.948967   40311.400967   38871.760400   
std    6.934939e+04   64332.856134   60797.155770   59554.107537   
min   -1.572640e+05 -170000.000000  -81334.000000 -339603.000000   
25%    2.666250e+03    2326.750000    1763.000000    1256.000000   
50%    2.008850e+04   19052.000000   18104.500000   17071.000000   
75%    6.016475e+04   54506.000000   50190.500000   49198.250000   
max    1.664089e+06  891586.000000  927171.000000  961664.000000   

            PAY_AMT1      PAY_AMT2      ...        EDUCATION_1   EDUCATION_2  \
count   30000.000000  3.000000e+04      ...       30000.000000  30000.000000   
mean     5663.580500  5.921163e+03      ...           0.352833      0.467667   
std     16563.280354  2.304087e+04      ...           0.477859      0.498962   
min         0.000000  0.000000e+00      ...           0.000000      0.000000   
25%      1000.000000  8.330000e+02      ...           0.000000      0.000000   
50%      2100.000000  2.009000e+03      ...           0.000000      0.000000   
75%      5006.000000  5.000000e+03      ...           1.000000      1.000000   
max    873552.000000  1.684259e+06      ...           1.000000      1.000000   

        EDUCATION_3   EDUCATION_4   EDUCATION_5   EDUCATION_6    MARRIAGE_0  \
count  30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   
mean       0.163900      0.004100      0.009333      0.001700      0.001800   
std        0.370191      0.063901      0.096159      0.041197      0.042389   
min        0.000000      0.000000      0.000000      0.000000      0.000000   
25%        0.000000      0.000000      0.000000      0.000000      0.000000   
50%        0.000000      0.000000      0.000000      0.000000      0.000000   
75%        0.000000      0.000000      0.000000      0.000000      0.000000   
max        1.000000      1.000000      1.000000      1.000000      1.000000   

         MARRIAGE_1    MARRIAGE_2    MARRIAGE_3  
count  30000.000000  30000.000000  30000.000000  
mean       0.455300      0.532133      0.010767  
std        0.498006      0.498975      0.103204  
min        0.000000      0.000000      0.000000  
25%        0.000000      0.000000      0.000000  
50%        0.000000      1.000000      0.000000  
75%        1.000000      1.000000      0.000000  
max        1.000000      1.000000      1.000000  

[8 rows x 57 columns]
count     30000
unique        2
top           0
freq      23364
Name: default.payment.next.month, dtype: int64
done

splitting training and testing...
done

[0]	train-auc:0.761262+0.00106989	test-auc:0.751539+0.00488465
[1]	train-auc:0.771072+0.00185415	test-auc:0.759632+0.00185325
[2]	train-auc:0.777287+0.00265697	test-auc:0.764952+0.00216168
[3]	train-auc:0.779081+0.00219126	test-auc:0.765777+0.00269409
[4]	train-auc:0.782317+0.00176664	test-auc:0.768202+0.00276387
[5]	train-auc:0.785277+0.00103477	test-auc:0.769761+0.00221473
[6]	train-auc:0.788356+0.00125676	test-auc:0.771692+0.00206396
[7]	train-auc:0.790856+0.00188279	test-auc:0.772367+0.00186716
[8]	train-auc:0.792833+0.00165655	test-auc:0.772775+0.00216506
[9]	train-auc:0.794917+0.00164328	test-auc:0.77385+0.00233364
[10]	train-auc:0.796622+0.0015293	test-auc:0.77423+0.00305224
[11]	train-auc:0.798815+0.00142007	test-auc:0.775441+0.00311652
[12]	train-auc:0.800324+0.00145673	test-auc:0.775295+0.00323133
[13]	train-auc:0.801791+0.0014255	test-auc:0.775846+0.00385736
[14]	train-auc:0.80374+0.00187819	test-auc:0.776232+0.00383317
[15]	train-auc:0.805217+0.00195188	test-auc:0.776311+0.00424359
[16]	train-auc:0.807167+0.00183541	test-auc:0.77661+0.00472907
[17]	train-auc:0.808973+0.001942	test-auc:0.776781+0.00482842
[18]	train-auc:0.8104+0.00212596	test-auc:0.777076+0.00485489
[19]	train-auc:0.812423+0.00166502	test-auc:0.777443+0.00452554
[20]	train-auc:0.81388+0.0015861	test-auc:0.777732+0.00485978
[21]	train-auc:0.815005+0.00134057	test-auc:0.777879+0.00490432
[22]	train-auc:0.816509+0.00093758	test-auc:0.778064+0.00491148
[23]	train-auc:0.817611+0.000986015	test-auc:0.778214+0.00506704
[24]	train-auc:0.819164+0.000986998	test-auc:0.778547+0.00505034
[25]	train-auc:0.820369+0.00104845	test-auc:0.778307+0.00521259
[26]	train-auc:0.821224+0.00105315	test-auc:0.778494+0.00492913
[27]	train-auc:0.822384+0.00125479	test-auc:0.778787+0.00465943
[28]	train-auc:0.823657+0.00124557	test-auc:0.778863+0.00481394
[29]	train-auc:0.825105+0.00091974	test-auc:0.778497+0.00507161
[30]	train-auc:0.826058+0.000993684	test-auc:0.77869+0.00507767
[31]	train-auc:0.827133+0.000886206	test-auc:0.778643+0.00529385
[32]	train-auc:0.82831+0.00119047	test-auc:0.778559+0.00520448
[33]	train-auc:0.829652+0.001014	test-auc:0.778474+0.00536931
[34]	train-auc:0.830549+0.000928494	test-auc:0.778603+0.00538919
[35]	train-auc:0.831642+0.0007777	test-auc:0.77809+0.00534666
[36]	train-auc:0.832837+0.000802202	test-auc:0.777954+0.00537673
[37]	train-auc:0.834043+0.00114053	test-auc:0.777999+0.00542321
[38]	train-auc:0.834797+0.00119331	test-auc:0.778003+0.00542023
[39]	train-auc:0.835528+0.00137669	test-auc:0.77771+0.00514332
[40]	train-auc:0.836414+0.0015504	test-auc:0.777668+0.00540553
[41]	train-auc:0.837697+0.00166275	test-auc:0.777478+0.00564631
[42]	train-auc:0.838602+0.00137998	test-auc:0.777545+0.00545228
[43]	train-auc:0.839423+0.00150458	test-auc:0.77734+0.00540111
[44]	train-auc:0.840251+0.0016553	test-auc:0.777056+0.00541721
[45]	train-auc:0.841032+0.00151952	test-auc:0.776903+0.00552714
[46]	train-auc:0.842416+0.00123419	test-auc:0.776931+0.00547897
[47]	train-auc:0.843512+0.00141006	test-auc:0.777229+0.0055038
[48]	train-auc:0.844827+0.00115044	test-auc:0.777254+0.00529381
[49]	train-auc:0.845792+0.00120588	test-auc:0.777061+0.00517828
[50]	train-auc:0.846577+0.00112457	test-auc:0.776817+0.0052354
[51]	train-auc:0.847455+0.00112683	test-auc:0.776735+0.00513873
[52]	train-auc:0.848632+0.000907655	test-auc:0.776613+0.00523467
[53]	train-auc:0.849646+0.000912566	test-auc:0.776656+0.00556627
[54]	train-auc:0.851077+0.000519955	test-auc:0.776737+0.00533132
[55]	train-auc:0.852118+0.000797402	test-auc:0.77672+0.00518615
[56]	train-auc:0.853153+0.00109477	test-auc:0.776834+0.00524513
[57]	train-auc:0.85403+0.00150844	test-auc:0.776908+0.00549926
[58]	train-auc:0.854923+0.00194525	test-auc:0.776741+0.00543823
[59]	train-auc:0.855495+0.0019886	test-auc:0.776448+0.00548919
[60]	train-auc:0.856161+0.00207332	test-auc:0.776458+0.00526273
[61]	train-auc:0.857153+0.00217077	test-auc:0.776527+0.00519921
[62]	train-auc:0.857855+0.00229718	test-auc:0.776484+0.00505106
[63]	train-auc:0.858904+0.00221223	test-auc:0.776084+0.0051435
[64]	train-auc:0.859652+0.00179282	test-auc:0.775979+0.00524496
[65]	train-auc:0.860567+0.00165686	test-auc:0.775808+0.00548017
[66]	train-auc:0.861528+0.00175336	test-auc:0.775426+0.00546745
[67]	train-auc:0.862268+0.00203945	test-auc:0.775335+0.00570137
[68]	train-auc:0.863046+0.00193002	test-auc:0.7753+0.00564819
[69]	train-auc:0.864089+0.00204119	test-auc:0.775481+0.00616737
[70]	train-auc:0.864931+0.0018587	test-auc:0.775463+0.00606444
[71]	train-auc:0.865598+0.00154562	test-auc:0.775253+0.00612503
[72]	train-auc:0.86598+0.00170449	test-auc:0.775303+0.00612545
[73]	train-auc:0.866796+0.00172111	test-auc:0.775359+0.00614684
[74]	train-auc:0.867627+0.00152281	test-auc:0.775144+0.00618126
[75]	train-auc:0.868435+0.00133425	test-auc:0.774911+0.00630157
[76]	train-auc:0.869088+0.00142047	test-auc:0.774827+0.00619966
[77]	train-auc:0.869918+0.00132253	test-auc:0.774823+0.00637433

Model Report
 - train_acc : 0.8313
 - train auc score: 0.817859
 - text_acc : 0.834
 - text auc score: 0.806029
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/Desktop/風管碩一/bigdata/bigdata-final/final_xg.py in <module>()
    150  min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,
    151  objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), 
--> 152  param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)
    153 gsearch1.fit(X_train,y_train)
    154 print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)

~/miniconda3/lib/python3.6/site-packages/sklearn/grid_search.py in __init__(self, estimator, param_grid, scoring, fit_params, n_jobs, iid, refit, cv, verbose, pre_dispatch, error_score)
    819             refit, cv, verbose, pre_dispatch, error_score)
    820         self.param_grid = param_grid
--> 821         _check_param_grid(param_grid)
    822 
    823     def fit(self, X, y=None):

~/miniconda3/lib/python3.6/site-packages/sklearn/grid_search.py in _check_param_grid(param_grid)
    349             if True not in check:
    350                 raise ValueError("Parameter values for parameter ({0}) need "
--> 351                                  "to be a sequence.".format(name))
    352 
    353             if len(v) == 0:

ValueError: Parameter values for parameter (max_depth) need to be a sequence.

In [41]: range(3,10,2)
Out[41]: range(3, 10, 2)

In [42]: range(2,3)
Out[42]: range(2, 3)

In [43]: xrange(2,3)
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-43-5b5b81dc6ac9> in <module>()
----> 1 xrange(2,3)

NameError: name 'xrange' is not defined

In [44]: range(2,3)
Out[44]: range(2, 3)

In [45]: print(range(2,3))
range(2, 3)

In [46]: for i in range(2,3):
    ...:     print i
  File "<ipython-input-46-fbd674d8bae1>", line 2
    print i
          ^
SyntaxError: Missing parentheses in call to 'print'. Did you mean print(int i)?


In [47]: for i in range(2,3):
    ...:     print (i)
    ...:     
2

In [48]: run final_xg.py
load data...
done

check df2 NaN...
False

select PAY as df3, delete PAY as df4...
done

add PAY column as df4...
done

delete ID...
done

one hot encodding 'SEX','EDUCATION','MARRIAGE' as X, and default as Y...
done

describe X Y...
            LIMIT_BAL           AGE      BILL_AMT1      BILL_AMT2  \
count    30000.000000  30000.000000   30000.000000   30000.000000   
mean    167484.322667     35.485500   51223.330900   49179.075167   
std     129747.661567      9.217904   73635.860576   71173.768783   
min      10000.000000     21.000000 -165580.000000  -69777.000000   
25%      50000.000000     28.000000    3558.750000    2984.750000   
50%     140000.000000     34.000000   22381.500000   21200.000000   
75%     240000.000000     41.000000   67091.000000   64006.250000   
max    1000000.000000     79.000000  964511.000000  983931.000000   

          BILL_AMT3      BILL_AMT4      BILL_AMT5      BILL_AMT6  \
count  3.000000e+04   30000.000000   30000.000000   30000.000000   
mean   4.701315e+04   43262.948967   40311.400967   38871.760400   
std    6.934939e+04   64332.856134   60797.155770   59554.107537   
min   -1.572640e+05 -170000.000000  -81334.000000 -339603.000000   
25%    2.666250e+03    2326.750000    1763.000000    1256.000000   
50%    2.008850e+04   19052.000000   18104.500000   17071.000000   
75%    6.016475e+04   54506.000000   50190.500000   49198.250000   
max    1.664089e+06  891586.000000  927171.000000  961664.000000   

            PAY_AMT1      PAY_AMT2      ...        EDUCATION_1   EDUCATION_2  \
count   30000.000000  3.000000e+04      ...       30000.000000  30000.000000   
mean     5663.580500  5.921163e+03      ...           0.352833      0.467667   
std     16563.280354  2.304087e+04      ...           0.477859      0.498962   
min         0.000000  0.000000e+00      ...           0.000000      0.000000   
25%      1000.000000  8.330000e+02      ...           0.000000      0.000000   
50%      2100.000000  2.009000e+03      ...           0.000000      0.000000   
75%      5006.000000  5.000000e+03      ...           1.000000      1.000000   
max    873552.000000  1.684259e+06      ...           1.000000      1.000000   

        EDUCATION_3   EDUCATION_4   EDUCATION_5   EDUCATION_6    MARRIAGE_0  \
count  30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   
mean       0.163900      0.004100      0.009333      0.001700      0.001800   
std        0.370191      0.063901      0.096159      0.041197      0.042389   
min        0.000000      0.000000      0.000000      0.000000      0.000000   
25%        0.000000      0.000000      0.000000      0.000000      0.000000   
50%        0.000000      0.000000      0.000000      0.000000      0.000000   
75%        0.000000      0.000000      0.000000      0.000000      0.000000   
max        1.000000      1.000000      1.000000      1.000000      1.000000   

         MARRIAGE_1    MARRIAGE_2    MARRIAGE_3  
count  30000.000000  30000.000000  30000.000000  
mean       0.455300      0.532133      0.010767  
std        0.498006      0.498975      0.103204  
min        0.000000      0.000000      0.000000  
25%        0.000000      0.000000      0.000000  
50%        0.000000      1.000000      0.000000  
75%        1.000000      1.000000      0.000000  
max        1.000000      1.000000      1.000000  

[8 rows x 57 columns]
count     30000
unique        2
top           0
freq      23364
Name: default.payment.next.month, dtype: int64
done

splitting training and testing...
done

[0]	train-auc:0.761262+0.00106989	test-auc:0.751539+0.00488465
[1]	train-auc:0.771072+0.00185415	test-auc:0.759632+0.00185325
[2]	train-auc:0.777287+0.00265697	test-auc:0.764952+0.00216168
[3]	train-auc:0.779081+0.00219126	test-auc:0.765777+0.00269409
[4]	train-auc:0.782317+0.00176664	test-auc:0.768202+0.00276387
[5]	train-auc:0.785277+0.00103477	test-auc:0.769761+0.00221473
[6]	train-auc:0.788356+0.00125676	test-auc:0.771692+0.00206396
[7]	train-auc:0.790856+0.00188279	test-auc:0.772367+0.00186716
[8]	train-auc:0.792833+0.00165655	test-auc:0.772775+0.00216506
[9]	train-auc:0.794917+0.00164328	test-auc:0.77385+0.00233364
[10]	train-auc:0.796622+0.0015293	test-auc:0.77423+0.00305224
[11]	train-auc:0.798815+0.00142007	test-auc:0.775441+0.00311652
[12]	train-auc:0.800324+0.00145673	test-auc:0.775295+0.00323133
[13]	train-auc:0.801791+0.0014255	test-auc:0.775846+0.00385736
[14]	train-auc:0.80374+0.00187819	test-auc:0.776232+0.00383317
[15]	train-auc:0.805217+0.00195188	test-auc:0.776311+0.00424359
[16]	train-auc:0.807167+0.00183541	test-auc:0.77661+0.00472907
[17]	train-auc:0.808973+0.001942	test-auc:0.776781+0.00482842
[18]	train-auc:0.8104+0.00212596	test-auc:0.777076+0.00485489
[19]	train-auc:0.812423+0.00166502	test-auc:0.777443+0.00452554
[20]	train-auc:0.81388+0.0015861	test-auc:0.777732+0.00485978
[21]	train-auc:0.815005+0.00134057	test-auc:0.777879+0.00490432
[22]	train-auc:0.816509+0.00093758	test-auc:0.778064+0.00491148
[23]	train-auc:0.817611+0.000986015	test-auc:0.778214+0.00506704
[24]	train-auc:0.819164+0.000986998	test-auc:0.778547+0.00505034
[25]	train-auc:0.820369+0.00104845	test-auc:0.778307+0.00521259
[26]	train-auc:0.821224+0.00105315	test-auc:0.778494+0.00492913
[27]	train-auc:0.822384+0.00125479	test-auc:0.778787+0.00465943
[28]	train-auc:0.823657+0.00124557	test-auc:0.778863+0.00481394
[29]	train-auc:0.825105+0.00091974	test-auc:0.778497+0.00507161
[30]	train-auc:0.826058+0.000993684	test-auc:0.77869+0.00507767
[31]	train-auc:0.827133+0.000886206	test-auc:0.778643+0.00529385
[32]	train-auc:0.82831+0.00119047	test-auc:0.778559+0.00520448
[33]	train-auc:0.829652+0.001014	test-auc:0.778474+0.00536931
[34]	train-auc:0.830549+0.000928494	test-auc:0.778603+0.00538919
[35]	train-auc:0.831642+0.0007777	test-auc:0.77809+0.00534666
[36]	train-auc:0.832837+0.000802202	test-auc:0.777954+0.00537673
[37]	train-auc:0.834043+0.00114053	test-auc:0.777999+0.00542321
[38]	train-auc:0.834797+0.00119331	test-auc:0.778003+0.00542023
[39]	train-auc:0.835528+0.00137669	test-auc:0.77771+0.00514332
[40]	train-auc:0.836414+0.0015504	test-auc:0.777668+0.00540553
[41]	train-auc:0.837697+0.00166275	test-auc:0.777478+0.00564631
[42]	train-auc:0.838602+0.00137998	test-auc:0.777545+0.00545228
[43]	train-auc:0.839423+0.00150458	test-auc:0.77734+0.00540111
[44]	train-auc:0.840251+0.0016553	test-auc:0.777056+0.00541721
[45]	train-auc:0.841032+0.00151952	test-auc:0.776903+0.00552714
[46]	train-auc:0.842416+0.00123419	test-auc:0.776931+0.00547897
[47]	train-auc:0.843512+0.00141006	test-auc:0.777229+0.0055038
[48]	train-auc:0.844827+0.00115044	test-auc:0.777254+0.00529381
[49]	train-auc:0.845792+0.00120588	test-auc:0.777061+0.00517828
[50]	train-auc:0.846577+0.00112457	test-auc:0.776817+0.0052354
[51]	train-auc:0.847455+0.00112683	test-auc:0.776735+0.00513873
[52]	train-auc:0.848632+0.000907655	test-auc:0.776613+0.00523467
[53]	train-auc:0.849646+0.000912566	test-auc:0.776656+0.00556627
[54]	train-auc:0.851077+0.000519955	test-auc:0.776737+0.00533132
[55]	train-auc:0.852118+0.000797402	test-auc:0.77672+0.00518615
[56]	train-auc:0.853153+0.00109477	test-auc:0.776834+0.00524513
[57]	train-auc:0.85403+0.00150844	test-auc:0.776908+0.00549926
[58]	train-auc:0.854923+0.00194525	test-auc:0.776741+0.00543823
[59]	train-auc:0.855495+0.0019886	test-auc:0.776448+0.00548919
[60]	train-auc:0.856161+0.00207332	test-auc:0.776458+0.00526273
[61]	train-auc:0.857153+0.00217077	test-auc:0.776527+0.00519921
[62]	train-auc:0.857855+0.00229718	test-auc:0.776484+0.00505106
[63]	train-auc:0.858904+0.00221223	test-auc:0.776084+0.0051435
[64]	train-auc:0.859652+0.00179282	test-auc:0.775979+0.00524496
[65]	train-auc:0.860567+0.00165686	test-auc:0.775808+0.00548017
[66]	train-auc:0.861528+0.00175336	test-auc:0.775426+0.00546745
[67]	train-auc:0.862268+0.00203945	test-auc:0.775335+0.00570137
[68]	train-auc:0.863046+0.00193002	test-auc:0.7753+0.00564819
[69]	train-auc:0.864089+0.00204119	test-auc:0.775481+0.00616737
[70]	train-auc:0.864931+0.0018587	test-auc:0.775463+0.00606444
[71]	train-auc:0.865598+0.00154562	test-auc:0.775253+0.00612503
[72]	train-auc:0.86598+0.00170449	test-auc:0.775303+0.00612545
[73]	train-auc:0.866796+0.00172111	test-auc:0.775359+0.00614684
[74]	train-auc:0.867627+0.00152281	test-auc:0.775144+0.00618126
[75]	train-auc:0.868435+0.00133425	test-auc:0.774911+0.00630157
[76]	train-auc:0.869088+0.00142047	test-auc:0.774827+0.00619966
[77]	train-auc:0.869918+0.00132253	test-auc:0.774823+0.00637433

Model Report
 - train_acc : 0.8313
 - train auc score: 0.817859
 - text_acc : 0.834
 - text auc score: 0.806029
^C---------------------------------------------------------------------------
KeyboardInterrupt                         Traceback (most recent call last)
~/Desktop/風管碩一/bigdata/bigdata-final/final_xg.py in <module>()
    151  objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), 
    152  param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5,verbose=1)
--> 153 gsearch1.fit(X_train,y_train)
    154 print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)
    155 

~/miniconda3/lib/python3.6/site-packages/sklearn/grid_search.py in fit(self, X, y)
    836 
    837         """
--> 838         return self._fit(X, y, ParameterGrid(self.param_grid))
    839 
    840 

~/miniconda3/lib/python3.6/site-packages/sklearn/grid_search.py in _fit(self, X, y, parameter_iterable)
    572                                     self.fit_params, return_parameters=True,
    573                                     error_score=self.error_score)
--> 574                 for parameters in parameter_iterable
    575                 for train, test in cv)
    576 

~/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self, iterable)
    787                 # consumption.
    788                 self._iterating = False
--> 789             self.retrieve()
    790             # Make sure that we get a last message telling us we are done
    791             elapsed_time = time.time() - self._start_time

~/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in retrieve(self)
    697             try:
    698                 if getattr(self._backend, 'supports_timeout', False):
--> 699                     self._output.extend(job.get(timeout=self.timeout))
    700                 else:
    701                     self._output.extend(job.get())

~/miniconda3/lib/python3.6/multiprocessing/pool.py in get(self, timeout)
    636 
    637     def get(self, timeout=None):
--> 638         self.wait(timeout)
    639         if not self.ready():
    640             raise TimeoutError

~/miniconda3/lib/python3.6/multiprocessing/pool.py in wait(self, timeout)
    633 
    634     def wait(self, timeout=None):
--> 635         self._event.wait(timeout)
    636 
    637     def get(self, timeout=None):

~/miniconda3/lib/python3.6/threading.py in wait(self, timeout)
    549             signaled = self._flag
    550             if not signaled:
--> 551                 signaled = self._cond.wait(timeout)
    552             return signaled
    553 

~/miniconda3/lib/python3.6/threading.py in wait(self, timeout)
    293         try:    # restore state no matter what (e.g., KeyboardInterrupt)
    294             if timeout is None:
--> 295                 waiter.acquire()
    296                 gotit = True
    297             else:

KeyboardInterrupt: 

In [49]: run final_xg.py
load data...
done

check df2 NaN...
False

select PAY as df3, delete PAY as df4...
done

add PAY column as df4...
done

delete ID...
done

one hot encodding 'SEX','EDUCATION','MARRIAGE' as X, and default as Y...
done

describe X Y...
            LIMIT_BAL           AGE      BILL_AMT1      BILL_AMT2  \
count    30000.000000  30000.000000   30000.000000   30000.000000   
mean    167484.322667     35.485500   51223.330900   49179.075167   
std     129747.661567      9.217904   73635.860576   71173.768783   
min      10000.000000     21.000000 -165580.000000  -69777.000000   
25%      50000.000000     28.000000    3558.750000    2984.750000   
50%     140000.000000     34.000000   22381.500000   21200.000000   
75%     240000.000000     41.000000   67091.000000   64006.250000   
max    1000000.000000     79.000000  964511.000000  983931.000000   

          BILL_AMT3      BILL_AMT4      BILL_AMT5      BILL_AMT6  \
count  3.000000e+04   30000.000000   30000.000000   30000.000000   
mean   4.701315e+04   43262.948967   40311.400967   38871.760400   
std    6.934939e+04   64332.856134   60797.155770   59554.107537   
min   -1.572640e+05 -170000.000000  -81334.000000 -339603.000000   
25%    2.666250e+03    2326.750000    1763.000000    1256.000000   
50%    2.008850e+04   19052.000000   18104.500000   17071.000000   
75%    6.016475e+04   54506.000000   50190.500000   49198.250000   
max    1.664089e+06  891586.000000  927171.000000  961664.000000   

            PAY_AMT1      PAY_AMT2      ...        EDUCATION_1   EDUCATION_2  \
count   30000.000000  3.000000e+04      ...       30000.000000  30000.000000   
mean     5663.580500  5.921163e+03      ...           0.352833      0.467667   
std     16563.280354  2.304087e+04      ...           0.477859      0.498962   
min         0.000000  0.000000e+00      ...           0.000000      0.000000   
25%      1000.000000  8.330000e+02      ...           0.000000      0.000000   
50%      2100.000000  2.009000e+03      ...           0.000000      0.000000   
75%      5006.000000  5.000000e+03      ...           1.000000      1.000000   
max    873552.000000  1.684259e+06      ...           1.000000      1.000000   

        EDUCATION_3   EDUCATION_4   EDUCATION_5   EDUCATION_6    MARRIAGE_0  \
count  30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   
mean       0.163900      0.004100      0.009333      0.001700      0.001800   
std        0.370191      0.063901      0.096159      0.041197      0.042389   
min        0.000000      0.000000      0.000000      0.000000      0.000000   
25%        0.000000      0.000000      0.000000      0.000000      0.000000   
50%        0.000000      0.000000      0.000000      0.000000      0.000000   
75%        0.000000      0.000000      0.000000      0.000000      0.000000   
max        1.000000      1.000000      1.000000      1.000000      1.000000   

         MARRIAGE_1    MARRIAGE_2    MARRIAGE_3  
count  30000.000000  30000.000000  30000.000000  
mean       0.455300      0.532133      0.010767  
std        0.498006      0.498975      0.103204  
min        0.000000      0.000000      0.000000  
25%        0.000000      0.000000      0.000000  
50%        0.000000      1.000000      0.000000  
75%        1.000000      1.000000      0.000000  
max        1.000000      1.000000      1.000000  

[8 rows x 57 columns]
count     30000
unique        2
top           0
freq      23364
Name: default.payment.next.month, dtype: int64
done

splitting training and testing...
done

[0]	train-auc:0.761262+0.00106989	test-auc:0.751539+0.00488465
[1]	train-auc:0.771072+0.00185415	test-auc:0.759632+0.00185325
[2]	train-auc:0.777287+0.00265697	test-auc:0.764952+0.00216168
[3]	train-auc:0.779081+0.00219126	test-auc:0.765777+0.00269409
[4]	train-auc:0.782317+0.00176664	test-auc:0.768202+0.00276387
[5]	train-auc:0.785277+0.00103477	test-auc:0.769761+0.00221473
[6]	train-auc:0.788356+0.00125676	test-auc:0.771692+0.00206396
[7]	train-auc:0.790856+0.00188279	test-auc:0.772367+0.00186716
[8]	train-auc:0.792833+0.00165655	test-auc:0.772775+0.00216506
[9]	train-auc:0.794917+0.00164328	test-auc:0.77385+0.00233364
[10]	train-auc:0.796622+0.0015293	test-auc:0.77423+0.00305224
[11]	train-auc:0.798815+0.00142007	test-auc:0.775441+0.00311652
[12]	train-auc:0.800324+0.00145673	test-auc:0.775295+0.00323133
[13]	train-auc:0.801791+0.0014255	test-auc:0.775846+0.00385736
[14]	train-auc:0.80374+0.00187819	test-auc:0.776232+0.00383317
[15]	train-auc:0.805217+0.00195188	test-auc:0.776311+0.00424359
[16]	train-auc:0.807167+0.00183541	test-auc:0.77661+0.00472907
[17]	train-auc:0.808973+0.001942	test-auc:0.776781+0.00482842
[18]	train-auc:0.8104+0.00212596	test-auc:0.777076+0.00485489
[19]	train-auc:0.812423+0.00166502	test-auc:0.777443+0.00452554
[20]	train-auc:0.81388+0.0015861	test-auc:0.777732+0.00485978
[21]	train-auc:0.815005+0.00134057	test-auc:0.777879+0.00490432
[22]	train-auc:0.816509+0.00093758	test-auc:0.778064+0.00491148
[23]	train-auc:0.817611+0.000986015	test-auc:0.778214+0.00506704
[24]	train-auc:0.819164+0.000986998	test-auc:0.778547+0.00505034
[25]	train-auc:0.820369+0.00104845	test-auc:0.778307+0.00521259
[26]	train-auc:0.821224+0.00105315	test-auc:0.778494+0.00492913
[27]	train-auc:0.822384+0.00125479	test-auc:0.778787+0.00465943
[28]	train-auc:0.823657+0.00124557	test-auc:0.778863+0.00481394
[29]	train-auc:0.825105+0.00091974	test-auc:0.778497+0.00507161
[30]	train-auc:0.826058+0.000993684	test-auc:0.77869+0.00507767
[31]	train-auc:0.827133+0.000886206	test-auc:0.778643+0.00529385
[32]	train-auc:0.82831+0.00119047	test-auc:0.778559+0.00520448
[33]	train-auc:0.829652+0.001014	test-auc:0.778474+0.00536931
[34]	train-auc:0.830549+0.000928494	test-auc:0.778603+0.00538919
[35]	train-auc:0.831642+0.0007777	test-auc:0.77809+0.00534666
[36]	train-auc:0.832837+0.000802202	test-auc:0.777954+0.00537673
[37]	train-auc:0.834043+0.00114053	test-auc:0.777999+0.00542321
[38]	train-auc:0.834797+0.00119331	test-auc:0.778003+0.00542023
[39]	train-auc:0.835528+0.00137669	test-auc:0.77771+0.00514332
[40]	train-auc:0.836414+0.0015504	test-auc:0.777668+0.00540553
[41]	train-auc:0.837697+0.00166275	test-auc:0.777478+0.00564631
[42]	train-auc:0.838602+0.00137998	test-auc:0.777545+0.00545228
[43]	train-auc:0.839423+0.00150458	test-auc:0.77734+0.00540111
[44]	train-auc:0.840251+0.0016553	test-auc:0.777056+0.00541721
[45]	train-auc:0.841032+0.00151952	test-auc:0.776903+0.00552714
[46]	train-auc:0.842416+0.00123419	test-auc:0.776931+0.00547897
[47]	train-auc:0.843512+0.00141006	test-auc:0.777229+0.0055038
[48]	train-auc:0.844827+0.00115044	test-auc:0.777254+0.00529381
[49]	train-auc:0.845792+0.00120588	test-auc:0.777061+0.00517828
[50]	train-auc:0.846577+0.00112457	test-auc:0.776817+0.0052354
[51]	train-auc:0.847455+0.00112683	test-auc:0.776735+0.00513873
[52]	train-auc:0.848632+0.000907655	test-auc:0.776613+0.00523467
[53]	train-auc:0.849646+0.000912566	test-auc:0.776656+0.00556627
[54]	train-auc:0.851077+0.000519955	test-auc:0.776737+0.00533132
[55]	train-auc:0.852118+0.000797402	test-auc:0.77672+0.00518615
[56]	train-auc:0.853153+0.00109477	test-auc:0.776834+0.00524513
[57]	train-auc:0.85403+0.00150844	test-auc:0.776908+0.00549926
[58]	train-auc:0.854923+0.00194525	test-auc:0.776741+0.00543823
[59]	train-auc:0.855495+0.0019886	test-auc:0.776448+0.00548919
[60]	train-auc:0.856161+0.00207332	test-auc:0.776458+0.00526273
[61]	train-auc:0.857153+0.00217077	test-auc:0.776527+0.00519921
[62]	train-auc:0.857855+0.00229718	test-auc:0.776484+0.00505106
[63]	train-auc:0.858904+0.00221223	test-auc:0.776084+0.0051435
[64]	train-auc:0.859652+0.00179282	test-auc:0.775979+0.00524496
[65]	train-auc:0.860567+0.00165686	test-auc:0.775808+0.00548017
[66]	train-auc:0.861528+0.00175336	test-auc:0.775426+0.00546745
[67]	train-auc:0.862268+0.00203945	test-auc:0.775335+0.00570137
[68]	train-auc:0.863046+0.00193002	test-auc:0.7753+0.00564819
[69]	train-auc:0.864089+0.00204119	test-auc:0.775481+0.00616737
[70]	train-auc:0.864931+0.0018587	test-auc:0.775463+0.00606444
[71]	train-auc:0.865598+0.00154562	test-auc:0.775253+0.00612503
[72]	train-auc:0.86598+0.00170449	test-auc:0.775303+0.00612545
[73]	train-auc:0.866796+0.00172111	test-auc:0.775359+0.00614684
[74]	train-auc:0.867627+0.00152281	test-auc:0.775144+0.00618126
[75]	train-auc:0.868435+0.00133425	test-auc:0.774911+0.00630157
[76]	train-auc:0.869088+0.00142047	test-auc:0.774827+0.00619966
[77]	train-auc:0.869918+0.00132253	test-auc:0.774823+0.00637433

Model Report
 - train_acc : 0.8313
 - train auc score: 0.817859
 - text_acc : 0.834
 - text auc score: 0.806029
Fitting 5 folds for each of 12 candidates, totalling 60 fits
^C---------------------------------------------------------------------------
KeyboardInterrupt                         Traceback (most recent call last)
~/Desktop/風管碩一/bigdata/bigdata-final/final_xg.py in <module>()
    151  objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), 
    152  param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5,verbose=10)
--> 153 gsearch1.fit(X_train,y_train)
    154 print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)
    155 

~/miniconda3/lib/python3.6/site-packages/sklearn/grid_search.py in fit(self, X, y)
    836 
    837         """
--> 838         return self._fit(X, y, ParameterGrid(self.param_grid))
    839 
    840 

~/miniconda3/lib/python3.6/site-packages/sklearn/grid_search.py in _fit(self, X, y, parameter_iterable)
    572                                     self.fit_params, return_parameters=True,
    573                                     error_score=self.error_score)
--> 574                 for parameters in parameter_iterable
    575                 for train, test in cv)
    576 

~/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self, iterable)
    787                 # consumption.
    788                 self._iterating = False
--> 789             self.retrieve()
    790             # Make sure that we get a last message telling us we are done
    791             elapsed_time = time.time() - self._start_time

~/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in retrieve(self)
    697             try:
    698                 if getattr(self._backend, 'supports_timeout', False):
--> 699                     self._output.extend(job.get(timeout=self.timeout))
    700                 else:
    701                     self._output.extend(job.get())

~/miniconda3/lib/python3.6/multiprocessing/pool.py in get(self, timeout)
    636 
    637     def get(self, timeout=None):
--> 638         self.wait(timeout)
    639         if not self.ready():
    640             raise TimeoutError

~/miniconda3/lib/python3.6/multiprocessing/pool.py in wait(self, timeout)
    633 
    634     def wait(self, timeout=None):
--> 635         self._event.wait(timeout)
    636 
    637     def get(self, timeout=None):

~/miniconda3/lib/python3.6/threading.py in wait(self, timeout)
    549             signaled = self._flag
    550             if not signaled:
--> 551                 signaled = self._cond.wait(timeout)
    552             return signaled
    553 

~/miniconda3/lib/python3.6/threading.py in wait(self, timeout)
    293         try:    # restore state no matter what (e.g., KeyboardInterrupt)
    294             if timeout is None:
--> 295                 waiter.acquire()
    296                 gotit = True
    297             else:

KeyboardInterrupt: 

In [50]: run final_xg.py
load data...
done

check df2 NaN...
False

select PAY as df3, delete PAY as df4...
done

add PAY column as df4...
done

delete ID...
done

one hot encodding 'SEX','EDUCATION','MARRIAGE' as X, and default as Y...
done

describe X Y...
            LIMIT_BAL           AGE      BILL_AMT1      BILL_AMT2  \
count    30000.000000  30000.000000   30000.000000   30000.000000   
mean    167484.322667     35.485500   51223.330900   49179.075167   
std     129747.661567      9.217904   73635.860576   71173.768783   
min      10000.000000     21.000000 -165580.000000  -69777.000000   
25%      50000.000000     28.000000    3558.750000    2984.750000   
50%     140000.000000     34.000000   22381.500000   21200.000000   
75%     240000.000000     41.000000   67091.000000   64006.250000   
max    1000000.000000     79.000000  964511.000000  983931.000000   

          BILL_AMT3      BILL_AMT4      BILL_AMT5      BILL_AMT6  \
count  3.000000e+04   30000.000000   30000.000000   30000.000000   
mean   4.701315e+04   43262.948967   40311.400967   38871.760400   
std    6.934939e+04   64332.856134   60797.155770   59554.107537   
min   -1.572640e+05 -170000.000000  -81334.000000 -339603.000000   
25%    2.666250e+03    2326.750000    1763.000000    1256.000000   
50%    2.008850e+04   19052.000000   18104.500000   17071.000000   
75%    6.016475e+04   54506.000000   50190.500000   49198.250000   
max    1.664089e+06  891586.000000  927171.000000  961664.000000   

            PAY_AMT1      PAY_AMT2      ...        EDUCATION_1   EDUCATION_2  \
count   30000.000000  3.000000e+04      ...       30000.000000  30000.000000   
mean     5663.580500  5.921163e+03      ...           0.352833      0.467667   
std     16563.280354  2.304087e+04      ...           0.477859      0.498962   
min         0.000000  0.000000e+00      ...           0.000000      0.000000   
25%      1000.000000  8.330000e+02      ...           0.000000      0.000000   
50%      2100.000000  2.009000e+03      ...           0.000000      0.000000   
75%      5006.000000  5.000000e+03      ...           1.000000      1.000000   
max    873552.000000  1.684259e+06      ...           1.000000      1.000000   

        EDUCATION_3   EDUCATION_4   EDUCATION_5   EDUCATION_6    MARRIAGE_0  \
count  30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   
mean       0.163900      0.004100      0.009333      0.001700      0.001800   
std        0.370191      0.063901      0.096159      0.041197      0.042389   
min        0.000000      0.000000      0.000000      0.000000      0.000000   
25%        0.000000      0.000000      0.000000      0.000000      0.000000   
50%        0.000000      0.000000      0.000000      0.000000      0.000000   
75%        0.000000      0.000000      0.000000      0.000000      0.000000   
max        1.000000      1.000000      1.000000      1.000000      1.000000   

         MARRIAGE_1    MARRIAGE_2    MARRIAGE_3  
count  30000.000000  30000.000000  30000.000000  
mean       0.455300      0.532133      0.010767  
std        0.498006      0.498975      0.103204  
min        0.000000      0.000000      0.000000  
25%        0.000000      0.000000      0.000000  
50%        0.000000      1.000000      0.000000  
75%        1.000000      1.000000      0.000000  
max        1.000000      1.000000      1.000000  

[8 rows x 57 columns]
count     30000
unique        2
top           0
freq      23364
Name: default.payment.next.month, dtype: int64
done

splitting training and testing...
done

[0]	train-auc:0.761262+0.00106989	test-auc:0.751539+0.00488465
[1]	train-auc:0.771072+0.00185415	test-auc:0.759632+0.00185325
[2]	train-auc:0.777287+0.00265697	test-auc:0.764952+0.00216168
[3]	train-auc:0.779081+0.00219126	test-auc:0.765777+0.00269409
[4]	train-auc:0.782317+0.00176664	test-auc:0.768202+0.00276387
[5]	train-auc:0.785277+0.00103477	test-auc:0.769761+0.00221473
[6]	train-auc:0.788356+0.00125676	test-auc:0.771692+0.00206396
[7]	train-auc:0.790856+0.00188279	test-auc:0.772367+0.00186716
[8]	train-auc:0.792833+0.00165655	test-auc:0.772775+0.00216506
[9]	train-auc:0.794917+0.00164328	test-auc:0.77385+0.00233364
[10]	train-auc:0.796622+0.0015293	test-auc:0.77423+0.00305224
[11]	train-auc:0.798815+0.00142007	test-auc:0.775441+0.00311652
[12]	train-auc:0.800324+0.00145673	test-auc:0.775295+0.00323133
[13]	train-auc:0.801791+0.0014255	test-auc:0.775846+0.00385736
[14]	train-auc:0.80374+0.00187819	test-auc:0.776232+0.00383317
[15]	train-auc:0.805217+0.00195188	test-auc:0.776311+0.00424359
[16]	train-auc:0.807167+0.00183541	test-auc:0.77661+0.00472907
[17]	train-auc:0.808973+0.001942	test-auc:0.776781+0.00482842
[18]	train-auc:0.8104+0.00212596	test-auc:0.777076+0.00485489
[19]	train-auc:0.812423+0.00166502	test-auc:0.777443+0.00452554
[20]	train-auc:0.81388+0.0015861	test-auc:0.777732+0.00485978
[21]	train-auc:0.815005+0.00134057	test-auc:0.777879+0.00490432
[22]	train-auc:0.816509+0.00093758	test-auc:0.778064+0.00491148
[23]	train-auc:0.817611+0.000986015	test-auc:0.778214+0.00506704
[24]	train-auc:0.819164+0.000986998	test-auc:0.778547+0.00505034
[25]	train-auc:0.820369+0.00104845	test-auc:0.778307+0.00521259
[26]	train-auc:0.821224+0.00105315	test-auc:0.778494+0.00492913
[27]	train-auc:0.822384+0.00125479	test-auc:0.778787+0.00465943
[28]	train-auc:0.823657+0.00124557	test-auc:0.778863+0.00481394
[29]	train-auc:0.825105+0.00091974	test-auc:0.778497+0.00507161
[30]	train-auc:0.826058+0.000993684	test-auc:0.77869+0.00507767
[31]	train-auc:0.827133+0.000886206	test-auc:0.778643+0.00529385
[32]	train-auc:0.82831+0.00119047	test-auc:0.778559+0.00520448
[33]	train-auc:0.829652+0.001014	test-auc:0.778474+0.00536931
[34]	train-auc:0.830549+0.000928494	test-auc:0.778603+0.00538919
[35]	train-auc:0.831642+0.0007777	test-auc:0.77809+0.00534666
[36]	train-auc:0.832837+0.000802202	test-auc:0.777954+0.00537673
[37]	train-auc:0.834043+0.00114053	test-auc:0.777999+0.00542321
[38]	train-auc:0.834797+0.00119331	test-auc:0.778003+0.00542023
[39]	train-auc:0.835528+0.00137669	test-auc:0.77771+0.00514332
[40]	train-auc:0.836414+0.0015504	test-auc:0.777668+0.00540553
[41]	train-auc:0.837697+0.00166275	test-auc:0.777478+0.00564631
[42]	train-auc:0.838602+0.00137998	test-auc:0.777545+0.00545228
[43]	train-auc:0.839423+0.00150458	test-auc:0.77734+0.00540111
[44]	train-auc:0.840251+0.0016553	test-auc:0.777056+0.00541721
[45]	train-auc:0.841032+0.00151952	test-auc:0.776903+0.00552714
[46]	train-auc:0.842416+0.00123419	test-auc:0.776931+0.00547897
[47]	train-auc:0.843512+0.00141006	test-auc:0.777229+0.0055038
[48]	train-auc:0.844827+0.00115044	test-auc:0.777254+0.00529381
[49]	train-auc:0.845792+0.00120588	test-auc:0.777061+0.00517828
[50]	train-auc:0.846577+0.00112457	test-auc:0.776817+0.0052354
[51]	train-auc:0.847455+0.00112683	test-auc:0.776735+0.00513873
[52]	train-auc:0.848632+0.000907655	test-auc:0.776613+0.00523467
[53]	train-auc:0.849646+0.000912566	test-auc:0.776656+0.00556627
[54]	train-auc:0.851077+0.000519955	test-auc:0.776737+0.00533132
[55]	train-auc:0.852118+0.000797402	test-auc:0.77672+0.00518615
[56]	train-auc:0.853153+0.00109477	test-auc:0.776834+0.00524513
[57]	train-auc:0.85403+0.00150844	test-auc:0.776908+0.00549926
[58]	train-auc:0.854923+0.00194525	test-auc:0.776741+0.00543823
[59]	train-auc:0.855495+0.0019886	test-auc:0.776448+0.00548919
[60]	train-auc:0.856161+0.00207332	test-auc:0.776458+0.00526273
[61]	train-auc:0.857153+0.00217077	test-auc:0.776527+0.00519921
[62]	train-auc:0.857855+0.00229718	test-auc:0.776484+0.00505106
[63]	train-auc:0.858904+0.00221223	test-auc:0.776084+0.0051435
[64]	train-auc:0.859652+0.00179282	test-auc:0.775979+0.00524496
[65]	train-auc:0.860567+0.00165686	test-auc:0.775808+0.00548017
[66]	train-auc:0.861528+0.00175336	test-auc:0.775426+0.00546745
[67]	train-auc:0.862268+0.00203945	test-auc:0.775335+0.00570137
[68]	train-auc:0.863046+0.00193002	test-auc:0.7753+0.00564819
[69]	train-auc:0.864089+0.00204119	test-auc:0.775481+0.00616737
[70]	train-auc:0.864931+0.0018587	test-auc:0.775463+0.00606444
[71]	train-auc:0.865598+0.00154562	test-auc:0.775253+0.00612503
[72]	train-auc:0.86598+0.00170449	test-auc:0.775303+0.00612545
[73]	train-auc:0.866796+0.00172111	test-auc:0.775359+0.00614684
[74]	train-auc:0.867627+0.00152281	test-auc:0.775144+0.00618126
[75]	train-auc:0.868435+0.00133425	test-auc:0.774911+0.00630157
[76]	train-auc:0.869088+0.00142047	test-auc:0.774827+0.00619966
[77]	train-auc:0.869918+0.00132253	test-auc:0.774823+0.00637433

Model Report
 - train_acc : 0.8313
 - train auc score: 0.817859
 - text_acc : 0.834
 - text auc score: 0.806029
Fitting 5 folds for each of 12 candidates, totalling 60 fits
[CV] max_depth=3, min_child_weight=1 .................................
[CV] max_depth=3, min_child_weight=1 .................................
[CV] max_depth=3, min_child_weight=1 .................................
[CV] max_depth=3, min_child_weight=1 .................................
[CV] ........ max_depth=3, min_child_weight=1, score=0.768860 -  10.6s
[CV] ........ max_depth=3, min_child_weight=1, score=0.782882 -  10.6s
[CV] max_depth=3, min_child_weight=1 .................................
[CV] max_depth=3, min_child_weight=3 .................................
[CV] ........ max_depth=3, min_child_weight=1, score=0.775405 -  10.5s
[CV] ........ max_depth=3, min_child_weight=1, score=0.776595 -  10.6s
[CV] max_depth=3, min_child_weight=3 .................................
[CV] max_depth=3, min_child_weight=3 .................................
[CV] ........ max_depth=3, min_child_weight=3, score=0.770402 -  10.7s
[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:   21.5s
[CV] ........ max_depth=3, min_child_weight=3, score=0.783504 -  10.6s
[CV] max_depth=3, min_child_weight=3 .................................
[CV] max_depth=3, min_child_weight=3 .................................
[CV] ........ max_depth=3, min_child_weight=3, score=0.777891 -  10.6s
[CV] max_depth=3, min_child_weight=5 .................................
[CV] ........ max_depth=3, min_child_weight=1, score=0.772956 -  10.9s
[CV] max_depth=3, min_child_weight=5 .................................
[CV] ........ max_depth=3, min_child_weight=3, score=0.777397 -  10.5s
[CV] max_depth=3, min_child_weight=5 .................................
[CV] ........ max_depth=3, min_child_weight=5, score=0.781109 -  10.4s
[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:   32.2s
[CV] ........ max_depth=3, min_child_weight=5, score=0.773986 -  10.5s
[CV] max_depth=3, min_child_weight=5 .................................
[CV] max_depth=3, min_child_weight=5 .................................
[CV] ........ max_depth=3, min_child_weight=3, score=0.772618 -  10.7s
[CV] max_depth=5, min_child_weight=1 .................................
[CV] ........ max_depth=3, min_child_weight=5, score=0.777142 -  10.9s
[CV] max_depth=5, min_child_weight=1 .................................
[CV] ........ max_depth=3, min_child_weight=5, score=0.773129 -  10.9s
[CV] ........ max_depth=3, min_child_weight=5, score=0.771942 -  11.0s
[CV] max_depth=5, min_child_weight=1 .................................
[CV] max_depth=5, min_child_weight=1 .................................
[CV] ........ max_depth=5, min_child_weight=1, score=0.758650 -  17.5s
[CV] max_depth=5, min_child_weight=1 .................................
[CV] ........ max_depth=5, min_child_weight=1, score=0.775582 -  17.2s
[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:  1.0min
[CV] max_depth=5, min_child_weight=3 .................................
[CV] ........ max_depth=5, min_child_weight=1, score=0.775729 -  17.1s
[CV] max_depth=5, min_child_weight=3 .................................
[CV] ........ max_depth=5, min_child_weight=1, score=0.770287 -  17.2s
[CV] max_depth=5, min_child_weight=3 .................................
[CV] ........ max_depth=5, min_child_weight=1, score=0.766407 -  17.3s
[CV] max_depth=5, min_child_weight=3 .................................
[CV] ........ max_depth=5, min_child_weight=3, score=0.762079 -  17.3s
[CV] ........ max_depth=5, min_child_weight=3, score=0.773383 -  17.2s
[CV] max_depth=5, min_child_weight=3 .................................
[CV] max_depth=5, min_child_weight=5 .................................
[CV] ........ max_depth=5, min_child_weight=3, score=0.770599 -  17.3s
[CV] max_depth=5, min_child_weight=5 .................................
[CV] ........ max_depth=5, min_child_weight=3, score=0.764597 -  17.4s
[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:  1.4min
[CV] max_depth=5, min_child_weight=5 .................................
[CV] ........ max_depth=5, min_child_weight=3, score=0.765308 -  17.2s
[CV] ........ max_depth=5, min_child_weight=5, score=0.772145 -  17.0s
[CV] max_depth=5, min_child_weight=5 .................................
[CV] max_depth=5, min_child_weight=5 .................................
[CV] ........ max_depth=5, min_child_weight=5, score=0.759396 -  17.3s
[CV] max_depth=7, min_child_weight=1 .................................
[CV] ........ max_depth=5, min_child_weight=5, score=0.770067 -  16.7s
[CV] max_depth=7, min_child_weight=1 .................................
[CV] ........ max_depth=5, min_child_weight=5, score=0.762503 -  16.7s
[CV] max_depth=7, min_child_weight=1 .................................
[CV] ........ max_depth=5, min_child_weight=5, score=0.767746 -  16.7s
[CV] max_depth=7, min_child_weight=1 .................................
[CV] ........ max_depth=7, min_child_weight=1, score=0.751891 -  23.3s
[CV] max_depth=7, min_child_weight=1 .................................
[CV] ........ max_depth=7, min_child_weight=1, score=0.765737 -  23.5s
[CV] max_depth=7, min_child_weight=3 .................................
[CV] ........ max_depth=7, min_child_weight=1, score=0.761661 -  23.8s
[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:  2.3min
[CV] max_depth=7, min_child_weight=3 .................................
[CV] ........ max_depth=7, min_child_weight=1, score=0.765288 -  23.9s
[CV] max_depth=7, min_child_weight=3 .................................
[CV] ........ max_depth=7, min_child_weight=1, score=0.756342 -  24.2s
[CV] max_depth=7, min_child_weight=3 .................................
[CV] ........ max_depth=7, min_child_weight=3, score=0.754820 -  23.9s
[CV] max_depth=7, min_child_weight=3 .................................
[CV] ........ max_depth=7, min_child_weight=3, score=0.762686 -  23.6s
[CV] max_depth=7, min_child_weight=5 .................................
[CV] ........ max_depth=7, min_child_weight=3, score=0.764005 -  23.6s
[CV] max_depth=7, min_child_weight=5 .................................
[CV] ........ max_depth=7, min_child_weight=3, score=0.757155 -  23.2s
[CV] max_depth=7, min_child_weight=5 .................................
[CV] ........ max_depth=7, min_child_weight=3, score=0.758042 -  23.3s
[CV] max_depth=7, min_child_weight=5 .................................
[CV] ........ max_depth=7, min_child_weight=5, score=0.761094 -  23.4s
[CV] max_depth=7, min_child_weight=5 .................................
[CV] ........ max_depth=7, min_child_weight=5, score=0.750718 -  23.6s
[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  3.0min
[CV] max_depth=9, min_child_weight=1 .................................
[CV] ........ max_depth=7, min_child_weight=5, score=0.762468 -  23.4s
[CV] max_depth=9, min_child_weight=1 .................................
[CV] ........ max_depth=7, min_child_weight=5, score=0.761286 -  23.4s
[CV] max_depth=9, min_child_weight=1 .................................
[CV] ........ max_depth=7, min_child_weight=5, score=0.759010 -  23.0s
[CV] max_depth=9, min_child_weight=1 .................................
[CV] ........ max_depth=9, min_child_weight=1, score=0.748502 -  31.0s
[CV] max_depth=9, min_child_weight=1 .................................
[CV] ........ max_depth=9, min_child_weight=1, score=0.762668 -  31.0s
[CV] max_depth=9, min_child_weight=3 .................................
[CV] ........ max_depth=9, min_child_weight=1, score=0.764249 -  31.2s
[CV] max_depth=9, min_child_weight=3 .................................
[CV] ........ max_depth=9, min_child_weight=1, score=0.753436 -  31.4s
[CV] max_depth=9, min_child_weight=3 .................................
[CV] ........ max_depth=9, min_child_weight=1, score=0.753723 -  30.9s
[CV] max_depth=9, min_child_weight=3 .................................
[CV] ........ max_depth=9, min_child_weight=3, score=0.740872 -  30.7s
[CV] max_depth=9, min_child_weight=3 .................................
[CV] ........ max_depth=9, min_child_weight=3, score=0.761133 -  30.3s
[CV] max_depth=9, min_child_weight=5 .................................
[CV] ........ max_depth=9, min_child_weight=3, score=0.763031 -  30.0s
[Parallel(n_jobs=4)]: Done  53 tasks      | elapsed:  4.5min
[CV] max_depth=9, min_child_weight=5 .................................
[CV] ........ max_depth=9, min_child_weight=3, score=0.752507 -  30.0s
[CV] max_depth=9, min_child_weight=5 .................................
[CV] ........ max_depth=9, min_child_weight=3, score=0.750949 -  30.0s
[CV] max_depth=9, min_child_weight=5 .................................
[CV] ........ max_depth=9, min_child_weight=5, score=0.744578 -  29.6s
[CV] max_depth=9, min_child_weight=5 .................................
[CV] ........ max_depth=9, min_child_weight=5, score=0.758818 -  31.2s
[CV] ........ max_depth=9, min_child_weight=5, score=0.757560 -  29.8s
[CV] ........ max_depth=9, min_child_weight=5, score=0.754622 -  27.5s
[CV] ........ max_depth=9, min_child_weight=5, score=0.752130 -  25.3s
[Parallel(n_jobs=4)]: Done  60 out of  60 | elapsed:  5.2min remaining:    0.0s
[Parallel(n_jobs=4)]: Done  60 out of  60 | elapsed:  5.2min finished
[mean: 0.77534, std: 0.00461, params: {'max_depth': 3, 'min_child_weight': 1}, mean: 0.77636, std: 0.00456, params: {'max_depth': 3, 'min_child_weight': 3}, mean: 0.77546, std: 0.00331, params: {'max_depth': 3, 'min_child_weight': 5}, mean: 0.76933, std: 0.00638, params: {'max_depth': 5, 'min_child_weight': 1}, mean: 0.76719, std: 0.00416, params: {'max_depth': 5, 'min_child_weight': 3}, mean: 0.76637, std: 0.00474, params: {'max_depth': 5, 'min_child_weight': 5}, mean: 0.76018, std: 0.00534, params: {'max_depth': 7, 'min_child_weight': 1}, mean: 0.75934, std: 0.00346, params: {'max_depth': 7, 'min_child_weight': 3}, mean: 0.75892, std: 0.00425, params: {'max_depth': 7, 'min_child_weight': 5}, mean: 0.75652, std: 0.00599, params: {'max_depth': 9, 'min_child_weight': 1}, mean: 0.75370, std: 0.00795, params: {'max_depth': 9, 'min_child_weight': 3}, mean: 0.75354, std: 0.00505, params: {'max_depth': 9, 'min_child_weight': 5}] {'max_depth': 3, 'min_child_weight': 3} 0.7763624312086403

In [51]: param_test2 = {
    ...:  'max_depth’:[2,3,4],
    ...:  'min_child_weight’:[2,3,4]
    ...: }
    ...: gsearch2 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.2, n_estimators=140, max_depth=5,
    ...:  min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,
    ...:  objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), 
    ...:  param_grid = param_test2, scoring='roc_auc',n_jobs=4,iid=False, cv=5,verbose=10)
    ...: gsearch2.fit(X_train,y_train)
    ...: print(gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_)
  File "<ipython-input-51-e29119abae73>", line 2
    'max_depth’:[2,3,4],
                        ^
SyntaxError: EOL while scanning string literal


In [52]: param_test2 = {
    ...:  'max_depth’:[2,3,4],
    ...:  'min_child_weight’:[2,3,4]
    ...: }
    ...: gsearch2 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.2, n_estimators=140, max_depth=5,
    ...:  min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,
    ...:  objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), 
    ...:  param_grid = param_test2, scoring='roc_auc',n_jobs=4,iid=False, cv=5,verbose=10)
    ...: gsearch2.fit(X_train,y_train)
    ...: print(gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_)
  File "<ipython-input-52-e29119abae73>", line 2
    'max_depth’:[2,3,4],
                        ^
SyntaxError: EOL while scanning string literal


In [53]: param_test2 = {
    ...:  'max_depth':[2,3,4],
    ...:  'min_child_weight’:[2,3,4]
    ...: }
    ...: gsearch2 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.2, n_estimators=140, max_depth=5,
    ...:  min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,
    ...:  objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), 
    ...:  param_grid = param_test2, scoring='roc_auc',n_jobs=4,iid=False, cv=5,verbose=10)
    ...: gsearch2.fit(X_train,y_train)
    ...: print(gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_)
  File "<ipython-input-53-6262b37ce366>", line 3
    'min_child_weight’:[2,3,4]
                              ^
SyntaxError: EOL while scanning string literal


In [54]: param_test2 = {
    ...:  'max_depth':[2,3,4],\
    ...:  'min_child_weight’:[2,3,4]
    ...: }
    ...: gsearch2 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.2, n_estimators=140, max_depth=5,
    ...:  min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,
    ...:  objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), 
    ...:  param_grid = param_test2, scoring='roc_auc',n_jobs=4,iid=False, cv=5,verbose=10)
    ...: gsearch2.fit(X_train,y_train)
    ...: print(gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_)
  File "<ipython-input-54-a9d9ef8c7dfe>", line 3
    'min_child_weight’:[2,3,4]
                              ^
SyntaxError: EOL while scanning string literal


In [55]: param_test2 = {
    ...:  'max_depth':[2,3,4],
    ...:  'min_child_weight':[2,3,4]
    ...: }
    ...: gsearch2 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.2, n_estimators=140, max_depth=5,
    ...:  min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,
    ...:  objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), 
    ...:  param_grid = param_test2, scoring='roc_auc',n_jobs=4,iid=False, cv=5,verbose=10)
    ...: gsearch2.fit(X_train,y_train)
    ...: print(gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_)
    ...:  
Fitting 5 folds for each of 9 candidates, totalling 45 fits
[CV] max_depth=2, min_child_weight=2 .................................
[CV] max_depth=2, min_child_weight=2 .................................
[CV] max_depth=2, min_child_weight=2 .................................
[CV] max_depth=2, min_child_weight=2 .................................
[CV] ........ max_depth=2, min_child_weight=2, score=0.774327 -   7.7s
[CV] max_depth=2, min_child_weight=2 .................................
[CV] ........ max_depth=2, min_child_weight=2, score=0.783353 -   7.8s
[CV] ........ max_depth=2, min_child_weight=2, score=0.779119 -   7.7s
[CV] max_depth=2, min_child_weight=3 .................................
[CV] max_depth=2, min_child_weight=3 .................................
[CV] ........ max_depth=2, min_child_weight=2, score=0.779222 -   7.8s
[CV] max_depth=2, min_child_weight=3 .................................
[CV] ........ max_depth=2, min_child_weight=3, score=0.784342 -   7.2s
[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:   15.4s
[CV] max_depth=2, min_child_weight=3 .................................
[CV] ........ max_depth=2, min_child_weight=2, score=0.773652 -   7.6s
[CV] ........ max_depth=2, min_child_weight=3, score=0.774000 -   7.4s
[CV] max_depth=2, min_child_weight=3 .................................
[CV] max_depth=2, min_child_weight=4 .................................
[CV] ........ max_depth=2, min_child_weight=3, score=0.779865 -   7.3s
[CV] max_depth=2, min_child_weight=4 .................................
[CV] ........ max_depth=2, min_child_weight=3, score=0.780261 -   7.2s
[CV] max_depth=2, min_child_weight=4 .................................
[CV] ........ max_depth=2, min_child_weight=4, score=0.776272 -   7.2s
[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:   22.8s
[CV] max_depth=2, min_child_weight=4 .................................
[CV] ........ max_depth=2, min_child_weight=3, score=0.774711 -   7.4s
[CV] max_depth=2, min_child_weight=4 .................................
[CV] ........ max_depth=2, min_child_weight=4, score=0.783595 -   7.3s
[CV] max_depth=3, min_child_weight=2 .................................
[CV] ........ max_depth=2, min_child_weight=4, score=0.779826 -   7.2s
[CV] max_depth=3, min_child_weight=2 .................................
[CV] ........ max_depth=2, min_child_weight=4, score=0.779573 -   7.4s
[CV] max_depth=3, min_child_weight=2 .................................
[CV] ........ max_depth=2, min_child_weight=4, score=0.774783 -   7.4s
[CV] max_depth=3, min_child_weight=2 .................................
[CV] ........ max_depth=3, min_child_weight=2, score=0.772481 -  10.6s
[CV] max_depth=3, min_child_weight=2 .................................
[CV] ........ max_depth=3, min_child_weight=2, score=0.782075 -  10.8s
[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:   40.9s
[CV] max_depth=3, min_child_weight=3 .................................
[CV] ........ max_depth=3, min_child_weight=2, score=0.774393 -  10.7s
[CV] max_depth=3, min_child_weight=3 .................................
[CV] ........ max_depth=3, min_child_weight=2, score=0.779202 -  11.0s
[CV] max_depth=3, min_child_weight=3 .................................
[CV] ........ max_depth=3, min_child_weight=2, score=0.775159 -  10.8s
[CV] max_depth=3, min_child_weight=3 .................................
[CV] ........ max_depth=3, min_child_weight=3, score=0.770402 -  10.4s
[CV] max_depth=3, min_child_weight=3 .................................
[CV] ........ max_depth=3, min_child_weight=3, score=0.783504 -  10.4s
[CV] max_depth=3, min_child_weight=4 .................................
[CV] ........ max_depth=3, min_child_weight=3, score=0.777891 -  10.5s
[CV] max_depth=3, min_child_weight=4 .................................
[CV] ........ max_depth=3, min_child_weight=3, score=0.777397 -  10.7s
[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:   55.3s
[CV] max_depth=3, min_child_weight=4 .................................
[CV] ........ max_depth=3, min_child_weight=3, score=0.772618 -  12.4s
[CV] max_depth=3, min_child_weight=4 .................................
[CV] ........ max_depth=3, min_child_weight=4, score=0.780104 -  12.2s
[CV] max_depth=3, min_child_weight=4 .................................
[CV] ........ max_depth=3, min_child_weight=4, score=0.771433 -  12.5s
[CV] max_depth=4, min_child_weight=2 .................................
[CV] ........ max_depth=3, min_child_weight=4, score=0.780540 -  12.5s
[CV] max_depth=4, min_child_weight=2 .................................
[CV] ........ max_depth=3, min_child_weight=4, score=0.776209 -  12.5s
[CV] max_depth=4, min_child_weight=2 .................................
[CV] ........ max_depth=3, min_child_weight=4, score=0.775888 -  12.3s
[CV] max_depth=4, min_child_weight=2 .................................
[CV] ........ max_depth=4, min_child_weight=2, score=0.768076 -  16.3s
[CV] max_depth=4, min_child_weight=2 .................................
[CV] ........ max_depth=4, min_child_weight=2, score=0.777482 -  15.6s
[CV] max_depth=4, min_child_weight=3 .................................
[CV] ........ max_depth=4, min_child_weight=2, score=0.778697 -  15.7s
[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:  1.5min
[CV] max_depth=4, min_child_weight=3 .................................
[CV] ........ max_depth=4, min_child_weight=2, score=0.768765 -  15.8s
[CV] max_depth=4, min_child_weight=3 .................................
[CV] ........ max_depth=4, min_child_weight=2, score=0.768072 -  16.5s
[CV] max_depth=4, min_child_weight=3 .................................
[CV] ........ max_depth=4, min_child_weight=3, score=0.765572 -  16.2s
[CV] max_depth=4, min_child_weight=3 .................................
[CV] ........ max_depth=4, min_child_weight=3, score=0.779497 -  14.8s
[CV] max_depth=4, min_child_weight=4 .................................
[CV] ........ max_depth=4, min_child_weight=3, score=0.779728 -  14.7s
[CV] max_depth=4, min_child_weight=4 .................................
[CV] ........ max_depth=4, min_child_weight=3, score=0.774182 -  13.7s
[CV] max_depth=4, min_child_weight=4 .................................
[CV] ........ max_depth=4, min_child_weight=3, score=0.768356 -  13.7s
[CV] max_depth=4, min_child_weight=4 .................................
[CV] ........ max_depth=4, min_child_weight=4, score=0.765353 -  13.7s
[CV] max_depth=4, min_child_weight=4 .................................
[CV] ........ max_depth=4, min_child_weight=4, score=0.777797 -  13.4s
[CV] ........ max_depth=4, min_child_weight=4, score=0.776876 -  12.8s
[Parallel(n_jobs=4)]: Done  43 out of  45 | elapsed:  2.1min remaining:    5.8s
[CV] ........ max_depth=4, min_child_weight=4, score=0.771975 -  11.8s
[CV] ........ max_depth=4, min_child_weight=4, score=0.768625 -   9.2s
[Parallel(n_jobs=4)]: Done  45 out of  45 | elapsed:  2.2min finished
[mean: 0.77793, std: 0.00357, params: {'max_depth': 2, 'min_child_weight': 2}, mean: 0.77864, std: 0.00384, params: {'max_depth': 2, 'min_child_weight': 3}, mean: 0.77881, std: 0.00307, params: {'max_depth': 2, 'min_child_weight': 4}, mean: 0.77666, std: 0.00348, params: {'max_depth': 3, 'min_child_weight': 2}, mean: 0.77636, std: 0.00456, params: {'max_depth': 3, 'min_child_weight': 3}, mean: 0.77683, std: 0.00331, params: {'max_depth': 3, 'min_child_weight': 4}, mean: 0.77222, std: 0.00482, params: {'max_depth': 4, 'min_child_weight': 2}, mean: 0.77347, std: 0.00574, params: {'max_depth': 4, 'min_child_weight': 3}, mean: 0.77213, std: 0.00475, params: {'max_depth': 4, 'min_child_weight': 4}] {'max_depth': 2, 'min_child_weight': 4} 0.7788097088081243

In [56]: param_test3 = {
    ...:     'gamma':[i/10.0 for i in range(0,5)]
    ...:         }
    ...: gsearch3 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.2, n_estimators=140, max_depth=2,
    ...:  min_child_weight=4, gamma=0, subsample=0.8, colsample_bytree=0.8,
    ...:  objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), 
    ...:  param_grid = param_test3, scoring='roc_auc',n_jobs=4,iid=False, cv=5,verbose=10)
    ...: gsearch3.fit(X_train,y_train)
    ...: print(gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_)
    ...:  
Fitting 5 folds for each of 5 candidates, totalling 25 fits
[CV] gamma=0.0 .......................................................
[CV] gamma=0.0 .......................................................
[CV] gamma=0.0 .......................................................
[CV] gamma=0.0 .......................................................
[CV] .............................. gamma=0.0, score=0.776272 -   7.2s
[CV] gamma=0.0 .......................................................
[CV] .............................. gamma=0.0, score=0.783595 -   7.2s
[CV] gamma=0.1 .......................................................
[CV] .............................. gamma=0.0, score=0.779826 -   7.2s
[CV] gamma=0.1 .......................................................
[CV] .............................. gamma=0.0, score=0.779573 -   7.2s
[CV] gamma=0.1 .......................................................
[CV] .............................. gamma=0.1, score=0.783595 -   7.1s
[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:   14.7s
[CV] .............................. gamma=0.0, score=0.774783 -   7.3s
[CV] .............................. gamma=0.1, score=0.776272 -   7.3s
[CV] gamma=0.1 .......................................................
[CV] gamma=0.1 .......................................................
[CV] .............................. gamma=0.1, score=0.779826 -   7.2s
[CV] gamma=0.2 .......................................................
[CV] gamma=0.2 .......................................................
[CV] .............................. gamma=0.1, score=0.774783 -   7.3s
[CV] .............................. gamma=0.1, score=0.778937 -   7.4s
[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:   22.1s
[CV] gamma=0.2 .......................................................
[CV] gamma=0.2 .......................................................
[CV] .............................. gamma=0.2, score=0.776272 -   7.4s
[CV] .............................. gamma=0.2, score=0.783595 -   7.3s
[CV] gamma=0.2 .......................................................
[CV] gamma=0.3 .......................................................
[CV] .............................. gamma=0.2, score=0.779826 -   7.3s
[CV] .............................. gamma=0.2, score=0.774783 -   7.2s
[CV] gamma=0.3 .......................................................
[CV] gamma=0.3 .......................................................
[CV] .............................. gamma=0.2, score=0.779508 -   7.4s
[CV] gamma=0.3 .......................................................
[CV] .............................. gamma=0.3, score=0.776272 -   7.3s
[CV] gamma=0.3 .......................................................
[CV] .............................. gamma=0.3, score=0.779826 -   7.2s
[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:   36.7s
[CV] gamma=0.4 .......................................................
[CV] .............................. gamma=0.3, score=0.783595 -   7.4s
[CV] .............................. gamma=0.3, score=0.779508 -   7.2s
[CV] gamma=0.4 .......................................................
[CV] gamma=0.4 .......................................................
[CV] .............................. gamma=0.3, score=0.774783 -   7.2s
[CV] gamma=0.4 .......................................................
[CV] .............................. gamma=0.4, score=0.776272 -   7.3s
[Parallel(n_jobs=4)]: Done  21 out of  25 | elapsed:   44.1s remaining:    8.4s
[CV] .............................. gamma=0.4, score=0.779826 -   7.1s
[CV] .............................. gamma=0.4, score=0.783595 -   7.2s
[CV] gamma=0.4 .......................................................
[CV] .............................. gamma=0.4, score=0.779508 -   7.1s
[CV] .............................. gamma=0.4, score=0.774783 -   4.4s
[Parallel(n_jobs=4)]: Done  25 out of  25 | elapsed:   48.5s finished
[mean: 0.77881, std: 0.00307, params: {'gamma': 0.0}, mean: 0.77868, std: 0.00305, params: {'gamma': 0.1}, mean: 0.77880, std: 0.00307, params: {'gamma': 0.2}, mean: 0.77880, std: 0.00307, params: {'gamma': 0.3}, mean: 0.77880, std: 0.00307, params: {'gamma': 0.4}] {'gamma': 0.0} 0.7788097088081243

In [57]: param_test4 = {
    ...:     'subsample':[i/10.0 for i in range(6,10)],
    ...:     'colsample_bytree':[i/10.0 for i in range(6,10)]
    ...:         }
    ...: gsearch4 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.2, n_estimators=140, max_depth=2,
    ...:  min_child_weight=4, gamma=0, subsample=0.8, colsample_bytree=0.8,
    ...:  objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), 
    ...:  param_grid = param_test4, scoring='roc_auc',n_jobs=4,iid=False, cv=5,verbose=10)
    ...: gsearch4.fit(X_train,y_train)
    ...: print(gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_)
    ...: 
Fitting 5 folds for each of 16 candidates, totalling 80 fits
[CV] colsample_bytree=0.6, subsample=0.6 .............................
[CV] colsample_bytree=0.6, subsample=0.6 .............................
[CV] colsample_bytree=0.6, subsample=0.6 .............................
[CV] colsample_bytree=0.6, subsample=0.6 .............................
[CV] .... colsample_bytree=0.6, subsample=0.6, score=0.781629 -   5.5s
[CV] .... colsample_bytree=0.6, subsample=0.6, score=0.776427 -   5.6s
[CV] colsample_bytree=0.6, subsample=0.6 .............................
[CV] colsample_bytree=0.6, subsample=0.7 .............................
[CV] .... colsample_bytree=0.6, subsample=0.6, score=0.777352 -   5.6s
[CV] .... colsample_bytree=0.6, subsample=0.6, score=0.779430 -   5.7s
[CV] colsample_bytree=0.6, subsample=0.7 .............................
[CV] colsample_bytree=0.6, subsample=0.7 .............................
[CV] .... colsample_bytree=0.6, subsample=0.6, score=0.775865 -   5.6s
[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:   11.4s
[CV] colsample_bytree=0.6, subsample=0.7 .............................
[CV] .... colsample_bytree=0.6, subsample=0.7, score=0.776009 -   5.7s
[CV] colsample_bytree=0.6, subsample=0.7 .............................
[CV] .... colsample_bytree=0.6, subsample=0.7, score=0.781652 -   5.7s
[CV] colsample_bytree=0.6, subsample=0.8 .............................
[CV] .... colsample_bytree=0.6, subsample=0.7, score=0.777651 -   5.8s
[CV] colsample_bytree=0.6, subsample=0.8 .............................
[CV] .... colsample_bytree=0.6, subsample=0.7, score=0.778550 -   5.6s
[CV] colsample_bytree=0.6, subsample=0.8 .............................
[CV] .... colsample_bytree=0.6, subsample=0.7, score=0.775495 -   5.7s
[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:   17.3s
[CV] colsample_bytree=0.6, subsample=0.8 .............................
[CV] .... colsample_bytree=0.6, subsample=0.8, score=0.773832 -   5.7s
[CV] colsample_bytree=0.6, subsample=0.8 .............................
[CV] .... colsample_bytree=0.6, subsample=0.8, score=0.782490 -   5.8s
[CV] colsample_bytree=0.6, subsample=0.9 .............................
[CV] .... colsample_bytree=0.6, subsample=0.8, score=0.779088 -   5.8s
[CV] colsample_bytree=0.6, subsample=0.9 .............................
[CV] .... colsample_bytree=0.6, subsample=0.8, score=0.779666 -   5.8s
[CV] .... colsample_bytree=0.6, subsample=0.8, score=0.776502 -   5.6s
[CV] colsample_bytree=0.6, subsample=0.9 .............................
[CV] colsample_bytree=0.6, subsample=0.9 .............................
[CV] .... colsample_bytree=0.6, subsample=0.9, score=0.777963 -   5.9s
[CV] colsample_bytree=0.6, subsample=0.9 .............................
[CV] .... colsample_bytree=0.6, subsample=0.9, score=0.782990 -   6.3s
[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:   29.3s
[CV] colsample_bytree=0.7, subsample=0.6 .............................
[CV] .... colsample_bytree=0.6, subsample=0.9, score=0.780395 -   6.2s
[CV] colsample_bytree=0.7, subsample=0.6 .............................
[CV] .... colsample_bytree=0.6, subsample=0.9, score=0.781708 -   6.4s
[CV] colsample_bytree=0.7, subsample=0.6 .............................
[CV] .... colsample_bytree=0.6, subsample=0.9, score=0.778350 -   6.3s
[CV] colsample_bytree=0.7, subsample=0.6 .............................
[CV] .... colsample_bytree=0.7, subsample=0.6, score=0.774473 -   6.3s
[CV] colsample_bytree=0.7, subsample=0.6 .............................
[CV] .... colsample_bytree=0.7, subsample=0.6, score=0.783433 -   6.2s
[CV] colsample_bytree=0.7, subsample=0.7 .............................
[CV] .... colsample_bytree=0.7, subsample=0.6, score=0.780244 -   6.3s
[CV] colsample_bytree=0.7, subsample=0.7 .............................
[CV] .... colsample_bytree=0.7, subsample=0.6, score=0.779134 -   6.2s
[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:   36.2s
[CV] colsample_bytree=0.7, subsample=0.7 .............................
[CV] .... colsample_bytree=0.7, subsample=0.6, score=0.776170 -   6.3s
[CV] colsample_bytree=0.7, subsample=0.7 .............................
[CV] .... colsample_bytree=0.7, subsample=0.7, score=0.774527 -   6.4s
[CV] colsample_bytree=0.7, subsample=0.7 .............................
[CV] .... colsample_bytree=0.7, subsample=0.7, score=0.781198 -   6.3s
[CV] colsample_bytree=0.7, subsample=0.8 .............................
[CV] .... colsample_bytree=0.7, subsample=0.7, score=0.779455 -   6.3s
[CV] colsample_bytree=0.7, subsample=0.8 .............................
[CV] .... colsample_bytree=0.7, subsample=0.7, score=0.778849 -   6.4s
[CV] colsample_bytree=0.7, subsample=0.8 .............................
[CV] .... colsample_bytree=0.7, subsample=0.7, score=0.778708 -   6.4s
[CV] colsample_bytree=0.7, subsample=0.8 .............................
[CV] .... colsample_bytree=0.7, subsample=0.8, score=0.775111 -   6.6s
[CV] colsample_bytree=0.7, subsample=0.8 .............................
[CV] .... colsample_bytree=0.7, subsample=0.8, score=0.783454 -   6.6s
[CV] colsample_bytree=0.7, subsample=0.9 .............................
[CV] .... colsample_bytree=0.7, subsample=0.8, score=0.779014 -   6.6s
[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:   55.0s
[CV] colsample_bytree=0.7, subsample=0.9 .............................
[CV] .... colsample_bytree=0.7, subsample=0.8, score=0.780332 -   6.6s
[CV] colsample_bytree=0.7, subsample=0.9 .............................
[CV] .... colsample_bytree=0.7, subsample=0.8, score=0.776508 -   6.7s
[CV] colsample_bytree=0.7, subsample=0.9 .............................
[CV] .... colsample_bytree=0.7, subsample=0.9, score=0.777139 -   6.8s
[CV] colsample_bytree=0.7, subsample=0.9 .............................
[CV] .... colsample_bytree=0.7, subsample=0.9, score=0.783572 -   6.7s
[CV] .... colsample_bytree=0.7, subsample=0.9, score=0.779237 -   6.6s
[CV] colsample_bytree=0.8, subsample=0.6 .............................
[CV] colsample_bytree=0.8, subsample=0.6 .............................
[CV] .... colsample_bytree=0.7, subsample=0.9, score=0.779344 -   6.5s
[CV] colsample_bytree=0.8, subsample=0.6 .............................
[CV] .... colsample_bytree=0.7, subsample=0.9, score=0.776178 -   6.6s
[CV] colsample_bytree=0.8, subsample=0.6 .............................
[CV] .... colsample_bytree=0.8, subsample=0.6, score=0.782416 -   6.9s
[CV] colsample_bytree=0.8, subsample=0.6 .............................
[CV] .... colsample_bytree=0.8, subsample=0.6, score=0.777122 -   7.2s
[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  1.2min
[CV] colsample_bytree=0.8, subsample=0.7 .............................
[CV] .... colsample_bytree=0.8, subsample=0.6, score=0.778445 -   7.0s
[CV] colsample_bytree=0.8, subsample=0.7 .............................
[CV] .... colsample_bytree=0.8, subsample=0.6, score=0.777610 -   7.1s
[CV] colsample_bytree=0.8, subsample=0.7 .............................
[CV] .... colsample_bytree=0.8, subsample=0.6, score=0.774235 -   6.9s
[CV] colsample_bytree=0.8, subsample=0.7 .............................
[CV] .... colsample_bytree=0.8, subsample=0.7, score=0.772595 -   7.1s
[CV] colsample_bytree=0.8, subsample=0.7 .............................
[CV] .... colsample_bytree=0.8, subsample=0.7, score=0.782035 -   7.2s
[CV] colsample_bytree=0.8, subsample=0.8 .............................
[CV] .... colsample_bytree=0.8, subsample=0.7, score=0.780148 -   7.1s
[CV] colsample_bytree=0.8, subsample=0.8 .............................
[CV] .... colsample_bytree=0.8, subsample=0.7, score=0.776665 -   7.0s
[CV] colsample_bytree=0.8, subsample=0.8 .............................
[CV] .... colsample_bytree=0.8, subsample=0.7, score=0.775017 -   7.0s
[CV] colsample_bytree=0.8, subsample=0.8 .............................
[CV] .... colsample_bytree=0.8, subsample=0.8, score=0.776272 -   7.1s
[CV] colsample_bytree=0.8, subsample=0.8 .............................
[CV] .... colsample_bytree=0.8, subsample=0.8, score=0.783595 -   7.1s
[CV] colsample_bytree=0.8, subsample=0.9 .............................
[CV] .... colsample_bytree=0.8, subsample=0.8, score=0.779826 -   7.2s
[Parallel(n_jobs=4)]: Done  53 tasks      | elapsed:  1.5min
[CV] colsample_bytree=0.8, subsample=0.9 .............................
[CV] .... colsample_bytree=0.8, subsample=0.8, score=0.779573 -   7.3s
[CV] colsample_bytree=0.8, subsample=0.9 .............................
[CV] .... colsample_bytree=0.8, subsample=0.8, score=0.774783 -   7.2s
[CV] colsample_bytree=0.8, subsample=0.9 .............................
[CV] .... colsample_bytree=0.8, subsample=0.9, score=0.776987 -   7.4s
[CV] colsample_bytree=0.8, subsample=0.9 .............................
[CV] .... colsample_bytree=0.8, subsample=0.9, score=0.784825 -   7.4s
[CV] colsample_bytree=0.9, subsample=0.6 .............................
[CV] .... colsample_bytree=0.8, subsample=0.9, score=0.780686 -   7.5s
[CV] colsample_bytree=0.9, subsample=0.6 .............................
[CV] .... colsample_bytree=0.8, subsample=0.9, score=0.780174 -   7.6s
[CV] colsample_bytree=0.9, subsample=0.6 .............................
[CV] .... colsample_bytree=0.8, subsample=0.9, score=0.777472 -   7.5s
[CV] colsample_bytree=0.9, subsample=0.6 .............................
[CV] .... colsample_bytree=0.9, subsample=0.6, score=0.774398 -   8.2s
[CV] colsample_bytree=0.9, subsample=0.6 .............................
[CV] .... colsample_bytree=0.9, subsample=0.6, score=0.781657 -   8.4s
[CV] colsample_bytree=0.9, subsample=0.7 .............................
[CV] .... colsample_bytree=0.9, subsample=0.6, score=0.778512 -   8.2s
[CV] colsample_bytree=0.9, subsample=0.7 .............................
[CV] .... colsample_bytree=0.9, subsample=0.6, score=0.779463 -   8.1s
[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:  1.8min
[CV] colsample_bytree=0.9, subsample=0.7 .............................
[CV] .... colsample_bytree=0.9, subsample=0.6, score=0.777482 -   8.0s
[CV] colsample_bytree=0.9, subsample=0.7 .............................
[CV] .... colsample_bytree=0.9, subsample=0.7, score=0.773965 -   8.1s
[CV] colsample_bytree=0.9, subsample=0.7 .............................
[CV] .... colsample_bytree=0.9, subsample=0.7, score=0.781547 -   8.1s
[CV] colsample_bytree=0.9, subsample=0.8 .............................
[CV] .... colsample_bytree=0.9, subsample=0.7, score=0.781124 -   8.0s
[CV] colsample_bytree=0.9, subsample=0.8 .............................
[CV] .... colsample_bytree=0.9, subsample=0.7, score=0.780323 -   7.9s
[CV] colsample_bytree=0.9, subsample=0.8 .............................
[CV] .... colsample_bytree=0.9, subsample=0.7, score=0.777142 -   7.9s
[CV] colsample_bytree=0.9, subsample=0.8 .............................
[CV] .... colsample_bytree=0.9, subsample=0.8, score=0.774815 -   8.1s
[CV] colsample_bytree=0.9, subsample=0.8 .............................
[CV] .... colsample_bytree=0.9, subsample=0.8, score=0.784211 -   8.1s
[CV] colsample_bytree=0.9, subsample=0.9 .............................
[CV] .... colsample_bytree=0.9, subsample=0.8, score=0.779691 -   8.2s
[CV] colsample_bytree=0.9, subsample=0.9 .............................
[CV] .... colsample_bytree=0.9, subsample=0.8, score=0.777884 -   8.2s
[CV] colsample_bytree=0.9, subsample=0.9 .............................
[CV] .... colsample_bytree=0.9, subsample=0.8, score=0.777577 -   8.2s
[CV] colsample_bytree=0.9, subsample=0.9 .............................
[CV] .... colsample_bytree=0.9, subsample=0.9, score=0.776334 -   8.4s
[CV] colsample_bytree=0.9, subsample=0.9 .............................
[CV] .... colsample_bytree=0.9, subsample=0.9, score=0.783608 -   8.1s
[CV] .... colsample_bytree=0.9, subsample=0.9, score=0.781846 -   8.0s
[CV] .... colsample_bytree=0.9, subsample=0.9, score=0.779379 -   7.8s
[CV] .... colsample_bytree=0.9, subsample=0.9, score=0.776354 -   7.6s
[Parallel(n_jobs=4)]: Done  80 out of  80 | elapsed:  2.3min finished
[mean: 0.77814, std: 0.00212, params: {'colsample_bytree': 0.6, 'subsample': 0.6}, mean: 0.77787, std: 0.00219, params: {'colsample_bytree': 0.6, 'subsample': 0.7}, mean: 0.77832, std: 0.00294, params: {'colsample_bytree': 0.6, 'subsample': 0.8}, mean: 0.78028, std: 0.00192, params: {'colsample_bytree': 0.6, 'subsample': 0.9}, mean: 0.77869, std: 0.00314, params: {'colsample_bytree': 0.7, 'subsample': 0.6}, mean: 0.77855, std: 0.00220, params: {'colsample_bytree': 0.7, 'subsample': 0.7}, mean: 0.77888, std: 0.00293, params: {'colsample_bytree': 0.7, 'subsample': 0.8}, mean: 0.77909, std: 0.00255, params: {'colsample_bytree': 0.7, 'subsample': 0.9}, mean: 0.77797, std: 0.00264, params: {'colsample_bytree': 0.8, 'subsample': 0.6}, mean: 0.77729, std: 0.00341, params: {'colsample_bytree': 0.8, 'subsample': 0.7}, mean: 0.77881, std: 0.00307, params: {'colsample_bytree': 0.8, 'subsample': 0.8}, mean: 0.78003, std: 0.00280, params: {'colsample_bytree': 0.8, 'subsample': 0.9}, mean: 0.77830, std: 0.00239, params: {'colsample_bytree': 0.9, 'subsample': 0.6}, mean: 0.77882, std: 0.00288, params: {'colsample_bytree': 0.9, 'subsample': 0.7}, mean: 0.77884, std: 0.00311, params: {'colsample_bytree': 0.9, 'subsample': 0.8}, mean: 0.77950, std: 0.00291, params: {'colsample_bytree': 0.9, 'subsample': 0.9}] {'colsample_bytree': 0.6, 'subsample': 0.9} 0.7802810317260898

In [58]: param_test5 = {
    ...:     'subsample':[i/10.0 for i in range(6,10)],
    ...:     'colsample_bytree':[i/10.0 for i in range(6,10)]
    ...:         }
    ...: gsearch5 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.2, n_estimators=140, max_depth=2,
    ...:  min_child_weight=4, gamma=0, subsample=0.9, colsample_bytree=0.6,
    ...:  objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), 
    ...:  param_grid = param_test5, scoring='roc_auc',n_jobs=4,iid=False, cv=5,verbose=10)
    ...: gsearch5.fit(X_train,y_train)
    ...: print(gsearch5.grid_scores_, gsearch5.best_params_, gsearch5.best_score_)
    ...: 
Fitting 5 folds for each of 16 candidates, totalling 80 fits
[CV] colsample_bytree=0.6, subsample=0.6 .............................
[CV] colsample_bytree=0.6, subsample=0.6 .............................
[CV] colsample_bytree=0.6, subsample=0.6 .............................
[CV] colsample_bytree=0.6, subsample=0.6 .............................
^C---------------------------------------------------------------------------
KeyboardInterrupt                         Traceback (most recent call last)
<ipython-input-58-236f014c75b4> in <module>()
      7  objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), 
      8  param_grid = param_test5, scoring='roc_auc',n_jobs=4,iid=False, cv=5,verbose=10)
----> 9 gsearch5.fit(X_train,y_train)
     10 print(gsearch5.grid_scores_, gsearch5.best_params_, gsearch5.best_score_)

~/miniconda3/lib/python3.6/site-packages/sklearn/grid_search.py in fit(self, X, y)
    836 
    837         """
--> 838         return self._fit(X, y, ParameterGrid(self.param_grid))
    839 
    840 

~/miniconda3/lib/python3.6/site-packages/sklearn/grid_search.py in _fit(self, X, y, parameter_iterable)
    572                                     self.fit_params, return_parameters=True,
    573                                     error_score=self.error_score)
--> 574                 for parameters in parameter_iterable
    575                 for train, test in cv)
    576 

~/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self, iterable)
    787                 # consumption.
    788                 self._iterating = False
--> 789             self.retrieve()
    790             # Make sure that we get a last message telling us we are done
    791             elapsed_time = time.time() - self._start_time

~/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in retrieve(self)
    697             try:
    698                 if getattr(self._backend, 'supports_timeout', False):
--> 699                     self._output.extend(job.get(timeout=self.timeout))
    700                 else:
    701                     self._output.extend(job.get())

~/miniconda3/lib/python3.6/multiprocessing/pool.py in get(self, timeout)
    636 
    637     def get(self, timeout=None):
--> 638         self.wait(timeout)
    639         if not self.ready():
    640             raise TimeoutError

~/miniconda3/lib/python3.6/multiprocessing/pool.py in wait(self, timeout)
    633 
    634     def wait(self, timeout=None):
--> 635         self._event.wait(timeout)
    636 
    637     def get(self, timeout=None):

~/miniconda3/lib/python3.6/threading.py in wait(self, timeout)
    549             signaled = self._flag
    550             if not signaled:
--> 551                 signaled = self._cond.wait(timeout)
    552             return signaled
    553 

~/miniconda3/lib/python3.6/threading.py in wait(self, timeout)
    293         try:    # restore state no matter what (e.g., KeyboardInterrupt)
    294             if timeout is None:
--> 295                 waiter.acquire()
    296                 gotit = True
    297             else:

KeyboardInterrupt: 

In [59]: param_test5 = {
    ...:     'subsample':[i/100.0 for i in range(80,100,5)],
    ...:     'colsample_bytree':[i/100.0 for i in range(50,70,5)]
    ...:         }
    ...: gsearch5 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.2, n_estimators=140, max_depth=2,
    ...:  min_child_weight=4, gamma=0, subsample=0.9, colsample_bytree=0.6,
    ...:  objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), 
    ...:  param_grid = param_test5, scoring='roc_auc',n_jobs=4,iid=False, cv=5,verbose=10)
    ...: gsearch5.fit(X_train,y_train)
    ...: print(gsearch5.grid_scores_, gsearch5.best_params_, gsearch5.best_score_)
    ...:  
Fitting 5 folds for each of 16 candidates, totalling 80 fits
[CV] colsample_bytree=0.5, subsample=0.8 .............................
[CV] colsample_bytree=0.5, subsample=0.8 .............................
[CV] colsample_bytree=0.5, subsample=0.8 .............................
[CV] colsample_bytree=0.5, subsample=0.8 .............................
[CV] .... colsample_bytree=0.5, subsample=0.8, score=0.773041 -   4.9s
[CV] colsample_bytree=0.5, subsample=0.8 .............................
[CV] .... colsample_bytree=0.5, subsample=0.8, score=0.783438 -   5.0s
[CV] .... colsample_bytree=0.5, subsample=0.8, score=0.781672 -   4.9s
[CV] colsample_bytree=0.5, subsample=0.85 ............................
[CV] .... colsample_bytree=0.5, subsample=0.8, score=0.777981 -   5.0s
[CV] colsample_bytree=0.5, subsample=0.85 ............................
[CV] colsample_bytree=0.5, subsample=0.85 ............................
[CV] .... colsample_bytree=0.5, subsample=0.8, score=0.776322 -   5.2s
[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:   10.4s
[CV] colsample_bytree=0.5, subsample=0.85 ............................
[CV] ... colsample_bytree=0.5, subsample=0.85, score=0.784729 -   5.1s
[CV] colsample_bytree=0.5, subsample=0.85 ............................
[CV] ... colsample_bytree=0.5, subsample=0.85, score=0.775869 -   5.3s
[CV] colsample_bytree=0.5, subsample=0.9 .............................
[CV] ... colsample_bytree=0.5, subsample=0.85, score=0.780724 -   5.3s
[CV] colsample_bytree=0.5, subsample=0.9 .............................
[CV] ... colsample_bytree=0.5, subsample=0.85, score=0.780774 -   5.0s
[CV] colsample_bytree=0.5, subsample=0.9 .............................
[CV] ... colsample_bytree=0.5, subsample=0.85, score=0.774399 -   5.0s
[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:   15.6s
[CV] colsample_bytree=0.5, subsample=0.9 .............................
[CV] .... colsample_bytree=0.5, subsample=0.9, score=0.775623 -   5.1s
[CV] colsample_bytree=0.5, subsample=0.9 .............................
[CV] .... colsample_bytree=0.5, subsample=0.9, score=0.784568 -   5.0s
[CV] colsample_bytree=0.5, subsample=0.95 ............................
[CV] .... colsample_bytree=0.5, subsample=0.9, score=0.779885 -   5.1s
[CV] colsample_bytree=0.5, subsample=0.95 ............................
[CV] .... colsample_bytree=0.5, subsample=0.9, score=0.779563 -   5.1s
[CV] colsample_bytree=0.5, subsample=0.95 ............................
[CV] .... colsample_bytree=0.5, subsample=0.9, score=0.775580 -   5.1s
[CV] colsample_bytree=0.5, subsample=0.95 ............................
[CV] ... colsample_bytree=0.5, subsample=0.95, score=0.776436 -   5.2s
[CV] colsample_bytree=0.5, subsample=0.95 ............................
[CV] ... colsample_bytree=0.5, subsample=0.95, score=0.785442 -   5.2s
[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:   25.9s
[CV] colsample_bytree=0.55, subsample=0.8 ............................
[CV] ... colsample_bytree=0.5, subsample=0.95, score=0.779463 -   5.2s
[CV] colsample_bytree=0.55, subsample=0.8 ............................
[CV] ... colsample_bytree=0.5, subsample=0.95, score=0.778482 -   5.1s
[CV] colsample_bytree=0.55, subsample=0.8 ............................
[CV] ... colsample_bytree=0.5, subsample=0.95, score=0.774710 -   5.3s
[CV] colsample_bytree=0.55, subsample=0.8 ............................
[CV] ... colsample_bytree=0.55, subsample=0.8, score=0.774485 -   5.4s
[CV] colsample_bytree=0.55, subsample=0.8 ............................
[CV] ... colsample_bytree=0.55, subsample=0.8, score=0.781433 -   5.4s
[CV] colsample_bytree=0.55, subsample=0.85 ...........................
[CV] ... colsample_bytree=0.55, subsample=0.8, score=0.779625 -   5.3s
[CV] colsample_bytree=0.55, subsample=0.85 ...........................
[CV] ... colsample_bytree=0.55, subsample=0.8, score=0.778119 -   5.4s
[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:   31.9s
[CV] colsample_bytree=0.55, subsample=0.85 ...........................
[CV] ... colsample_bytree=0.55, subsample=0.8, score=0.776347 -   5.4s
[CV] colsample_bytree=0.55, subsample=0.85 ...........................
[CV] .. colsample_bytree=0.55, subsample=0.85, score=0.777477 -   5.4s
[CV] .. colsample_bytree=0.55, subsample=0.85, score=0.783122 -   5.4s
[CV] colsample_bytree=0.55, subsample=0.85 ...........................
[CV] colsample_bytree=0.55, subsample=0.9 ............................
[CV] .. colsample_bytree=0.55, subsample=0.85, score=0.780853 -   5.4s
[CV] colsample_bytree=0.55, subsample=0.9 ............................
[CV] .. colsample_bytree=0.55, subsample=0.85, score=0.778841 -   5.4s
[CV] colsample_bytree=0.55, subsample=0.9 ............................
[CV] .. colsample_bytree=0.55, subsample=0.85, score=0.774246 -   5.4s
[CV] colsample_bytree=0.55, subsample=0.9 ............................
[CV] ... colsample_bytree=0.55, subsample=0.9, score=0.775999 -   5.5s
[CV] colsample_bytree=0.55, subsample=0.9 ............................
[CV] ... colsample_bytree=0.55, subsample=0.9, score=0.783399 -   5.5s
[CV] colsample_bytree=0.55, subsample=0.95 ...........................
[CV] ... colsample_bytree=0.55, subsample=0.9, score=0.780986 -   5.5s
[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:   47.8s
[CV] colsample_bytree=0.55, subsample=0.95 ...........................
[CV] ... colsample_bytree=0.55, subsample=0.9, score=0.778722 -   5.6s
[CV] colsample_bytree=0.55, subsample=0.95 ...........................
[CV] ... colsample_bytree=0.55, subsample=0.9, score=0.774584 -   5.6s
[CV] colsample_bytree=0.55, subsample=0.95 ...........................
[CV] .. colsample_bytree=0.55, subsample=0.95, score=0.776399 -   5.7s
[CV] colsample_bytree=0.55, subsample=0.95 ...........................
[CV] .. colsample_bytree=0.55, subsample=0.95, score=0.783700 -   5.7s
[CV] colsample_bytree=0.6, subsample=0.8 .............................
[CV] .. colsample_bytree=0.55, subsample=0.95, score=0.781847 -   5.6s
[CV] colsample_bytree=0.6, subsample=0.8 .............................
[CV] .. colsample_bytree=0.55, subsample=0.95, score=0.779599 -   5.6s
[CV] colsample_bytree=0.6, subsample=0.8 .............................
[CV] .. colsample_bytree=0.55, subsample=0.95, score=0.775211 -   5.5s
[CV] colsample_bytree=0.6, subsample=0.8 .............................
[CV] .... colsample_bytree=0.6, subsample=0.8, score=0.782490 -   5.9s
[CV] .... colsample_bytree=0.6, subsample=0.8, score=0.773832 -   6.0s
[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   59.6s
[CV] colsample_bytree=0.6, subsample=0.8 .............................
[CV] colsample_bytree=0.6, subsample=0.85 ............................
[CV] .... colsample_bytree=0.6, subsample=0.8, score=0.779088 -   5.9s
[CV] colsample_bytree=0.6, subsample=0.85 ............................
[CV] .... colsample_bytree=0.6, subsample=0.8, score=0.779666 -   6.0s
[CV] colsample_bytree=0.6, subsample=0.85 ............................
[CV] .... colsample_bytree=0.6, subsample=0.8, score=0.776502 -   5.8s
[CV] colsample_bytree=0.6, subsample=0.85 ............................
[CV] ... colsample_bytree=0.6, subsample=0.85, score=0.776352 -   5.8s
[CV] colsample_bytree=0.6, subsample=0.85 ............................
[CV] ... colsample_bytree=0.6, subsample=0.85, score=0.783189 -   5.9s
[CV] colsample_bytree=0.6, subsample=0.9 .............................
[CV] ... colsample_bytree=0.6, subsample=0.85, score=0.779612 -   5.9s
[CV] colsample_bytree=0.6, subsample=0.9 .............................
[CV] ... colsample_bytree=0.6, subsample=0.85, score=0.779648 -   5.9s
[CV] ... colsample_bytree=0.6, subsample=0.85, score=0.775536 -   5.8s
[CV] colsample_bytree=0.6, subsample=0.9 .............................
[CV] colsample_bytree=0.6, subsample=0.9 .............................
[CV] .... colsample_bytree=0.6, subsample=0.9, score=0.777963 -   5.9s
[CV] colsample_bytree=0.6, subsample=0.9 .............................
[CV] .... colsample_bytree=0.6, subsample=0.9, score=0.782990 -   5.9s
[CV] colsample_bytree=0.6, subsample=0.95 ............................
[CV] .... colsample_bytree=0.6, subsample=0.9, score=0.780395 -   6.1s
[Parallel(n_jobs=4)]: Done  53 tasks      | elapsed:  1.3min
[CV] .... colsample_bytree=0.6, subsample=0.9, score=0.781708 -   6.2s
[CV] colsample_bytree=0.6, subsample=0.95 ............................
[CV] colsample_bytree=0.6, subsample=0.95 ............................
[CV] .... colsample_bytree=0.6, subsample=0.9, score=0.778350 -   6.2s
[CV] colsample_bytree=0.6, subsample=0.95 ............................
[CV] ... colsample_bytree=0.6, subsample=0.95, score=0.776560 -   6.2s
[CV] colsample_bytree=0.6, subsample=0.95 ............................
[CV] ... colsample_bytree=0.6, subsample=0.95, score=0.781416 -   5.8s
[CV] colsample_bytree=0.65, subsample=0.8 ............................
[CV] ... colsample_bytree=0.6, subsample=0.95, score=0.784290 -   6.0s
[CV] colsample_bytree=0.65, subsample=0.8 ............................
[CV] ... colsample_bytree=0.6, subsample=0.95, score=0.780111 -   5.9s
[CV] colsample_bytree=0.65, subsample=0.8 ............................
[CV] ... colsample_bytree=0.6, subsample=0.95, score=0.774959 -   5.9s
[CV] colsample_bytree=0.65, subsample=0.8 ............................
[CV] ... colsample_bytree=0.65, subsample=0.8, score=0.773710 -   6.2s
[CV] colsample_bytree=0.65, subsample=0.8 ............................
[CV] ... colsample_bytree=0.65, subsample=0.8, score=0.784115 -   6.2s
[CV] colsample_bytree=0.65, subsample=0.85 ...........................
[CV] ... colsample_bytree=0.65, subsample=0.8, score=0.778317 -   6.2s
[CV] colsample_bytree=0.65, subsample=0.85 ...........................
[CV] ... colsample_bytree=0.65, subsample=0.8, score=0.780554 -   6.3s
[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:  1.5min
[CV] colsample_bytree=0.65, subsample=0.85 ...........................
[CV] ... colsample_bytree=0.65, subsample=0.8, score=0.777770 -   6.1s
[CV] colsample_bytree=0.65, subsample=0.85 ...........................
[CV] .. colsample_bytree=0.65, subsample=0.85, score=0.777376 -   6.2s
[CV] colsample_bytree=0.65, subsample=0.85 ...........................
[CV] .. colsample_bytree=0.65, subsample=0.85, score=0.783218 -   6.2s
[CV] colsample_bytree=0.65, subsample=0.9 ............................
[CV] .. colsample_bytree=0.65, subsample=0.85, score=0.780636 -   6.3s
[CV] colsample_bytree=0.65, subsample=0.9 ............................
[CV] .. colsample_bytree=0.65, subsample=0.85, score=0.780680 -   6.3s
[CV] colsample_bytree=0.65, subsample=0.9 ............................
[CV] .. colsample_bytree=0.65, subsample=0.85, score=0.775903 -   6.2s
[CV] colsample_bytree=0.65, subsample=0.9 ............................
[CV] ... colsample_bytree=0.65, subsample=0.9, score=0.777365 -   6.3s
[CV] colsample_bytree=0.65, subsample=0.9 ............................
[CV] ... colsample_bytree=0.65, subsample=0.9, score=0.784292 -   6.3s
[CV] colsample_bytree=0.65, subsample=0.95 ...........................
[CV] ... colsample_bytree=0.65, subsample=0.9, score=0.780141 -   6.3s
[CV] colsample_bytree=0.65, subsample=0.95 ...........................
[CV] ... colsample_bytree=0.65, subsample=0.9, score=0.779355 -   6.4s
[CV] colsample_bytree=0.65, subsample=0.95 ...........................
[CV] ... colsample_bytree=0.65, subsample=0.9, score=0.778604 -   6.2s
[CV] colsample_bytree=0.65, subsample=0.95 ...........................
[CV] .. colsample_bytree=0.65, subsample=0.95, score=0.776935 -   6.4s
[CV] colsample_bytree=0.65, subsample=0.95 ...........................
[CV] .. colsample_bytree=0.65, subsample=0.95, score=0.782884 -   6.3s
[CV] .. colsample_bytree=0.65, subsample=0.95, score=0.779487 -   6.2s
[CV] .. colsample_bytree=0.65, subsample=0.95, score=0.781261 -   6.4s
[CV] .. colsample_bytree=0.65, subsample=0.95, score=0.775477 -   5.9s
[Parallel(n_jobs=4)]: Done  80 out of  80 | elapsed:  1.9min finished
[mean: 0.77849, std: 0.00372, params: {'colsample_bytree': 0.5, 'subsample': 0.8}, mean: 0.77930, std: 0.00373, params: {'colsample_bytree': 0.5, 'subsample': 0.85}, mean: 0.77904, std: 0.00332, params: {'colsample_bytree': 0.5, 'subsample': 0.9}, mean: 0.77891, std: 0.00366, params: {'colsample_bytree': 0.5, 'subsample': 0.95}, mean: 0.77800, std: 0.00243, params: {'colsample_bytree': 0.55, 'subsample': 0.8}, mean: 0.77891, std: 0.00301, params: {'colsample_bytree': 0.55, 'subsample': 0.85}, mean: 0.77874, std: 0.00321, params: {'colsample_bytree': 0.55, 'subsample': 0.9}, mean: 0.77935, std: 0.00320, params: {'colsample_bytree': 0.55, 'subsample': 0.95}, mean: 0.77832, std: 0.00294, params: {'colsample_bytree': 0.6, 'subsample': 0.8}, mean: 0.77887, std: 0.00273, params: {'colsample_bytree': 0.6, 'subsample': 0.85}, mean: 0.78028, std: 0.00192, params: {'colsample_bytree': 0.6, 'subsample': 0.9}, mean: 0.77947, std: 0.00335, params: {'colsample_bytree': 0.6, 'subsample': 0.95}, mean: 0.77889, std: 0.00342, params: {'colsample_bytree': 0.65, 'subsample': 0.8}, mean: 0.77956, std: 0.00261, params: {'colsample_bytree': 0.65, 'subsample': 0.85}, mean: 0.77995, std: 0.00236, params: {'colsample_bytree': 0.65, 'subsample': 0.9}, mean: 0.77921, std: 0.00272, params: {'colsample_bytree': 0.65, 'subsample': 0.95}] {'colsample_bytree': 0.6, 'subsample': 0.9} 0.7802810317260898

In [60]: param_test6 = {
    ...:     'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]
    ...:         }
    ...: gsearch6 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.2, n_estimators=177, max_depth=2,
    ...:  min_child_weight=4, gamma=0, subsample=0.9, colsample_bytree=0.6,
    ...:  objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), 
    ...:  param_grid = param_test6, scoring='roc_auc',n_jobs=4,iid=False, cv=5,verbose=10)
    ...: gsearch6.fit(X_train,y_train)
    ...: print(gsearch6.grid_scores_, gsearch6.best_params_, gsearch6.best_score_)
    ...: 
    ...: 
Fitting 5 folds for each of 5 candidates, totalling 25 fits
[CV] reg_alpha=1e-05 .................................................
[CV] reg_alpha=1e-05 .................................................
[CV] reg_alpha=1e-05 .................................................
[CV] reg_alpha=1e-05 .................................................
[CV] ........................ reg_alpha=1e-05, score=0.783103 -   7.9s
[CV] reg_alpha=1e-05 .................................................
[CV] ........................ reg_alpha=1e-05, score=0.776943 -   8.1s
[CV] reg_alpha=0.01 ..................................................
[CV] ........................ reg_alpha=1e-05, score=0.782268 -   8.0s
[CV] ........................ reg_alpha=1e-05, score=0.778981 -   8.0s
[CV] reg_alpha=0.01 ..................................................
[CV] reg_alpha=0.01 ..................................................
[CV] ........................ reg_alpha=1e-05, score=0.777534 -   8.4s
[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:   16.7s
[CV] reg_alpha=0.01 ..................................................
[CV] ......................... reg_alpha=0.01, score=0.776945 -   8.5s
[CV] reg_alpha=0.01 ..................................................
[CV] ......................... reg_alpha=0.01, score=0.783100 -   8.6s
[CV] reg_alpha=0.1 ...................................................
[CV] ......................... reg_alpha=0.01, score=0.782250 -   8.6s
[CV] reg_alpha=0.1 ...................................................
[CV] ......................... reg_alpha=0.01, score=0.778798 -   7.7s
[CV] reg_alpha=0.1 ...................................................
[CV] ......................... reg_alpha=0.01, score=0.777605 -   7.7s
[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:   24.7s
[CV] reg_alpha=0.1 ...................................................
[CV] .......................... reg_alpha=0.1, score=0.775670 -   7.7s
[CV] reg_alpha=0.1 ...................................................
[CV] .......................... reg_alpha=0.1, score=0.785452 -   7.7s
[CV] reg_alpha=1 .....................................................
[CV] .......................... reg_alpha=0.1, score=0.782794 -   8.6s
[CV] reg_alpha=1 .....................................................
[CV] .......................... reg_alpha=0.1, score=0.778856 -   8.6s
[CV] reg_alpha=1 .....................................................
[CV] .......................... reg_alpha=0.1, score=0.777614 -   8.8s
[CV] reg_alpha=1 .....................................................
[CV] ............................ reg_alpha=1, score=0.775691 -   8.8s
[CV] reg_alpha=1 .....................................................
[CV] ............................ reg_alpha=1, score=0.780987 -   8.2s
[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:   41.6s
[CV] reg_alpha=100 ...................................................
[CV] ............................ reg_alpha=1, score=0.784642 -   8.5s
[CV] reg_alpha=100 ...................................................
[CV] ............................ reg_alpha=1, score=0.780272 -   8.3s
[CV] reg_alpha=100 ...................................................
[CV] ............................ reg_alpha=1, score=0.778793 -   8.1s
[CV] reg_alpha=100 ...................................................
[CV] .......................... reg_alpha=100, score=0.775411 -   5.7s
[Parallel(n_jobs=4)]: Done  21 out of  25 | elapsed:   47.4s remaining:    9.0s
[CV] reg_alpha=100 ...................................................
[CV] .......................... reg_alpha=100, score=0.774260 -   5.6s
[CV] .......................... reg_alpha=100, score=0.777039 -   5.6s
[CV] .......................... reg_alpha=100, score=0.768908 -   5.7s
[CV] .......................... reg_alpha=100, score=0.768727 -   3.8s
[Parallel(n_jobs=4)]: Done  25 out of  25 | elapsed:   51.2s finished
[mean: 0.77977, std: 0.00249, params: {'reg_alpha': 1e-05}, mean: 0.77974, std: 0.00248, params: {'reg_alpha': 0.01}, mean: 0.78008, std: 0.00356, params: {'reg_alpha': 0.1}, mean: 0.78008, std: 0.00292, params: {'reg_alpha': 1}, mean: 0.77287, std: 0.00342, params: {'reg_alpha': 100}] {'reg_alpha': 1} 0.7800770815089887

In [61]: param_test7 = {
    ...:     'reg_alpha':[0.1,0.5,1,5,10]
    ...:         }
    ...: gsearch7 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.2, n_estimators=177, max_depth=2,
    ...:  min_child_weight=4, gamma=0, subsample=0.9, colsample_bytree=0.6,
    ...:  objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), 
    ...:  param_grid = param_test7, scoring='roc_auc',n_jobs=4,iid=False, cv=5,verbose=10)
    ...: gsearch7.fit(X_train,y_train)
    ...: print(gsearch7.grid_scores_, gsearch7.best_params_, gsearch7.best_score_)
    ...: 
Fitting 5 folds for each of 5 candidates, totalling 25 fits
[CV] reg_alpha=0.1 ...................................................
[CV] reg_alpha=0.1 ...................................................
[CV] reg_alpha=0.1 ...................................................
[CV] reg_alpha=0.1 ...................................................
[CV] .......................... reg_alpha=0.1, score=0.775670 -   7.5s
[CV] .......................... reg_alpha=0.1, score=0.785452 -   7.4s
[CV] reg_alpha=0.1 ...................................................
[CV] reg_alpha=0.5 ...................................................
[CV] .......................... reg_alpha=0.1, score=0.782794 -   7.5s
[CV] .......................... reg_alpha=0.1, score=0.778856 -   7.5s
[CV] reg_alpha=0.5 ...................................................
[CV] reg_alpha=0.5 ...................................................
[CV] .......................... reg_alpha=0.5, score=0.775549 -   7.5s
[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:   15.2s
[CV] reg_alpha=0.5 ...................................................
[CV] .......................... reg_alpha=0.1, score=0.777614 -   7.6s
[CV] reg_alpha=0.5 ...................................................
[CV] .......................... reg_alpha=0.5, score=0.785250 -   7.5s
[CV] reg_alpha=1 .....................................................
[CV] .......................... reg_alpha=0.5, score=0.780935 -   7.6s
[CV] reg_alpha=1 .....................................................
[CV] .......................... reg_alpha=0.5, score=0.782121 -   7.5s
[CV] reg_alpha=1 .....................................................
[CV] .......................... reg_alpha=0.5, score=0.777792 -   7.5s
[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:   22.9s
[CV] ............................ reg_alpha=1, score=0.775691 -   7.5s
[CV] reg_alpha=1 .....................................................
[CV] reg_alpha=1 .....................................................
[CV] ............................ reg_alpha=1, score=0.784642 -   7.5s
[CV] reg_alpha=5 .....................................................
[CV] ............................ reg_alpha=1, score=0.780987 -   7.8s
[CV] ............................ reg_alpha=1, score=0.780272 -   7.7s
[CV] reg_alpha=5 .....................................................
[CV] ............................ reg_alpha=1, score=0.778793 -   7.8s
[CV] reg_alpha=5 .....................................................
[CV] ............................ reg_alpha=5, score=0.777823 -   7.7s
[CV] reg_alpha=5 .....................................................
[CV] reg_alpha=5 .....................................................
[CV] ............................ reg_alpha=5, score=0.781870 -   7.5s
[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:   38.2s
[CV] reg_alpha=10 ....................................................
[CV] ............................ reg_alpha=5, score=0.777741 -   7.4s
[CV] ............................ reg_alpha=5, score=0.778908 -   7.5s
[CV] ............................ reg_alpha=5, score=0.783584 -   7.6s
[CV] reg_alpha=10 ....................................................
[CV] reg_alpha=10 ....................................................
[CV] reg_alpha=10 ....................................................
[CV] ........................... reg_alpha=10, score=0.782254 -   8.7s
[Parallel(n_jobs=4)]: Done  21 out of  25 | elapsed:   47.1s remaining:    9.0s
[CV] reg_alpha=10 ....................................................
[CV] ........................... reg_alpha=10, score=0.781732 -   8.8s
[CV] ........................... reg_alpha=10, score=0.777793 -   8.9s
[CV] ........................... reg_alpha=10, score=0.780353 -   8.8s
[CV] ........................... reg_alpha=10, score=0.778074 -   4.9s
[Parallel(n_jobs=4)]: Done  25 out of  25 | elapsed:   52.0s finished
[mean: 0.78008, std: 0.00356, params: {'reg_alpha': 0.1}, mean: 0.78033, std: 0.00338, params: {'reg_alpha': 0.5}, mean: 0.78008, std: 0.00292, params: {'reg_alpha': 1}, mean: 0.77999, std: 0.00234, params: {'reg_alpha': 5}, mean: 0.78004, std: 0.00183, params: {'reg_alpha': 10}] {'reg_alpha': 0.5} 0.7803296402879432

In [62]: predictors = X_train.columns.values.tolist()
    ...: xgb2 = XGBClassifier(
    ...:  learning_rate =0.2,
    ...:  n_estimators=1000,
    ...:  max_depth=2,
    ...:  min_child_weight=4,
    ...:  gamma=0,
    ...:  subsample=0.9,
    ...:  colsample_bytree=0.6,
    ...:  objective= 'binary:logistic',
    ...:  nthread=4,
    ...:  scale_pos_weight=1,
    ...:  reg_alpha=0.5,
    ...:  seed=27)
    ...: modelfit(xgb2, X_train, y_train, X_test, y_test, predictors)
    ...:  
[0]	train-auc:0.692347+0.00143208	test-auc:0.692246+0.00518667
[1]	train-auc:0.721976+0.00739711	test-auc:0.719567+0.00812688
[2]	train-auc:0.732125+0.00902749	test-auc:0.730492+0.0114762
[3]	train-auc:0.74552+0.008084	test-auc:0.74101+0.00895907
[4]	train-auc:0.753225+0.00256776	test-auc:0.747337+0.00628423
[5]	train-auc:0.756011+0.00208459	test-auc:0.751843+0.00567081
[6]	train-auc:0.759901+0.00236391	test-auc:0.756258+0.00510863
[7]	train-auc:0.761716+0.00293144	test-auc:0.757694+0.0058998
[8]	train-auc:0.762774+0.00220962	test-auc:0.759356+0.00590356
[9]	train-auc:0.764365+0.00147618	test-auc:0.759976+0.0049817
[10]	train-auc:0.766049+0.00133979	test-auc:0.760977+0.00452156
[11]	train-auc:0.768127+0.00194803	test-auc:0.762583+0.00466091
[12]	train-auc:0.770072+0.00212835	test-auc:0.764766+0.00288999
[13]	train-auc:0.770696+0.000732089	test-auc:0.765694+0.00328115
[14]	train-auc:0.771783+0.000192197	test-auc:0.766588+0.00316194
[15]	train-auc:0.771973+0.00018703	test-auc:0.766781+0.00330451
[16]	train-auc:0.77236+0.000487869	test-auc:0.767254+0.00326394
[17]	train-auc:0.773341+0.000531508	test-auc:0.768+0.003381
[18]	train-auc:0.774042+0.00104644	test-auc:0.768662+0.00305154
[19]	train-auc:0.774663+0.000730434	test-auc:0.769207+0.00288956
[20]	train-auc:0.775348+0.00101346	test-auc:0.769596+0.00243759
[21]	train-auc:0.775696+0.00100209	test-auc:0.769936+0.00237345
[22]	train-auc:0.776562+0.000694307	test-auc:0.770432+0.00248689
[23]	train-auc:0.777104+0.000610999	test-auc:0.770905+0.00245998
[24]	train-auc:0.777955+0.000619299	test-auc:0.771477+0.00247329
[25]	train-auc:0.778802+0.000763652	test-auc:0.77197+0.00222854
[26]	train-auc:0.779546+0.000810492	test-auc:0.772543+0.00202931
[27]	train-auc:0.780226+0.00102832	test-auc:0.772832+0.00206745
[28]	train-auc:0.780811+0.00128813	test-auc:0.773315+0.00208411
[29]	train-auc:0.781328+0.00118477	test-auc:0.773454+0.0021553
[30]	train-auc:0.781902+0.00109308	test-auc:0.773938+0.00236927
[31]	train-auc:0.782257+0.00122852	test-auc:0.774214+0.00228897
[32]	train-auc:0.78262+0.00126089	test-auc:0.774399+0.00234181
[33]	train-auc:0.78308+0.00132164	test-auc:0.774767+0.00236297
[34]	train-auc:0.783473+0.00124524	test-auc:0.774889+0.00231347
[35]	train-auc:0.783782+0.00137154	test-auc:0.774931+0.00230941
[36]	train-auc:0.784114+0.00121302	test-auc:0.775176+0.00226831
[37]	train-auc:0.784413+0.00114	test-auc:0.775452+0.0023443
[38]	train-auc:0.784727+0.00107274	test-auc:0.775529+0.00243371
[39]	train-auc:0.785208+0.00102225	test-auc:0.775694+0.00261788
[40]	train-auc:0.785487+0.00108281	test-auc:0.775745+0.00240746
[41]	train-auc:0.785895+0.000909897	test-auc:0.776041+0.0025244
[42]	train-auc:0.786156+0.000894823	test-auc:0.776029+0.00253039
[43]	train-auc:0.786429+0.00097028	test-auc:0.77602+0.00242051
[44]	train-auc:0.786761+0.000896593	test-auc:0.776128+0.00247646
[45]	train-auc:0.786944+0.000967716	test-auc:0.776294+0.00261909
[46]	train-auc:0.787213+0.000966432	test-auc:0.776587+0.00253027
[47]	train-auc:0.787381+0.000945762	test-auc:0.776838+0.00263247
[48]	train-auc:0.787689+0.00105057	test-auc:0.776909+0.00272249
[49]	train-auc:0.787922+0.00119706	test-auc:0.776964+0.00279726
[50]	train-auc:0.788048+0.00116422	test-auc:0.776992+0.00278443
[51]	train-auc:0.78819+0.00112308	test-auc:0.776956+0.00288151
[52]	train-auc:0.78834+0.00110118	test-auc:0.777118+0.00290544
[53]	train-auc:0.788455+0.00117864	test-auc:0.777197+0.00275974
[54]	train-auc:0.788783+0.00125279	test-auc:0.77714+0.00274461
[55]	train-auc:0.78907+0.00117114	test-auc:0.777314+0.0029977
[56]	train-auc:0.789351+0.0011332	test-auc:0.777335+0.00297047
[57]	train-auc:0.789648+0.00108035	test-auc:0.777576+0.00317104
[58]	train-auc:0.789968+0.0011164	test-auc:0.777622+0.0030913
[59]	train-auc:0.790145+0.00108152	test-auc:0.777674+0.0030528
[60]	train-auc:0.790325+0.00110616	test-auc:0.777713+0.00304684
[61]	train-auc:0.790482+0.00106821	test-auc:0.777789+0.00319024
[62]	train-auc:0.7907+0.00113255	test-auc:0.777788+0.00319947
[63]	train-auc:0.790992+0.0010924	test-auc:0.777886+0.00322348
[64]	train-auc:0.791338+0.00116883	test-auc:0.777807+0.00331261
[65]	train-auc:0.791567+0.00116596	test-auc:0.777922+0.00337517
[66]	train-auc:0.791721+0.00105997	test-auc:0.778014+0.00357974
[67]	train-auc:0.791881+0.00102054	test-auc:0.778194+0.00350443
[68]	train-auc:0.791996+0.001039	test-auc:0.778315+0.00344395
[69]	train-auc:0.792192+0.00110099	test-auc:0.778562+0.00335731
[70]	train-auc:0.792421+0.00118654	test-auc:0.77845+0.00349776
[71]	train-auc:0.792635+0.00117519	test-auc:0.778575+0.00343813
[72]	train-auc:0.792914+0.00117406	test-auc:0.778585+0.00351386
[73]	train-auc:0.793053+0.00119377	test-auc:0.778718+0.00357892
[74]	train-auc:0.793238+0.00112335	test-auc:0.778842+0.00364664
[75]	train-auc:0.793408+0.00111696	test-auc:0.778919+0.00358812
[76]	train-auc:0.79351+0.00117966	test-auc:0.778846+0.00354574
[77]	train-auc:0.793661+0.0011298	test-auc:0.778783+0.00366518
[78]	train-auc:0.793891+0.00114864	test-auc:0.778753+0.00365592
[79]	train-auc:0.794117+0.00112926	test-auc:0.778792+0.00365
[80]	train-auc:0.794315+0.00106428	test-auc:0.778793+0.00367985
[81]	train-auc:0.794472+0.00104421	test-auc:0.778763+0.00372907
[82]	train-auc:0.794687+0.0010263	test-auc:0.778815+0.00380797
[83]	train-auc:0.794859+0.00103885	test-auc:0.778793+0.00393301
[84]	train-auc:0.795008+0.00115641	test-auc:0.778693+0.00406578
[85]	train-auc:0.795141+0.00122281	test-auc:0.778667+0.00411759
[86]	train-auc:0.79527+0.00121084	test-auc:0.778696+0.0041537
[87]	train-auc:0.79553+0.00112201	test-auc:0.778653+0.00403767
[88]	train-auc:0.795701+0.00110486	test-auc:0.778582+0.00403513
[89]	train-auc:0.795871+0.00109372	test-auc:0.778596+0.00394734
[90]	train-auc:0.796042+0.00111826	test-auc:0.778523+0.00387625
[91]	train-auc:0.796133+0.00113783	test-auc:0.778576+0.00376894
[92]	train-auc:0.796331+0.00104998	test-auc:0.778563+0.00373724
[93]	train-auc:0.79652+0.0011052	test-auc:0.77862+0.00367981
[94]	train-auc:0.796666+0.00118468	test-auc:0.778676+0.00368593
[95]	train-auc:0.796872+0.0011534	test-auc:0.778686+0.00387172
[96]	train-auc:0.796998+0.00113888	test-auc:0.778684+0.0038778
[97]	train-auc:0.797101+0.00116855	test-auc:0.778721+0.00393951
[98]	train-auc:0.797252+0.00120344	test-auc:0.778684+0.00381934
[99]	train-auc:0.797442+0.00123997	test-auc:0.778789+0.0038466
[100]	train-auc:0.797598+0.00121653	test-auc:0.778779+0.00387369
[101]	train-auc:0.797752+0.00125242	test-auc:0.778656+0.00398597
[102]	train-auc:0.797845+0.00119674	test-auc:0.778711+0.00399613
[103]	train-auc:0.797941+0.00119848	test-auc:0.778719+0.00391196
[104]	train-auc:0.798041+0.00126994	test-auc:0.778667+0.00393977
[105]	train-auc:0.79815+0.00132991	test-auc:0.778581+0.00396706
[106]	train-auc:0.798308+0.00140744	test-auc:0.778494+0.00401862
[107]	train-auc:0.798459+0.00150135	test-auc:0.778528+0.00399648
[108]	train-auc:0.798545+0.0014592	test-auc:0.778537+0.00395669
[109]	train-auc:0.798691+0.0015367	test-auc:0.778581+0.00394452
[110]	train-auc:0.798839+0.00152466	test-auc:0.778493+0.00387773
[111]	train-auc:0.798958+0.00147333	test-auc:0.778452+0.00388829
[112]	train-auc:0.799154+0.0015196	test-auc:0.778583+0.00378356
[113]	train-auc:0.799274+0.00153908	test-auc:0.778599+0.00364785
[114]	train-auc:0.79944+0.00152313	test-auc:0.778528+0.00370189
[115]	train-auc:0.799583+0.00157216	test-auc:0.778547+0.00367099
[116]	train-auc:0.799699+0.00159691	test-auc:0.778498+0.00369528
[117]	train-auc:0.799817+0.0016156	test-auc:0.778494+0.00364368
[118]	train-auc:0.799969+0.00164286	test-auc:0.778483+0.0037389
[119]	train-auc:0.800078+0.00160788	test-auc:0.778489+0.0036597
[120]	train-auc:0.800216+0.00158705	test-auc:0.778558+0.0036559
[121]	train-auc:0.800354+0.00164968	test-auc:0.778495+0.00358045
[122]	train-auc:0.800466+0.00158133	test-auc:0.778638+0.00367337
[123]	train-auc:0.800688+0.00163503	test-auc:0.778502+0.00372618
[124]	train-auc:0.800826+0.00165653	test-auc:0.778534+0.00386429

Model Report
 - train_acc : 0.8221
 - train auc score: 0.791452
 - text_acc : 0.831
 - text auc score: 0.806210

In [63]: predictors = X_train.columns.values.tolist()
    ...: xgb2 = XGBClassifier(
    ...:  learning_rate =0.01,
    ...:  n_estimators=5000,
    ...:  max_depth=2,
    ...:  min_child_weight=4,
    ...:  gamma=0,
    ...:  subsample=0.9,
    ...:  colsample_bytree=0.6,
    ...:  objective= 'binary:logistic',
    ...:  nthread=4,
    ...:  scale_pos_weight=1,
    ...:  reg_alpha=0.5,
    ...:  seed=27)
    ...: modelfit(xgb2, X_train, y_train, X_test, y_test, predictors)
    ...:  
[0]	train-auc:0.692347+0.00143208	test-auc:0.692246+0.00518667
[1]	train-auc:0.717967+0.0137098	test-auc:0.717101+0.0100686
[2]	train-auc:0.71902+0.0146637	test-auc:0.718452+0.011778
[3]	train-auc:0.729211+0.00444987	test-auc:0.725057+0.00779048
[4]	train-auc:0.735814+0.00658528	test-auc:0.733232+0.0101408
[5]	train-auc:0.7394+0.00743237	test-auc:0.737147+0.0112964
[6]	train-auc:0.738958+0.00713592	test-auc:0.73628+0.010104
[7]	train-auc:0.744655+0.00723474	test-auc:0.742207+0.0112096
[8]	train-auc:0.745033+0.0075597	test-auc:0.743204+0.0122286
[9]	train-auc:0.74588+0.00766043	test-auc:0.743087+0.0119943
[10]	train-auc:0.745862+0.00764173	test-auc:0.743069+0.0119874
^[[11]	train-auc:0.745866+0.00763497	test-auc:0.74317+0.012039
[12]	train-auc:0.751843+0.00275325	test-auc:0.74881+0.00544092
[13]	train-auc:0.751901+0.00269855	test-auc:0.748943+0.00550534
[14]	train-auc:0.751812+0.00258797	test-auc:0.748811+0.0053038
[15]	train-auc:0.751834+0.00255538	test-auc:0.748721+0.00534762
[16]	train-auc:0.751144+0.00332673	test-auc:0.748075+0.00590552
[17]	train-auc:0.752372+0.00324324	test-auc:0.749507+0.00687832
[18]	train-auc:0.752499+0.00327229	test-auc:0.749721+0.00686668
[19]	train-auc:0.752531+0.00328956	test-auc:0.74972+0.0068815
[20]	train-auc:0.752606+0.00330822	test-auc:0.749601+0.00694741
[21]	train-auc:0.753438+0.00391408	test-auc:0.749952+0.00667628
[22]	train-auc:0.753561+0.00398115	test-auc:0.750102+0.00666924
[23]	train-auc:0.753598+0.00400265	test-auc:0.750004+0.0065679
[24]	train-auc:0.753619+0.00404737	test-auc:0.749613+0.00605558
[25]	train-auc:0.754108+0.00397224	test-auc:0.750261+0.00594597
[26]	train-auc:0.754121+0.00397656	test-auc:0.750278+0.00594536
[27]	train-auc:0.75414+0.00395476	test-auc:0.75034+0.0059173
[28]	train-auc:0.754203+0.0039932	test-auc:0.750393+0.00595529
[29]	train-auc:0.75439+0.00402755	test-auc:0.750652+0.00600068
[30]	train-auc:0.754385+0.00402056	test-auc:0.750649+0.0059892
[31]	train-auc:0.75441+0.00401682	test-auc:0.750664+0.00599455
[32]	train-auc:0.754368+0.00409932	test-auc:0.750641+0.00603616
[33]	train-auc:0.754369+0.00409453	test-auc:0.750625+0.00603403
[34]	train-auc:0.755231+0.00270258	test-auc:0.751331+0.00478962
[35]	train-auc:0.755227+0.00273812	test-auc:0.751324+0.00489834
[36]	train-auc:0.755567+0.00290268	test-auc:0.751796+0.00528735
[37]	train-auc:0.755565+0.00285059	test-auc:0.751758+0.00530855
[38]	train-auc:0.756037+0.00342568	test-auc:0.752012+0.00509677
[39]	train-auc:0.755991+0.00344115	test-auc:0.752006+0.00504816
[40]	train-auc:0.756058+0.00341769	test-auc:0.751959+0.0050179
[41]	train-auc:0.75591+0.00330094	test-auc:0.751607+0.00476447
[42]	train-auc:0.755435+0.00354816	test-auc:0.751159+0.00423413
[43]	train-auc:0.75544+0.00354036	test-auc:0.751153+0.00422802
[44]	train-auc:0.756055+0.0033645	test-auc:0.751611+0.00479644
[45]	train-auc:0.756109+0.00326629	test-auc:0.751619+0.00476937
[46]	train-auc:0.755691+0.00403642	test-auc:0.751452+0.00505226
[47]	train-auc:0.756271+0.00363772	test-auc:0.751776+0.00465524
[48]	train-auc:0.756286+0.00363939	test-auc:0.75166+0.00457135
[49]	train-auc:0.756342+0.00365673	test-auc:0.751673+0.00457208
[50]	train-auc:0.756286+0.00355076	test-auc:0.751621+0.00453175
[51]	train-auc:0.756976+0.00375127	test-auc:0.752076+0.00516506
[52]	train-auc:0.757596+0.00290376	test-auc:0.753258+0.00374201
[53]	train-auc:0.757596+0.00289049	test-auc:0.753219+0.00370672
[54]	train-auc:0.758523+0.00335595	test-auc:0.753896+0.00442786
[55]	train-auc:0.758546+0.00340701	test-auc:0.753868+0.00446462
[56]	train-auc:0.758591+0.00335321	test-auc:0.753945+0.0044801
[57]	train-auc:0.758984+0.00309434	test-auc:0.753999+0.00441244
[58]	train-auc:0.758984+0.00307137	test-auc:0.754016+0.00440721
[59]	train-auc:0.75913+0.00301491	test-auc:0.754167+0.00442786
[60]	train-auc:0.759458+0.00285023	test-auc:0.754623+0.00450901
[61]	train-auc:0.760143+0.00239175	test-auc:0.755262+0.00451859
[62]	train-auc:0.760088+0.00234772	test-auc:0.755195+0.00459863
[63]	train-auc:0.760159+0.00235252	test-auc:0.755255+0.00462302
[64]	train-auc:0.760159+0.0023018	test-auc:0.75541+0.00444226
[65]	train-auc:0.760278+0.00243965	test-auc:0.755304+0.00426808
[66]	train-auc:0.760286+0.00245348	test-auc:0.75538+0.00432365
[67]	train-auc:0.760332+0.00243092	test-auc:0.755367+0.00434157
[68]	train-auc:0.760771+0.0023402	test-auc:0.755873+0.00423126
[69]	train-auc:0.760818+0.00228557	test-auc:0.75598+0.00422454
[70]	train-auc:0.760846+0.00235061	test-auc:0.756008+0.004274
[71]	train-auc:0.76094+0.00252142	test-auc:0.755943+0.00419519
[72]	train-auc:0.761063+0.0026999	test-auc:0.756017+0.00429729
[73]	train-auc:0.760957+0.00267418	test-auc:0.755929+0.00433754
[74]	train-auc:0.760911+0.00262156	test-auc:0.755969+0.00437652
[75]	train-auc:0.761041+0.00267477	test-auc:0.75619+0.00441084
[76]	train-auc:0.760938+0.002624	test-auc:0.756083+0.00447088
[77]	train-auc:0.760977+0.00259499	test-auc:0.756266+0.00453702
[78]	train-auc:0.761201+0.00224803	test-auc:0.756561+0.00458917
[79]	train-auc:0.761101+0.00218474	test-auc:0.756481+0.00454047
[80]	train-auc:0.761128+0.00218858	test-auc:0.756457+0.00442674
[81]	train-auc:0.761179+0.00220077	test-auc:0.756664+0.00407949
[82]	train-auc:0.761231+0.00239564	test-auc:0.756739+0.00379906
[83]	train-auc:0.761328+0.00225947	test-auc:0.756987+0.00388806
[84]	train-auc:0.761391+0.00229973	test-auc:0.756926+0.00388204
[85]	train-auc:0.761573+0.00251051	test-auc:0.756975+0.00407117
[86]	train-auc:0.761608+0.00252842	test-auc:0.756995+0.00410672
[87]	train-auc:0.761604+0.00253862	test-auc:0.756916+0.00405222
[88]	train-auc:0.761553+0.00256522	test-auc:0.75694+0.00411832
[89]	train-auc:0.761584+0.00252997	test-auc:0.75692+0.00406516
[90]	train-auc:0.761648+0.00249591	test-auc:0.757047+0.00414301
[91]	train-auc:0.76166+0.00247534	test-auc:0.757066+0.00415035
[92]	train-auc:0.76175+0.0024518	test-auc:0.757111+0.00412835
[93]	train-auc:0.761796+0.00249574	test-auc:0.757064+0.00405255
[94]	train-auc:0.761874+0.00245644	test-auc:0.757231+0.0041388
[95]	train-auc:0.762008+0.00220015	test-auc:0.757364+0.00409211
[96]	train-auc:0.762511+0.00226364	test-auc:0.758417+0.00406194
[97]	train-auc:0.76247+0.00227571	test-auc:0.758369+0.00407312
[98]	train-auc:0.762519+0.00224629	test-auc:0.758449+0.00415894
[99]	train-auc:0.762624+0.00211563	test-auc:0.758407+0.00416159
[100]	train-auc:0.762905+0.00226252	test-auc:0.758649+0.00380489
[101]	train-auc:0.762876+0.00229674	test-auc:0.758628+0.0037777
[102]	train-auc:0.762887+0.0021981	test-auc:0.758696+0.00379759
[103]	train-auc:0.76288+0.0021774	test-auc:0.758645+0.00374562
[104]	train-auc:0.763019+0.0022319	test-auc:0.758887+0.00391861
[105]	train-auc:0.763038+0.00218159	test-auc:0.758887+0.00392529
[106]	train-auc:0.763135+0.00216961	test-auc:0.758982+0.0038843
[107]	train-auc:0.763351+0.0018182	test-auc:0.759339+0.00395586
[108]	train-auc:0.763371+0.00189072	test-auc:0.759391+0.0040453
[109]	train-auc:0.763426+0.00185885	test-auc:0.759366+0.00399542
[110]	train-auc:0.763431+0.00178373	test-auc:0.759494+0.00398959
[111]	train-auc:0.763427+0.00173486	test-auc:0.759488+0.00396946
[112]	train-auc:0.7634+0.00181215	test-auc:0.75945+0.00394727
[113]	train-auc:0.763423+0.00191697	test-auc:0.759352+0.00396524
[114]	train-auc:0.76343+0.0019035	test-auc:0.759425+0.00391186
[115]	train-auc:0.76352+0.00198491	test-auc:0.75953+0.0039862
[116]	train-auc:0.763555+0.00193633	test-auc:0.759607+0.00398643
[117]	train-auc:0.763578+0.00198922	test-auc:0.759525+0.0038888
[118]	train-auc:0.763647+0.00191995	test-auc:0.759462+0.00388023
[119]	train-auc:0.763897+0.00182533	test-auc:0.75961+0.00397473
[120]	train-auc:0.764006+0.0017384	test-auc:0.759653+0.00397876
[121]	train-auc:0.763979+0.00173567	test-auc:0.759643+0.00398159
[122]	train-auc:0.764032+0.00172524	test-auc:0.759725+0.00393803
[123]	train-auc:0.764099+0.00165564	test-auc:0.759733+0.00394444
[124]	train-auc:0.764165+0.00162744	test-auc:0.759871+0.00394673
[125]	train-auc:0.764255+0.00159933	test-auc:0.75996+0.00400371
[126]	train-auc:0.764416+0.00147244	test-auc:0.76009+0.00411203
[127]	train-auc:0.764644+0.00152502	test-auc:0.760193+0.00407146
[128]	train-auc:0.764617+0.00149338	test-auc:0.760189+0.00401475
[129]	train-auc:0.764655+0.00156246	test-auc:0.760193+0.00403067
[130]	train-auc:0.764936+0.00163104	test-auc:0.760411+0.0041573
[131]	train-auc:0.764943+0.00164553	test-auc:0.760364+0.00409767
[132]	train-auc:0.764976+0.00164813	test-auc:0.760321+0.00408942
[133]	train-auc:0.765013+0.00162478	test-auc:0.760401+0.00417114
[134]	train-auc:0.765014+0.00161112	test-auc:0.760367+0.00417153
[135]	train-auc:0.765009+0.00163546	test-auc:0.760321+0.00414442
[136]	train-auc:0.765028+0.00159094	test-auc:0.76029+0.00415111
[137]	train-auc:0.765152+0.00161738	test-auc:0.760356+0.0041995
[138]	train-auc:0.765238+0.00154681	test-auc:0.760495+0.00425004
[139]	train-auc:0.765214+0.00156402	test-auc:0.760494+0.00426559
[140]	train-auc:0.765232+0.00159417	test-auc:0.76051+0.00423258
[141]	train-auc:0.765269+0.00157672	test-auc:0.760606+0.00423476
[142]	train-auc:0.765369+0.00158022	test-auc:0.760795+0.0041907
[143]	train-auc:0.765367+0.00157281	test-auc:0.760788+0.00419204
[144]	train-auc:0.765346+0.00152353	test-auc:0.760753+0.00414073
[145]	train-auc:0.76537+0.00154645	test-auc:0.760768+0.0041787
[146]	train-auc:0.765499+0.00155804	test-auc:0.760861+0.00421974
[147]	train-auc:0.765619+0.00165508	test-auc:0.760925+0.00430262
[148]	train-auc:0.765669+0.00163545	test-auc:0.761023+0.00424459
[149]	train-auc:0.76574+0.00167731	test-auc:0.761014+0.00423305
[150]	train-auc:0.765748+0.00167777	test-auc:0.761151+0.00421217
[151]	train-auc:0.765846+0.00165807	test-auc:0.76129+0.00415012
[152]	train-auc:0.765861+0.00160321	test-auc:0.761381+0.00417802
[153]	train-auc:0.765909+0.00159963	test-auc:0.761436+0.0041741
[154]	train-auc:0.765876+0.00158558	test-auc:0.761435+0.00418896
[155]	train-auc:0.765871+0.00159497	test-auc:0.761455+0.00415032
[156]	train-auc:0.765972+0.00156654	test-auc:0.761508+0.00408909
[157]	train-auc:0.766049+0.00152299	test-auc:0.761648+0.00396336
[158]	train-auc:0.76609+0.00153085	test-auc:0.761619+0.00387911
[159]	train-auc:0.766099+0.00156764	test-auc:0.76163+0.00387934
[160]	train-auc:0.766141+0.00159696	test-auc:0.76169+0.00380895
[161]	train-auc:0.766233+0.00161022	test-auc:0.761706+0.00382199
[162]	train-auc:0.76623+0.00164132	test-auc:0.761681+0.00381947
[163]	train-auc:0.766262+0.00162111	test-auc:0.761743+0.00381913
[164]	train-auc:0.766311+0.00158993	test-auc:0.761798+0.0037799
[165]	train-auc:0.766294+0.00159016	test-auc:0.761777+0.00378452
[166]	train-auc:0.766355+0.00157562	test-auc:0.761895+0.00373584
[167]	train-auc:0.766434+0.00162565	test-auc:0.761883+0.00376748
[168]	train-auc:0.766484+0.00162376	test-auc:0.761952+0.00385169
[169]	train-auc:0.766514+0.00160929	test-auc:0.761915+0.00380594
[170]	train-auc:0.766529+0.00159206	test-auc:0.76187+0.00377962
[171]	train-auc:0.766554+0.00162114	test-auc:0.761863+0.00376445
[172]	train-auc:0.766581+0.00163409	test-auc:0.761887+0.00378497
[173]	train-auc:0.766674+0.00151018	test-auc:0.762067+0.00375454
[174]	train-auc:0.766794+0.00145161	test-auc:0.762188+0.00362404
[175]	train-auc:0.766821+0.00150031	test-auc:0.76216+0.00360025
[176]	train-auc:0.766907+0.00150644	test-auc:0.762206+0.00350548
[177]	train-auc:0.766859+0.00146777	test-auc:0.762155+0.00352415
[178]	train-auc:0.766989+0.00148256	test-auc:0.762229+0.00355671
[179]	train-auc:0.767008+0.00145304	test-auc:0.762263+0.00351431
[180]	train-auc:0.76704+0.00146612	test-auc:0.762295+0.00341343
[181]	train-auc:0.767076+0.00143906	test-auc:0.762405+0.00348242
[182]	train-auc:0.767113+0.00139343	test-auc:0.762423+0.0034773
[183]	train-auc:0.767184+0.00139068	test-auc:0.762454+0.00349142
[184]	train-auc:0.767224+0.00142041	test-auc:0.762478+0.00347496
[185]	train-auc:0.767264+0.00141944	test-auc:0.762499+0.00349493
[186]	train-auc:0.767253+0.00144636	test-auc:0.762492+0.0035324
[187]	train-auc:0.767253+0.0014028	test-auc:0.762502+0.00352513
[188]	train-auc:0.767334+0.00136358	test-auc:0.762592+0.00349229
[189]	train-auc:0.76736+0.00141223	test-auc:0.762611+0.00351877
[190]	train-auc:0.767375+0.00139338	test-auc:0.76262+0.0035001
[191]	train-auc:0.767433+0.00141714	test-auc:0.762636+0.00352365
[192]	train-auc:0.767454+0.00141776	test-auc:0.762619+0.00345915
[193]	train-auc:0.76751+0.00143902	test-auc:0.762672+0.00346951
[194]	train-auc:0.767521+0.00145619	test-auc:0.762673+0.00349046
[195]	train-auc:0.767571+0.00144243	test-auc:0.762701+0.00345807
[196]	train-auc:0.767702+0.0013528	test-auc:0.762857+0.00351105
[197]	train-auc:0.767762+0.00132039	test-auc:0.762882+0.00349158
[198]	train-auc:0.767785+0.00129914	test-auc:0.762933+0.00348253
[199]	train-auc:0.767804+0.00133412	test-auc:0.762923+0.00347902
[200]	train-auc:0.767788+0.00135016	test-auc:0.762916+0.00341938
[201]	train-auc:0.767833+0.00137218	test-auc:0.762951+0.00343641
[202]	train-auc:0.767845+0.00137574	test-auc:0.762971+0.00347756
[203]	train-auc:0.767893+0.0013483	test-auc:0.76301+0.00338688
[204]	train-auc:0.767924+0.00135799	test-auc:0.763024+0.00341573
[205]	train-auc:0.767952+0.00134304	test-auc:0.763115+0.00341001
[206]	train-auc:0.768004+0.00133109	test-auc:0.763122+0.00340772
[207]	train-auc:0.768055+0.00133548	test-auc:0.763139+0.00339673
[208]	train-auc:0.768093+0.00137793	test-auc:0.763167+0.00340152
[209]	train-auc:0.768131+0.00135367	test-auc:0.763182+0.00338335
[210]	train-auc:0.768176+0.00136691	test-auc:0.763238+0.00338749
[211]	train-auc:0.768205+0.00136073	test-auc:0.763203+0.00338785
[212]	train-auc:0.768253+0.00136062	test-auc:0.763227+0.00339055
[213]	train-auc:0.768279+0.0013389	test-auc:0.763338+0.00335336
[214]	train-auc:0.768305+0.00134652	test-auc:0.763397+0.00333576
[215]	train-auc:0.768331+0.00132562	test-auc:0.763411+0.0033493
[216]	train-auc:0.768403+0.00131071	test-auc:0.763434+0.00329264
[217]	train-auc:0.768393+0.00133329	test-auc:0.763397+0.00323334
[218]	train-auc:0.768414+0.00133512	test-auc:0.7634+0.00324441
[219]	train-auc:0.768479+0.00135181	test-auc:0.763464+0.00324892
[220]	train-auc:0.768489+0.00133929	test-auc:0.763444+0.00325892
[221]	train-auc:0.768532+0.00134956	test-auc:0.763447+0.00325103
[222]	train-auc:0.768573+0.00133267	test-auc:0.763476+0.00320095
[223]	train-auc:0.768644+0.00131702	test-auc:0.763568+0.00313044
[224]	train-auc:0.768797+0.00128003	test-auc:0.763674+0.00316556
[225]	train-auc:0.768814+0.00126339	test-auc:0.763705+0.00312059
[226]	train-auc:0.768952+0.00126795	test-auc:0.763801+0.00312011
[227]	train-auc:0.768965+0.0012649	test-auc:0.763871+0.00315499
[228]	train-auc:0.769012+0.00125284	test-auc:0.763916+0.00316135
[229]	train-auc:0.76905+0.00128821	test-auc:0.763952+0.00318592
[230]	train-auc:0.769073+0.0012787	test-auc:0.76395+0.00321623
[231]	train-auc:0.769093+0.00127414	test-auc:0.764009+0.00323783
[232]	train-auc:0.769139+0.0012763	test-auc:0.764072+0.00323594
[233]	train-auc:0.769382+0.00150268	test-auc:0.764249+0.00332326
[234]	train-auc:0.769387+0.00147212	test-auc:0.764265+0.00333878
[235]	train-auc:0.769427+0.00146674	test-auc:0.764299+0.00333015
[236]	train-auc:0.769457+0.0014958	test-auc:0.764324+0.0033148
[237]	train-auc:0.769491+0.00149216	test-auc:0.764351+0.00330652
[238]	train-auc:0.769571+0.00146824	test-auc:0.764397+0.00332579
[239]	train-auc:0.76958+0.00144653	test-auc:0.764394+0.00330988
[240]	train-auc:0.769594+0.00143035	test-auc:0.76441+0.00331082
[241]	train-auc:0.769707+0.00147317	test-auc:0.764552+0.00308861
[242]	train-auc:0.769738+0.00144096	test-auc:0.7646+0.00303734
[243]	train-auc:0.769784+0.00138812	test-auc:0.76466+0.00305277
[244]	train-auc:0.769811+0.0014121	test-auc:0.76464+0.00305247
[245]	train-auc:0.769843+0.00141474	test-auc:0.764684+0.00306134
[246]	train-auc:0.769877+0.00143461	test-auc:0.764702+0.00308004
[247]	train-auc:0.76992+0.00144533	test-auc:0.764738+0.00308064
[248]	train-auc:0.769944+0.00144281	test-auc:0.76476+0.00308079
[249]	train-auc:0.769971+0.0014269	test-auc:0.764788+0.00304624
[250]	train-auc:0.770009+0.00145129	test-auc:0.764825+0.00305303
[251]	train-auc:0.770073+0.00150207	test-auc:0.764847+0.00306384
[252]	train-auc:0.770262+0.00148934	test-auc:0.764964+0.00311519
[253]	train-auc:0.770308+0.0015027	test-auc:0.764992+0.00310002
[254]	train-auc:0.770369+0.0014835	test-auc:0.765016+0.00310735
[255]	train-auc:0.770377+0.00149719	test-auc:0.765082+0.00312702
[256]	train-auc:0.77042+0.00151578	test-auc:0.765148+0.00314617
[257]	train-auc:0.770425+0.0014927	test-auc:0.765125+0.00309921
[258]	train-auc:0.770448+0.00152069	test-auc:0.765124+0.00311853
[259]	train-auc:0.770467+0.00151766	test-auc:0.765136+0.00310604
[260]	train-auc:0.770509+0.0015202	test-auc:0.765159+0.00308784
[261]	train-auc:0.770524+0.00152011	test-auc:0.765165+0.00308541
[262]	train-auc:0.770584+0.00154079	test-auc:0.765218+0.00312398
[263]	train-auc:0.770638+0.00156194	test-auc:0.765273+0.00317376
[264]	train-auc:0.770659+0.00157448	test-auc:0.765301+0.00317867
[265]	train-auc:0.770702+0.00152377	test-auc:0.765394+0.00316071
[266]	train-auc:0.770856+0.00129701	test-auc:0.765559+0.00324449
[267]	train-auc:0.770898+0.00130299	test-auc:0.765572+0.00325958
[268]	train-auc:0.771102+0.00124722	test-auc:0.765793+0.00290178
[269]	train-auc:0.771138+0.00127532	test-auc:0.765819+0.00287161
[270]	train-auc:0.771173+0.0013421	test-auc:0.765833+0.00285527
[271]	train-auc:0.771184+0.00138517	test-auc:0.765872+0.00288757
[272]	train-auc:0.771216+0.0013947	test-auc:0.765905+0.00290017
[273]	train-auc:0.771237+0.00137197	test-auc:0.765922+0.00291458
[274]	train-auc:0.771286+0.00133555	test-auc:0.765964+0.00285293
[275]	train-auc:0.771322+0.00131752	test-auc:0.766019+0.00285798
[276]	train-auc:0.771372+0.00133009	test-auc:0.766087+0.00289438
[277]	train-auc:0.771426+0.0013202	test-auc:0.766143+0.00278596
[278]	train-auc:0.771497+0.001322	test-auc:0.766171+0.00269226
[279]	train-auc:0.77154+0.0013156	test-auc:0.766202+0.00265712
[280]	train-auc:0.771603+0.00131333	test-auc:0.76628+0.00259248
[281]	train-auc:0.771632+0.00132334	test-auc:0.766293+0.00256482
[282]	train-auc:0.771721+0.00128648	test-auc:0.766362+0.0025775
[283]	train-auc:0.771752+0.00124902	test-auc:0.766428+0.00254929
[284]	train-auc:0.771817+0.00125621	test-auc:0.766447+0.00256412
[285]	train-auc:0.771858+0.00124892	test-auc:0.766489+0.0025541
[286]	train-auc:0.771883+0.00124656	test-auc:0.766493+0.00256074
[287]	train-auc:0.771938+0.00121857	test-auc:0.766533+0.00249333
[288]	train-auc:0.772+0.00119607	test-auc:0.766585+0.00253015
[289]	train-auc:0.772029+0.00119756	test-auc:0.766609+0.00247383
[290]	train-auc:0.77205+0.00120606	test-auc:0.766629+0.00246977
[291]	train-auc:0.772065+0.00120607	test-auc:0.766614+0.00247287
[292]	train-auc:0.772173+0.0012269	test-auc:0.76667+0.00250983
[293]	train-auc:0.772185+0.00121665	test-auc:0.766696+0.00253654
[294]	train-auc:0.772216+0.00121876	test-auc:0.766738+0.00255627
[295]	train-auc:0.772259+0.00120612	test-auc:0.766767+0.00255986
[296]	train-auc:0.772288+0.00123631	test-auc:0.766762+0.00248403
[297]	train-auc:0.772337+0.00121621	test-auc:0.766835+0.00250483
[298]	train-auc:0.77236+0.00122251	test-auc:0.766847+0.00250694
[299]	train-auc:0.772395+0.00123033	test-auc:0.766903+0.00248983
[300]	train-auc:0.772428+0.00125135	test-auc:0.766944+0.00248593
[301]	train-auc:0.772472+0.0012625	test-auc:0.767014+0.00251563
[302]	train-auc:0.772488+0.00125624	test-auc:0.767024+0.00248939
[303]	train-auc:0.772532+0.00128392	test-auc:0.767075+0.00245209
[304]	train-auc:0.772556+0.00128569	test-auc:0.76709+0.00244424
[305]	train-auc:0.772598+0.00132197	test-auc:0.767124+0.00242707
[306]	train-auc:0.772756+0.00136124	test-auc:0.767288+0.00243171
[307]	train-auc:0.772794+0.00138709	test-auc:0.76732+0.00245832
[308]	train-auc:0.772827+0.00138729	test-auc:0.767345+0.00246385
[309]	train-auc:0.772874+0.00136179	test-auc:0.767392+0.00245957
[310]	train-auc:0.772901+0.00137562	test-auc:0.7674+0.00243742
[311]	train-auc:0.772924+0.001388	test-auc:0.767414+0.00243064
[312]	train-auc:0.772956+0.00139212	test-auc:0.767424+0.00240699
[313]	train-auc:0.773016+0.00131771	test-auc:0.7675+0.00243284
[314]	train-auc:0.773041+0.00126919	test-auc:0.767541+0.00247166
[315]	train-auc:0.773081+0.00126731	test-auc:0.767556+0.00246961
[316]	train-auc:0.773106+0.00127003	test-auc:0.767593+0.00245757
[317]	train-auc:0.773153+0.00128632	test-auc:0.76763+0.00247071
[318]	train-auc:0.773212+0.00132577	test-auc:0.767673+0.00243962
[319]	train-auc:0.773256+0.00131802	test-auc:0.767729+0.00246844
[320]	train-auc:0.773301+0.00132439	test-auc:0.767729+0.00248099
[321]	train-auc:0.773342+0.00131973	test-auc:0.767742+0.00248343
[322]	train-auc:0.773426+0.00119633	test-auc:0.767831+0.00249183
[323]	train-auc:0.773455+0.00122295	test-auc:0.767854+0.00249477
[324]	train-auc:0.773468+0.00122592	test-auc:0.767875+0.002522
[325]	train-auc:0.77355+0.00123536	test-auc:0.767927+0.00253929
[326]	train-auc:0.773563+0.0012515	test-auc:0.76792+0.00253394
[327]	train-auc:0.773606+0.00128345	test-auc:0.767947+0.0024966
[328]	train-auc:0.773629+0.00128285	test-auc:0.76798+0.00250238
[329]	train-auc:0.773645+0.00127277	test-auc:0.768004+0.00251582
[330]	train-auc:0.77368+0.00128408	test-auc:0.768017+0.00250923
[331]	train-auc:0.773705+0.00127978	test-auc:0.768007+0.00249905
[332]	train-auc:0.77377+0.00133588	test-auc:0.768068+0.00249468
[333]	train-auc:0.773847+0.00127325	test-auc:0.768113+0.00251121
[334]	train-auc:0.773909+0.00130289	test-auc:0.768157+0.00250672
[335]	train-auc:0.77394+0.00131014	test-auc:0.768197+0.00247975
[336]	train-auc:0.773985+0.00127076	test-auc:0.768268+0.00248402
[337]	train-auc:0.774012+0.00124809	test-auc:0.768282+0.00247029
[338]	train-auc:0.774057+0.00123078	test-auc:0.768335+0.00245598
[339]	train-auc:0.774083+0.00121954	test-auc:0.768357+0.00246255
[340]	train-auc:0.774118+0.00121371	test-auc:0.7684+0.00245309
[341]	train-auc:0.774153+0.00121649	test-auc:0.768439+0.00247436
[342]	train-auc:0.77421+0.00122661	test-auc:0.768447+0.00247331
[343]	train-auc:0.774249+0.00125055	test-auc:0.76852+0.0024581
[344]	train-auc:0.774273+0.00125302	test-auc:0.76854+0.00246872
[345]	train-auc:0.774328+0.00122458	test-auc:0.768625+0.00248287
[346]	train-auc:0.774377+0.00125448	test-auc:0.76866+0.00245754
[347]	train-auc:0.774438+0.00125791	test-auc:0.768705+0.00242003
[348]	train-auc:0.774472+0.0012712	test-auc:0.768748+0.00240873
[349]	train-auc:0.774512+0.0012869	test-auc:0.768778+0.00241018
[350]	train-auc:0.774539+0.00129787	test-auc:0.76879+0.0024205
[351]	train-auc:0.774543+0.00132205	test-auc:0.768791+0.00241479
[352]	train-auc:0.774607+0.00128247	test-auc:0.768827+0.00241085
[353]	train-auc:0.774623+0.00126715	test-auc:0.76883+0.00241251
[354]	train-auc:0.774662+0.00126209	test-auc:0.768883+0.00237474
[355]	train-auc:0.774693+0.00126994	test-auc:0.768906+0.00235473
[356]	train-auc:0.774712+0.00129776	test-auc:0.768913+0.00233179
[357]	train-auc:0.77476+0.00129441	test-auc:0.768967+0.00233519
[358]	train-auc:0.774776+0.0013025	test-auc:0.76897+0.00234663
[359]	train-auc:0.774794+0.00130256	test-auc:0.769004+0.00233873
[360]	train-auc:0.774816+0.00132328	test-auc:0.769034+0.00234891
[361]	train-auc:0.774857+0.00130112	test-auc:0.769051+0.00235381
[362]	train-auc:0.774906+0.00130203	test-auc:0.769077+0.0023419
[363]	train-auc:0.774961+0.0012838	test-auc:0.769084+0.00234276
[364]	train-auc:0.774997+0.00128646	test-auc:0.769097+0.00232573
[365]	train-auc:0.775023+0.00129207	test-auc:0.769105+0.00232623
[366]	train-auc:0.77506+0.00129819	test-auc:0.769152+0.00232468
[367]	train-auc:0.775112+0.00130483	test-auc:0.769184+0.00235388
[368]	train-auc:0.775135+0.00131731	test-auc:0.769195+0.00234842
[369]	train-auc:0.77517+0.00132861	test-auc:0.769224+0.00230533
[370]	train-auc:0.775201+0.00131755	test-auc:0.769251+0.0022948
[371]	train-auc:0.775243+0.00131308	test-auc:0.769304+0.00229537
[372]	train-auc:0.775291+0.00133105	test-auc:0.76933+0.00229567
[373]	train-auc:0.775318+0.00133625	test-auc:0.769361+0.00228218
[374]	train-auc:0.775358+0.00135796	test-auc:0.769392+0.00227611
[375]	train-auc:0.77539+0.0013658	test-auc:0.769421+0.00225967
[376]	train-auc:0.775407+0.00136867	test-auc:0.769434+0.0022717
[377]	train-auc:0.775436+0.00137928	test-auc:0.769437+0.00224989
[378]	train-auc:0.775463+0.00137679	test-auc:0.769485+0.00227382
[379]	train-auc:0.775501+0.00139489	test-auc:0.769504+0.00227878
[380]	train-auc:0.775528+0.0013934	test-auc:0.769517+0.00228423
[381]	train-auc:0.775559+0.00140452	test-auc:0.769548+0.0022881
[382]	train-auc:0.775616+0.00140083	test-auc:0.769589+0.00226907
[383]	train-auc:0.775628+0.00137712	test-auc:0.769604+0.00226079
[384]	train-auc:0.775658+0.00136004	test-auc:0.769625+0.00224669
[385]	train-auc:0.775671+0.00136826	test-auc:0.769651+0.00223989
[386]	train-auc:0.77569+0.00136909	test-auc:0.769675+0.00225305
[387]	train-auc:0.775732+0.00135605	test-auc:0.769684+0.00222493
[388]	train-auc:0.775772+0.00133729	test-auc:0.769711+0.00221649
[389]	train-auc:0.775792+0.00133957	test-auc:0.769729+0.00219906
[390]	train-auc:0.775819+0.00135078	test-auc:0.769749+0.00219654
[391]	train-auc:0.775869+0.00134804	test-auc:0.769782+0.00219506
[392]	train-auc:0.7759+0.00133824	test-auc:0.769779+0.002206
[393]	train-auc:0.775932+0.0013462	test-auc:0.769791+0.00221858
[394]	train-auc:0.775992+0.00135726	test-auc:0.769825+0.00220106
[395]	train-auc:0.776016+0.00135019	test-auc:0.769844+0.00220036
[396]	train-auc:0.776043+0.00136362	test-auc:0.769867+0.00218766
[397]	train-auc:0.776082+0.00132908	test-auc:0.769897+0.0021607
[398]	train-auc:0.776121+0.00134228	test-auc:0.769927+0.00217502
[399]	train-auc:0.776157+0.00133676	test-auc:0.769951+0.002169
[400]	train-auc:0.776182+0.00132721	test-auc:0.769971+0.00214961
[401]	train-auc:0.776201+0.00132596	test-auc:0.769973+0.00215112
[402]	train-auc:0.77624+0.00134209	test-auc:0.769993+0.00215546
[403]	train-auc:0.776267+0.00132657	test-auc:0.770016+0.00216718
[404]	train-auc:0.776298+0.00134351	test-auc:0.770046+0.0021513
[405]	train-auc:0.776323+0.00134392	test-auc:0.770061+0.00217156
[406]	train-auc:0.776355+0.00135872	test-auc:0.770082+0.00215
[407]	train-auc:0.776387+0.00134842	test-auc:0.770099+0.00215584
[408]	train-auc:0.776417+0.00136234	test-auc:0.770115+0.0021299
[409]	train-auc:0.776455+0.00138014	test-auc:0.770149+0.00212935
[410]	train-auc:0.776484+0.00137585	test-auc:0.770197+0.00209799
[411]	train-auc:0.776529+0.00138807	test-auc:0.770231+0.00209744
[412]	train-auc:0.776552+0.00138505	test-auc:0.77021+0.00208302
[413]	train-auc:0.776595+0.00138216	test-auc:0.770222+0.00209341
[414]	train-auc:0.776619+0.00139081	test-auc:0.770258+0.00209259
[415]	train-auc:0.776638+0.00139408	test-auc:0.770259+0.00209632
[416]	train-auc:0.776666+0.00139984	test-auc:0.770282+0.00208806
[417]	train-auc:0.776712+0.00140185	test-auc:0.770331+0.00206009
[418]	train-auc:0.776741+0.00140208	test-auc:0.770354+0.00204653
[419]	train-auc:0.776778+0.00141748	test-auc:0.770359+0.00204478
[420]	train-auc:0.776835+0.00138372	test-auc:0.770403+0.0020364
[421]	train-auc:0.776887+0.00136001	test-auc:0.770406+0.00202087
[422]	train-auc:0.776912+0.00135714	test-auc:0.770426+0.00202927
[423]	train-auc:0.776959+0.00133388	test-auc:0.770447+0.00201998
[424]	train-auc:0.776996+0.00134554	test-auc:0.770461+0.00202085
[425]	train-auc:0.777023+0.0013655	test-auc:0.770488+0.00202831
[426]	train-auc:0.777039+0.00137395	test-auc:0.770505+0.00199505
[427]	train-auc:0.777058+0.00136844	test-auc:0.770529+0.00198579
[428]	train-auc:0.777087+0.00135946	test-auc:0.77054+0.0019808
[429]	train-auc:0.777102+0.00134638	test-auc:0.770566+0.00198139
[430]	train-auc:0.777144+0.00133947	test-auc:0.770584+0.00198262
[431]	train-auc:0.777168+0.00133262	test-auc:0.770593+0.00198604
[432]	train-auc:0.777207+0.00132016	test-auc:0.770601+0.00197724
[433]	train-auc:0.777232+0.00132313	test-auc:0.770612+0.00196518
[434]	train-auc:0.777274+0.00130616	test-auc:0.770632+0.00195958
[435]	train-auc:0.777331+0.00129103	test-auc:0.770679+0.00197518
[436]	train-auc:0.777363+0.00130118	test-auc:0.7707+0.00193403
[437]	train-auc:0.777398+0.00128843	test-auc:0.770725+0.0019546
[438]	train-auc:0.777421+0.00129703	test-auc:0.770738+0.00195756
[439]	train-auc:0.777439+0.0013006	test-auc:0.770751+0.00194199
[440]	train-auc:0.777468+0.00128334	test-auc:0.770781+0.00195079
[441]	train-auc:0.777496+0.0012768	test-auc:0.77082+0.00194163
[442]	train-auc:0.777534+0.00130939	test-auc:0.770832+0.00192553
[443]	train-auc:0.777558+0.0013166	test-auc:0.770861+0.00192693
[444]	train-auc:0.777596+0.00133051	test-auc:0.770883+0.00194425
[445]	train-auc:0.777642+0.00133144	test-auc:0.770904+0.00196154
[446]	train-auc:0.777676+0.00133577	test-auc:0.770928+0.00195927
[447]	train-auc:0.777702+0.0013319	test-auc:0.770937+0.00195157
[448]	train-auc:0.777735+0.00131037	test-auc:0.770957+0.00197053
[449]	train-auc:0.77777+0.0013001	test-auc:0.770977+0.00195892
[450]	train-auc:0.777799+0.00132113	test-auc:0.771009+0.00196334
[451]	train-auc:0.777826+0.00131452	test-auc:0.771023+0.00195524
[452]	train-auc:0.777847+0.00131919	test-auc:0.771032+0.00197393
[453]	train-auc:0.777875+0.00131673	test-auc:0.771066+0.00195564
[454]	train-auc:0.777907+0.00131546	test-auc:0.771093+0.00194729
[455]	train-auc:0.777933+0.00131569	test-auc:0.771112+0.00195195
[456]	train-auc:0.777956+0.00131412	test-auc:0.771115+0.00193879
[457]	train-auc:0.777981+0.00131	test-auc:0.771139+0.0019478
[458]	train-auc:0.778015+0.00129692	test-auc:0.771157+0.00197075
[459]	train-auc:0.778031+0.0012982	test-auc:0.771155+0.00197314
[460]	train-auc:0.778051+0.00130111	test-auc:0.771172+0.00195396
[461]	train-auc:0.778097+0.00130356	test-auc:0.771211+0.00193715
[462]	train-auc:0.778122+0.00128553	test-auc:0.771217+0.00194691
[463]	train-auc:0.778153+0.00129876	test-auc:0.771221+0.00194669
[464]	train-auc:0.778176+0.0012988	test-auc:0.771236+0.00194054
[465]	train-auc:0.778204+0.00129674	test-auc:0.771254+0.00192955
[466]	train-auc:0.778224+0.00128968	test-auc:0.771255+0.00193296
[467]	train-auc:0.77825+0.00129007	test-auc:0.77126+0.00192238
[468]	train-auc:0.778286+0.00128163	test-auc:0.771294+0.00191497
[469]	train-auc:0.778322+0.00127045	test-auc:0.771327+0.00191981
[470]	train-auc:0.778352+0.00126901	test-auc:0.771341+0.00192172
[471]	train-auc:0.778388+0.00126866	test-auc:0.771357+0.00192972
[472]	train-auc:0.77844+0.00127758	test-auc:0.771383+0.00192948
[473]	train-auc:0.778466+0.00128667	test-auc:0.771393+0.00191695
[474]	train-auc:0.778494+0.00128455	test-auc:0.771416+0.00190995
[475]	train-auc:0.778524+0.00129022	test-auc:0.771443+0.00190748
[476]	train-auc:0.778546+0.00129626	test-auc:0.771461+0.00190215
[477]	train-auc:0.778577+0.00130141	test-auc:0.771497+0.00191554
[478]	train-auc:0.778607+0.00129542	test-auc:0.771511+0.00189858
[479]	train-auc:0.778637+0.001313	test-auc:0.771525+0.0019055
[480]	train-auc:0.778678+0.00133199	test-auc:0.771544+0.00191718
[481]	train-auc:0.778698+0.00133484	test-auc:0.771563+0.00190372
[482]	train-auc:0.778723+0.00131773	test-auc:0.771595+0.00191289
[483]	train-auc:0.778766+0.00131345	test-auc:0.771617+0.00191928
[484]	train-auc:0.778824+0.00131516	test-auc:0.771655+0.00192796
[485]	train-auc:0.778858+0.00133072	test-auc:0.771689+0.00193539
[486]	train-auc:0.778891+0.00132455	test-auc:0.771709+0.00192739
[487]	train-auc:0.77892+0.00131426	test-auc:0.771722+0.0019317
[488]	train-auc:0.778942+0.00131683	test-auc:0.771749+0.00194134
[489]	train-auc:0.778964+0.00132608	test-auc:0.771762+0.00192772
[490]	train-auc:0.778997+0.00133424	test-auc:0.771774+0.00193669
[491]	train-auc:0.779035+0.00132697	test-auc:0.771813+0.00192693
[492]	train-auc:0.779068+0.00132089	test-auc:0.771824+0.00192522
[493]	train-auc:0.779088+0.00132432	test-auc:0.771829+0.00192261
[494]	train-auc:0.779106+0.00132234	test-auc:0.771849+0.00192864
[495]	train-auc:0.779137+0.00133582	test-auc:0.771858+0.00193542
[496]	train-auc:0.779168+0.00133695	test-auc:0.77188+0.00192913
[497]	train-auc:0.779204+0.00134507	test-auc:0.771893+0.00194019
[498]	train-auc:0.779235+0.00134406	test-auc:0.771911+0.00194789
[499]	train-auc:0.779264+0.00135271	test-auc:0.771916+0.00193861
[500]	train-auc:0.779293+0.00134543	test-auc:0.771934+0.00194492
[501]	train-auc:0.779335+0.00136022	test-auc:0.771968+0.00192748
[502]	train-auc:0.779365+0.00136536	test-auc:0.771993+0.00191788
[503]	train-auc:0.779388+0.00136752	test-auc:0.772002+0.00190563
[504]	train-auc:0.779412+0.00136886	test-auc:0.772015+0.00190537
[505]	train-auc:0.779447+0.00136817	test-auc:0.772025+0.0019
[506]	train-auc:0.779482+0.00135458	test-auc:0.772043+0.00189723
[507]	train-auc:0.779511+0.00134476	test-auc:0.772072+0.00190478
[508]	train-auc:0.779539+0.00134994	test-auc:0.772095+0.00190489
[509]	train-auc:0.779565+0.00135691	test-auc:0.772124+0.00189108
[510]	train-auc:0.779585+0.00136341	test-auc:0.772148+0.00189032
[511]	train-auc:0.779613+0.00136345	test-auc:0.772156+0.00189028
[512]	train-auc:0.779639+0.00135394	test-auc:0.772181+0.00190342
[513]	train-auc:0.779671+0.00135804	test-auc:0.772189+0.00191099
[514]	train-auc:0.779717+0.00136752	test-auc:0.772221+0.00189977
[515]	train-auc:0.779742+0.00137771	test-auc:0.772237+0.00191495
[516]	train-auc:0.77976+0.00137867	test-auc:0.772265+0.00190787
[517]	train-auc:0.779784+0.00137782	test-auc:0.772284+0.00190215
[518]	train-auc:0.779801+0.0013779	test-auc:0.772293+0.00190561
[519]	train-auc:0.77982+0.00136999	test-auc:0.772314+0.00190561
[520]	train-auc:0.779853+0.00136011	test-auc:0.772336+0.00191648
[521]	train-auc:0.779886+0.0013562	test-auc:0.772356+0.00192899
[522]	train-auc:0.779918+0.00136811	test-auc:0.772388+0.00191636
[523]	train-auc:0.779946+0.00136008	test-auc:0.7724+0.00192264
[524]	train-auc:0.779964+0.00136271	test-auc:0.772409+0.00191514
[525]	train-auc:0.779985+0.00136702	test-auc:0.772405+0.00189915
[526]	train-auc:0.780016+0.00136216	test-auc:0.772422+0.00190516
[527]	train-auc:0.78004+0.00136379	test-auc:0.772429+0.00189655
[528]	train-auc:0.780059+0.00135945	test-auc:0.772446+0.00188218
[529]	train-auc:0.780087+0.00135981	test-auc:0.772466+0.00189157
[530]	train-auc:0.780113+0.00137659	test-auc:0.772475+0.00189128
[531]	train-auc:0.780132+0.00137957	test-auc:0.772508+0.00186787
[532]	train-auc:0.780156+0.00138642	test-auc:0.772511+0.00186845
[533]	train-auc:0.780178+0.00138709	test-auc:0.772534+0.00186595
[534]	train-auc:0.7802+0.00138647	test-auc:0.772547+0.00185823
[535]	train-auc:0.780226+0.0013819	test-auc:0.772562+0.00186345
[536]	train-auc:0.780262+0.00136988	test-auc:0.772596+0.00186783
[537]	train-auc:0.780292+0.00138483	test-auc:0.772607+0.00188071
[538]	train-auc:0.780303+0.00138864	test-auc:0.772608+0.00188012
[539]	train-auc:0.780322+0.00137978	test-auc:0.772612+0.00187492
[540]	train-auc:0.780351+0.00138666	test-auc:0.772628+0.00187924
[541]	train-auc:0.78037+0.00139125	test-auc:0.772635+0.00186622
[542]	train-auc:0.780395+0.00138429	test-auc:0.772659+0.00186042
[543]	train-auc:0.780416+0.00138225	test-auc:0.772662+0.00186311
[544]	train-auc:0.780437+0.00137675	test-auc:0.772673+0.00186136
[545]	train-auc:0.780461+0.00138355	test-auc:0.772686+0.00185038
[546]	train-auc:0.780492+0.00138563	test-auc:0.772703+0.00186862
[547]	train-auc:0.780516+0.0013814	test-auc:0.772708+0.00187927
[548]	train-auc:0.780549+0.00138322	test-auc:0.772727+0.00185496
[549]	train-auc:0.780562+0.00137962	test-auc:0.772739+0.00186257
[550]	train-auc:0.780601+0.00138115	test-auc:0.772754+0.00187098
[551]	train-auc:0.780624+0.00138784	test-auc:0.772766+0.00187133
[552]	train-auc:0.780654+0.00138784	test-auc:0.772791+0.00188802
[553]	train-auc:0.780678+0.00138853	test-auc:0.772803+0.00188992
[554]	train-auc:0.780707+0.00138658	test-auc:0.77283+0.0018879
[555]	train-auc:0.780743+0.00136455	test-auc:0.772867+0.00190503
[556]	train-auc:0.780763+0.00136106	test-auc:0.772875+0.0019131
[557]	train-auc:0.7808+0.00136378	test-auc:0.772888+0.00192205
[558]	train-auc:0.780832+0.00136138	test-auc:0.772913+0.0019389
[559]	train-auc:0.780851+0.00135702	test-auc:0.772916+0.00193847
[560]	train-auc:0.780872+0.00136145	test-auc:0.772927+0.00194439
[561]	train-auc:0.780897+0.00135705	test-auc:0.77294+0.00195232
[562]	train-auc:0.780917+0.00134629	test-auc:0.772942+0.00195526
[563]	train-auc:0.780945+0.00134982	test-auc:0.772956+0.00195627
[564]	train-auc:0.780978+0.00135124	test-auc:0.772979+0.00193734
[565]	train-auc:0.781009+0.00136293	test-auc:0.773004+0.00193918
[566]	train-auc:0.78103+0.0013594	test-auc:0.773014+0.0019387
[567]	train-auc:0.781054+0.00135443	test-auc:0.773029+0.00194066
[568]	train-auc:0.781078+0.00134718	test-auc:0.773041+0.00193423
[569]	train-auc:0.781108+0.00134845	test-auc:0.773064+0.00191864
[570]	train-auc:0.781133+0.00134167	test-auc:0.773071+0.00191769
[571]	train-auc:0.781168+0.00133984	test-auc:0.773087+0.00191257
[572]	train-auc:0.781196+0.00133279	test-auc:0.773103+0.00191957
[573]	train-auc:0.781221+0.00132677	test-auc:0.773125+0.00192421
[574]	train-auc:0.781246+0.00132529	test-auc:0.773148+0.00192699
[575]	train-auc:0.781275+0.00133213	test-auc:0.773178+0.00194779
[576]	train-auc:0.781304+0.00133562	test-auc:0.773187+0.00194624
[577]	train-auc:0.781322+0.00133573	test-auc:0.773201+0.00193465
[578]	train-auc:0.78134+0.00133066	test-auc:0.773202+0.00193586
[579]	train-auc:0.781368+0.00132348	test-auc:0.773222+0.00195791
[580]	train-auc:0.78139+0.00132432	test-auc:0.773235+0.00196627
[581]	train-auc:0.781411+0.00131826	test-auc:0.773243+0.00199054
[582]	train-auc:0.781442+0.00132192	test-auc:0.773267+0.00199052
[583]	train-auc:0.781474+0.00132746	test-auc:0.773288+0.00199229
[584]	train-auc:0.781496+0.00132391	test-auc:0.773298+0.00199766
[585]	train-auc:0.781514+0.00132891	test-auc:0.773303+0.00199603
[586]	train-auc:0.781533+0.0013236	test-auc:0.773313+0.0019938
[587]	train-auc:0.781555+0.00131901	test-auc:0.773333+0.00199331
[588]	train-auc:0.78158+0.00132306	test-auc:0.773358+0.00199421
[589]	train-auc:0.781599+0.00132448	test-auc:0.773351+0.00199412
[590]	train-auc:0.781623+0.0013146	test-auc:0.773375+0.00201749
[591]	train-auc:0.781647+0.00131476	test-auc:0.773388+0.00199758
[592]	train-auc:0.781671+0.00129946	test-auc:0.773414+0.00200632
[593]	train-auc:0.781703+0.00131259	test-auc:0.773424+0.00202992
[594]	train-auc:0.781725+0.00132385	test-auc:0.773434+0.00203383
[595]	train-auc:0.781754+0.00131297	test-auc:0.773456+0.0020226
[596]	train-auc:0.781776+0.00131112	test-auc:0.773483+0.00202875
[597]	train-auc:0.781807+0.00132141	test-auc:0.773518+0.00202169
[598]	train-auc:0.781825+0.0013295	test-auc:0.773522+0.00202431
[599]	train-auc:0.781848+0.00132881	test-auc:0.773547+0.0020249
[600]	train-auc:0.781873+0.00133544	test-auc:0.773563+0.00203078
[601]	train-auc:0.781893+0.0013282	test-auc:0.773586+0.002035
[602]	train-auc:0.781918+0.00133178	test-auc:0.773598+0.00204509
[603]	train-auc:0.781937+0.0013234	test-auc:0.773612+0.00204649
[604]	train-auc:0.781958+0.00131728	test-auc:0.773607+0.00203861
[605]	train-auc:0.781978+0.00131207	test-auc:0.77362+0.00203978
[606]	train-auc:0.781995+0.0013145	test-auc:0.773639+0.0020275
[607]	train-auc:0.782015+0.00132073	test-auc:0.773648+0.00203422
[608]	train-auc:0.782041+0.00131324	test-auc:0.773667+0.0020372
[609]	train-auc:0.782065+0.00131608	test-auc:0.773683+0.00204837
[610]	train-auc:0.782088+0.00132625	test-auc:0.773697+0.00202576
[611]	train-auc:0.782116+0.0013417	test-auc:0.773697+0.00203393
[612]	train-auc:0.782137+0.00133654	test-auc:0.773722+0.00202581
[613]	train-auc:0.782161+0.00134126	test-auc:0.773735+0.00201965
[614]	train-auc:0.782184+0.00134279	test-auc:0.773746+0.00202339
[615]	train-auc:0.782203+0.00135176	test-auc:0.77376+0.00203086
[616]	train-auc:0.782228+0.00135333	test-auc:0.773761+0.00203083
[617]	train-auc:0.782245+0.00136231	test-auc:0.773775+0.0020272
[618]	train-auc:0.782271+0.00137442	test-auc:0.773791+0.00202812
[619]	train-auc:0.782293+0.00138475	test-auc:0.773798+0.00202612
[620]	train-auc:0.782321+0.00138398	test-auc:0.773812+0.00203092
[621]	train-auc:0.78234+0.00138129	test-auc:0.773822+0.00202229
[622]	train-auc:0.782354+0.00137309	test-auc:0.773841+0.00202307
[623]	train-auc:0.782371+0.00136591	test-auc:0.773859+0.0020132
[624]	train-auc:0.782404+0.0013673	test-auc:0.773883+0.00202191
[625]	train-auc:0.782429+0.0013596	test-auc:0.773895+0.00201718
[626]	train-auc:0.782448+0.00135954	test-auc:0.77392+0.00203189
[627]	train-auc:0.782475+0.00135326	test-auc:0.773934+0.0020296
[628]	train-auc:0.782497+0.00135523	test-auc:0.77395+0.0020232
[629]	train-auc:0.782526+0.00136016	test-auc:0.773954+0.00201746
[630]	train-auc:0.782556+0.00136265	test-auc:0.773977+0.0020125
[631]	train-auc:0.782576+0.00136759	test-auc:0.773991+0.00201208
[632]	train-auc:0.782605+0.00136901	test-auc:0.774013+0.00200176
[633]	train-auc:0.782626+0.0013688	test-auc:0.774019+0.00200001
[634]	train-auc:0.782646+0.00137529	test-auc:0.77402+0.00199792
[635]	train-auc:0.782666+0.00137542	test-auc:0.774028+0.00200145
[636]	train-auc:0.782696+0.00138062	test-auc:0.774037+0.00200329
[637]	train-auc:0.782721+0.00138589	test-auc:0.774053+0.00200334
[638]	train-auc:0.782751+0.00138629	test-auc:0.774065+0.00199689
[639]	train-auc:0.782765+0.00138665	test-auc:0.774078+0.00198249
[640]	train-auc:0.782791+0.00139895	test-auc:0.774093+0.00197571
[641]	train-auc:0.782815+0.00139887	test-auc:0.774105+0.00197967
[642]	train-auc:0.78284+0.00140622	test-auc:0.774125+0.00197123
[643]	train-auc:0.782873+0.00140128	test-auc:0.774148+0.00197345
[644]	train-auc:0.782897+0.00140351	test-auc:0.774155+0.00196413
[645]	train-auc:0.782913+0.00140919	test-auc:0.774173+0.00195987
[646]	train-auc:0.782938+0.00141732	test-auc:0.774189+0.00195426
[647]	train-auc:0.782958+0.00141548	test-auc:0.774206+0.00195723
[648]	train-auc:0.782986+0.00140126	test-auc:0.774221+0.00196994
[649]	train-auc:0.783007+0.00140108	test-auc:0.774242+0.00196566
[650]	train-auc:0.78303+0.00139676	test-auc:0.774258+0.0019637
[651]	train-auc:0.783052+0.00139892	test-auc:0.774261+0.00196033
[652]	train-auc:0.78308+0.0014054	test-auc:0.774276+0.00195161
[653]	train-auc:0.783107+0.00141073	test-auc:0.774284+0.00193209
[654]	train-auc:0.783138+0.00141465	test-auc:0.774305+0.00192269
[655]	train-auc:0.783167+0.00141204	test-auc:0.774324+0.00192303
[656]	train-auc:0.783194+0.001407	test-auc:0.774347+0.00194355
[657]	train-auc:0.783212+0.00140865	test-auc:0.774365+0.00195074
[658]	train-auc:0.783237+0.00141109	test-auc:0.77438+0.00196221
[659]	train-auc:0.783258+0.00141015	test-auc:0.774396+0.00195693
[660]	train-auc:0.783281+0.00141244	test-auc:0.774416+0.00194395
[661]	train-auc:0.7833+0.00140977	test-auc:0.774428+0.00194756
[662]	train-auc:0.783336+0.00141435	test-auc:0.77443+0.00193197
[663]	train-auc:0.783357+0.00141666	test-auc:0.774447+0.00193763
[664]	train-auc:0.783375+0.00141036	test-auc:0.774471+0.0019479
[665]	train-auc:0.783395+0.00141068	test-auc:0.77448+0.00194155
[666]	train-auc:0.783417+0.0014186	test-auc:0.774488+0.0019327
[667]	train-auc:0.783437+0.00141221	test-auc:0.774498+0.00192243
[668]	train-auc:0.783459+0.00141255	test-auc:0.774516+0.0019205
[669]	train-auc:0.78349+0.00142203	test-auc:0.774533+0.00190692
[670]	train-auc:0.783517+0.0014221	test-auc:0.774552+0.00188919
[671]	train-auc:0.78354+0.00142773	test-auc:0.774572+0.00188889
[672]	train-auc:0.783558+0.00142387	test-auc:0.774586+0.00188239
[673]	train-auc:0.783582+0.00143101	test-auc:0.774606+0.00188872
[674]	train-auc:0.783608+0.00143979	test-auc:0.774613+0.00188387
[675]	train-auc:0.783627+0.00143313	test-auc:0.77463+0.00187726
[676]	train-auc:0.78366+0.00144403	test-auc:0.774653+0.00188205
[677]	train-auc:0.78368+0.00144289	test-auc:0.774666+0.00187614
[678]	train-auc:0.783708+0.00144537	test-auc:0.774672+0.00187558
[679]	train-auc:0.783727+0.00143745	test-auc:0.774687+0.00188644
[680]	train-auc:0.783747+0.00143687	test-auc:0.774707+0.00187579
[681]	train-auc:0.783774+0.00143744	test-auc:0.77473+0.00188829
[682]	train-auc:0.78379+0.00143506	test-auc:0.77474+0.00188739
[683]	train-auc:0.783822+0.00143285	test-auc:0.774761+0.00188825
[684]	train-auc:0.783849+0.00143753	test-auc:0.774779+0.001894
[685]	train-auc:0.783867+0.00143447	test-auc:0.774808+0.00188916
[686]	train-auc:0.783885+0.00143436	test-auc:0.774816+0.00189662
[687]	train-auc:0.783907+0.00143682	test-auc:0.774818+0.00189173
[688]	train-auc:0.783921+0.00143517	test-auc:0.774817+0.00189344
[689]	train-auc:0.783947+0.00144678	test-auc:0.774823+0.00188626
[690]	train-auc:0.78397+0.00144541	test-auc:0.774839+0.00188458
[691]	train-auc:0.783991+0.00144874	test-auc:0.774849+0.00188114
[692]	train-auc:0.784008+0.00144715	test-auc:0.774867+0.00187246
[693]	train-auc:0.784028+0.0014534	test-auc:0.77489+0.00186485
[694]	train-auc:0.784048+0.00146091	test-auc:0.774892+0.00185933
[695]	train-auc:0.78407+0.00146547	test-auc:0.774912+0.00185571
[696]	train-auc:0.784096+0.00147286	test-auc:0.774927+0.00184968
[697]	train-auc:0.784121+0.001474	test-auc:0.774941+0.00185448
[698]	train-auc:0.784144+0.00146683	test-auc:0.77496+0.00184626
[699]	train-auc:0.78417+0.00147725	test-auc:0.774969+0.00184149
[700]	train-auc:0.784185+0.00147766	test-auc:0.77498+0.00184292
[701]	train-auc:0.784201+0.00147486	test-auc:0.774979+0.00184931
[702]	train-auc:0.78422+0.00147337	test-auc:0.774982+0.0018558
[703]	train-auc:0.784236+0.00147407	test-auc:0.774989+0.00185091
[704]	train-auc:0.784249+0.00147572	test-auc:0.774984+0.00184854
[705]	train-auc:0.784273+0.00147298	test-auc:0.774992+0.00183564
[706]	train-auc:0.784294+0.00147076	test-auc:0.775015+0.00183754
[707]	train-auc:0.784321+0.00146638	test-auc:0.775037+0.00183886
[708]	train-auc:0.784345+0.00146826	test-auc:0.77505+0.00183454
[709]	train-auc:0.784367+0.00146726	test-auc:0.775063+0.00183476
[710]	train-auc:0.78439+0.00146909	test-auc:0.775082+0.00182746
[711]	train-auc:0.784412+0.00147372	test-auc:0.775077+0.00181909
[712]	train-auc:0.784422+0.00147668	test-auc:0.775087+0.001824
[713]	train-auc:0.784444+0.0014777	test-auc:0.775093+0.00181737
[714]	train-auc:0.78446+0.00147569	test-auc:0.775102+0.00181988
[715]	train-auc:0.784484+0.0014777	test-auc:0.775112+0.00183283
[716]	train-auc:0.784507+0.00148536	test-auc:0.775121+0.00183717
[717]	train-auc:0.784526+0.00148657	test-auc:0.775127+0.00183644
[718]	train-auc:0.784538+0.0014869	test-auc:0.77514+0.00184186
[719]	train-auc:0.78456+0.00149286	test-auc:0.775156+0.00184165
[720]	train-auc:0.784578+0.00149259	test-auc:0.775166+0.00182946
[721]	train-auc:0.784588+0.00150144	test-auc:0.775166+0.00182468
[722]	train-auc:0.784613+0.00150553	test-auc:0.775182+0.00182646
[723]	train-auc:0.784632+0.00150271	test-auc:0.77519+0.00183911
[724]	train-auc:0.784652+0.00149673	test-auc:0.775202+0.00185058
[725]	train-auc:0.784675+0.00149152	test-auc:0.77522+0.00185742
[726]	train-auc:0.784685+0.00148859	test-auc:0.775213+0.00186229
[727]	train-auc:0.784706+0.00148469	test-auc:0.775242+0.001868
[728]	train-auc:0.784724+0.00148602	test-auc:0.775243+0.00186618
[729]	train-auc:0.784752+0.00148876	test-auc:0.775257+0.0018566
[730]	train-auc:0.784768+0.0014827	test-auc:0.775269+0.00186391
[731]	train-auc:0.784796+0.00148086	test-auc:0.775294+0.00188152
[732]	train-auc:0.784818+0.0014779	test-auc:0.775305+0.00189572
[733]	train-auc:0.784833+0.00147816	test-auc:0.775317+0.00188584
[734]	train-auc:0.784857+0.00147951	test-auc:0.775326+0.0018829
[735]	train-auc:0.784875+0.0014802	test-auc:0.775353+0.00187939
[736]	train-auc:0.784885+0.00147755	test-auc:0.775371+0.00185278
[737]	train-auc:0.784904+0.00147582	test-auc:0.775389+0.00184995
[738]	train-auc:0.78492+0.00147617	test-auc:0.775407+0.00184955
[739]	train-auc:0.784948+0.0014804	test-auc:0.775432+0.00185155
[740]	train-auc:0.784961+0.00147686	test-auc:0.775437+0.00184058
[741]	train-auc:0.784974+0.00147394	test-auc:0.775452+0.00185262
[742]	train-auc:0.784994+0.00147718	test-auc:0.775466+0.0018539
[743]	train-auc:0.785018+0.001482	test-auc:0.775479+0.00184681
[744]	train-auc:0.785034+0.00148214	test-auc:0.77549+0.00183743
[745]	train-auc:0.785045+0.00148183	test-auc:0.775502+0.00183416
[746]	train-auc:0.785064+0.00148388	test-auc:0.775493+0.00183052
[747]	train-auc:0.785081+0.00147769	test-auc:0.775504+0.00183772
[748]	train-auc:0.785096+0.00147976	test-auc:0.775511+0.00183446
[749]	train-auc:0.785124+0.00148077	test-auc:0.775514+0.00183859
[750]	train-auc:0.785138+0.00147165	test-auc:0.775525+0.00183628
[751]	train-auc:0.785154+0.00146909	test-auc:0.775522+0.00183125
[752]	train-auc:0.785177+0.00146631	test-auc:0.775547+0.00183896
[753]	train-auc:0.785204+0.00146582	test-auc:0.775561+0.00183603
[754]	train-auc:0.785222+0.00146699	test-auc:0.775576+0.00183848
[755]	train-auc:0.785243+0.00146396	test-auc:0.77559+0.00184132
[756]	train-auc:0.785257+0.00146613	test-auc:0.775592+0.00184117
[757]	train-auc:0.785282+0.00147153	test-auc:0.775588+0.00183561
[758]	train-auc:0.785301+0.00147698	test-auc:0.775604+0.00183395
[759]	train-auc:0.785316+0.0014682	test-auc:0.775609+0.0018278
[760]	train-auc:0.785342+0.0014709	test-auc:0.775623+0.00183488
[761]	train-auc:0.785359+0.00147006	test-auc:0.775628+0.00182932
[762]	train-auc:0.785374+0.00146372	test-auc:0.775641+0.00183974
[763]	train-auc:0.785397+0.00146824	test-auc:0.775654+0.00183677
[764]	train-auc:0.785417+0.00146872	test-auc:0.775667+0.00183941
[765]	train-auc:0.785434+0.00146786	test-auc:0.775677+0.00183688
[766]	train-auc:0.785457+0.00146929	test-auc:0.775688+0.00183586
[767]	train-auc:0.785471+0.00147283	test-auc:0.775686+0.00184614
[768]	train-auc:0.785493+0.00148114	test-auc:0.775694+0.00184967
[769]	train-auc:0.785509+0.00147518	test-auc:0.775711+0.00184801
[770]	train-auc:0.785528+0.00148563	test-auc:0.775723+0.00183985
[771]	train-auc:0.785554+0.00149854	test-auc:0.77573+0.00183931
[772]	train-auc:0.785575+0.00149131	test-auc:0.775746+0.00183789
[773]	train-auc:0.785593+0.00149439	test-auc:0.775741+0.00183251
[774]	train-auc:0.785614+0.00150066	test-auc:0.775758+0.00183712
[775]	train-auc:0.785633+0.00149433	test-auc:0.775765+0.00183478
[776]	train-auc:0.785657+0.00149712	test-auc:0.77578+0.00184321
[777]	train-auc:0.785674+0.00148504	test-auc:0.775795+0.00184993
[778]	train-auc:0.785694+0.00148784	test-auc:0.775805+0.00184655
[779]	train-auc:0.785712+0.00149073	test-auc:0.775809+0.00185266
[780]	train-auc:0.785718+0.00148532	test-auc:0.77581+0.00185649
[781]	train-auc:0.785739+0.00148794	test-auc:0.775817+0.00185242
[782]	train-auc:0.785757+0.00149494	test-auc:0.775817+0.0018644
[783]	train-auc:0.785772+0.00148782	test-auc:0.775824+0.00186325
[784]	train-auc:0.785803+0.00148566	test-auc:0.775836+0.00186013
[785]	train-auc:0.785823+0.00148672	test-auc:0.775842+0.00187717
[786]	train-auc:0.785836+0.00148115	test-auc:0.775844+0.00188423
[787]	train-auc:0.785856+0.00147936	test-auc:0.77587+0.00188234
[788]	train-auc:0.785874+0.0014792	test-auc:0.775874+0.00188235
[789]	train-auc:0.785891+0.00147785	test-auc:0.775887+0.00187626
[790]	train-auc:0.785903+0.00148104	test-auc:0.775898+0.00188411
[791]	train-auc:0.785924+0.00147887	test-auc:0.775911+0.00187342
[792]	train-auc:0.785945+0.00148126	test-auc:0.775914+0.00186801
[793]	train-auc:0.785967+0.00148458	test-auc:0.775928+0.00186862
[794]	train-auc:0.785981+0.00148381	test-auc:0.775938+0.00188859
[795]	train-auc:0.785994+0.0014836	test-auc:0.775943+0.0018856
[796]	train-auc:0.78601+0.00148775	test-auc:0.775948+0.00189418
[797]	train-auc:0.786027+0.00148964	test-auc:0.775965+0.00189691
[798]	train-auc:0.786047+0.00148791	test-auc:0.775966+0.00188472
[799]	train-auc:0.786068+0.00148364	test-auc:0.775982+0.00188935
[800]	train-auc:0.786091+0.00148341	test-auc:0.775981+0.00189098
[801]	train-auc:0.786108+0.00148557	test-auc:0.775986+0.00188139
[802]	train-auc:0.786121+0.00149357	test-auc:0.775989+0.00188412
[803]	train-auc:0.786132+0.00149299	test-auc:0.775995+0.00188186
[804]	train-auc:0.786148+0.00148537	test-auc:0.776006+0.00187173
[805]	train-auc:0.78617+0.00148747	test-auc:0.776008+0.0018757
[806]	train-auc:0.786188+0.00148722	test-auc:0.776016+0.00187486
[807]	train-auc:0.786204+0.00148159	test-auc:0.776024+0.00187744
[808]	train-auc:0.786217+0.00148032	test-auc:0.77602+0.00187927
[809]	train-auc:0.786233+0.00148481	test-auc:0.776027+0.00187569
[810]	train-auc:0.786252+0.00147834	test-auc:0.77605+0.00188115
[811]	train-auc:0.786277+0.00148189	test-auc:0.776067+0.00188765
[812]	train-auc:0.78629+0.00148714	test-auc:0.776073+0.00189273
[813]	train-auc:0.786304+0.00148992	test-auc:0.776078+0.00189844
[814]	train-auc:0.786331+0.00148693	test-auc:0.776091+0.00190226
[815]	train-auc:0.786343+0.00148103	test-auc:0.776101+0.00192416
[816]	train-auc:0.786361+0.00148073	test-auc:0.776101+0.001924
[817]	train-auc:0.786378+0.0014871	test-auc:0.77611+0.00192455
[818]	train-auc:0.786395+0.00148097	test-auc:0.776126+0.00193498
[819]	train-auc:0.786413+0.00147855	test-auc:0.776131+0.00193429
[820]	train-auc:0.786432+0.00148291	test-auc:0.776143+0.00192953
[821]	train-auc:0.786449+0.00148392	test-auc:0.776152+0.00194131
[822]	train-auc:0.786466+0.00148086	test-auc:0.776153+0.00193439
[823]	train-auc:0.786481+0.00148014	test-auc:0.776161+0.00194399
[824]	train-auc:0.786495+0.00148074	test-auc:0.776175+0.00194188
[825]	train-auc:0.786509+0.0014781	test-auc:0.776187+0.00193039
[826]	train-auc:0.786527+0.00147971	test-auc:0.776201+0.00193452
[827]	train-auc:0.786538+0.00147785	test-auc:0.776192+0.00193148
[828]	train-auc:0.786562+0.00148483	test-auc:0.776198+0.00192694
[829]	train-auc:0.786585+0.00148511	test-auc:0.776212+0.00193605
[830]	train-auc:0.786601+0.00148823	test-auc:0.776229+0.00193194
[831]	train-auc:0.786613+0.00148682	test-auc:0.776235+0.00192808
[832]	train-auc:0.786622+0.00148309	test-auc:0.776245+0.00191928
[833]	train-auc:0.786639+0.0014846	test-auc:0.776245+0.00191603
[834]	train-auc:0.78666+0.00148885	test-auc:0.776245+0.00192037
[835]	train-auc:0.786671+0.00148569	test-auc:0.776261+0.00190426
[836]	train-auc:0.786698+0.00149633	test-auc:0.776275+0.00191767
[837]	train-auc:0.786713+0.00149053	test-auc:0.776279+0.00192387
[838]	train-auc:0.78673+0.00148329	test-auc:0.776287+0.00193674
[839]	train-auc:0.78674+0.00148238	test-auc:0.776293+0.00193796
[840]	train-auc:0.786756+0.00149124	test-auc:0.776287+0.00194505
[841]	train-auc:0.78677+0.00149208	test-auc:0.776296+0.00194149
[842]	train-auc:0.786787+0.00149	test-auc:0.776306+0.00192364
[843]	train-auc:0.786805+0.00149875	test-auc:0.77632+0.00192361
[844]	train-auc:0.786826+0.00149873	test-auc:0.776339+0.00193065
[845]	train-auc:0.786848+0.00150316	test-auc:0.776346+0.0019303
[846]	train-auc:0.786867+0.00149209	test-auc:0.776351+0.00193663
[847]	train-auc:0.786888+0.00148898	test-auc:0.776366+0.00194882
[848]	train-auc:0.786902+0.00148765	test-auc:0.776374+0.00195828
[849]	train-auc:0.786909+0.00148924	test-auc:0.776379+0.0019501
[850]	train-auc:0.786922+0.00149005	test-auc:0.776381+0.00195878
[851]	train-auc:0.786939+0.0015008	test-auc:0.776382+0.00195954
[852]	train-auc:0.786956+0.00149466	test-auc:0.776398+0.00195871
[853]	train-auc:0.786974+0.00149587	test-auc:0.77641+0.00195334
[854]	train-auc:0.786985+0.00149728	test-auc:0.776408+0.00195355
[855]	train-auc:0.787+0.00149125	test-auc:0.776414+0.00196018
[856]	train-auc:0.787011+0.00148807	test-auc:0.776428+0.00196623
[857]	train-auc:0.787026+0.00149228	test-auc:0.776431+0.00196413
[858]	train-auc:0.787043+0.00149617	test-auc:0.776443+0.00195899
[859]	train-auc:0.78706+0.00149178	test-auc:0.776453+0.00196824
[860]	train-auc:0.787082+0.00148879	test-auc:0.776467+0.00195972
[861]	train-auc:0.7871+0.00149	test-auc:0.776471+0.00195437
[862]	train-auc:0.787121+0.00149164	test-auc:0.776476+0.00196144
[863]	train-auc:0.787139+0.00149602	test-auc:0.776483+0.00196992
[864]	train-auc:0.787151+0.00149721	test-auc:0.776494+0.00197106
[865]	train-auc:0.787158+0.00149503	test-auc:0.776499+0.00197105
[866]	train-auc:0.787177+0.00149469	test-auc:0.776502+0.00196659
[867]	train-auc:0.787194+0.00149795	test-auc:0.776507+0.0019641
[868]	train-auc:0.787208+0.00149475	test-auc:0.776518+0.00196036
[869]	train-auc:0.787225+0.00149727	test-auc:0.776533+0.00196808
[870]	train-auc:0.787242+0.0015024	test-auc:0.776549+0.00197599
[871]	train-auc:0.787257+0.00150491	test-auc:0.776557+0.00197815
[872]	train-auc:0.787274+0.00151213	test-auc:0.776559+0.00198408
[873]	train-auc:0.78729+0.00151133	test-auc:0.776567+0.00198969
[874]	train-auc:0.787306+0.0015098	test-auc:0.776573+0.00200843
[875]	train-auc:0.78732+0.00150546	test-auc:0.77658+0.00200456
[876]	train-auc:0.787331+0.00150659	test-auc:0.776577+0.00200122
[877]	train-auc:0.787351+0.0015043	test-auc:0.776592+0.00200357
[878]	train-auc:0.787365+0.00150664	test-auc:0.776603+0.0020003
[879]	train-auc:0.787373+0.0015085	test-auc:0.77661+0.00199882
[880]	train-auc:0.787383+0.00150716	test-auc:0.776614+0.002008
[881]	train-auc:0.7874+0.00151225	test-auc:0.776619+0.0019964
[882]	train-auc:0.787415+0.0015163	test-auc:0.776632+0.00199597
[883]	train-auc:0.787428+0.00151711	test-auc:0.776643+0.00199163
[884]	train-auc:0.787443+0.00151488	test-auc:0.776643+0.00198503
[885]	train-auc:0.787457+0.0015122	test-auc:0.776643+0.00199685
[886]	train-auc:0.787475+0.00150938	test-auc:0.776655+0.00200786
[887]	train-auc:0.787496+0.00151545	test-auc:0.77667+0.0020051
[888]	train-auc:0.787514+0.00150981	test-auc:0.776678+0.00201502
[889]	train-auc:0.787532+0.00151484	test-auc:0.776693+0.0020162
[890]	train-auc:0.787542+0.00151275	test-auc:0.776708+0.00202633
[891]	train-auc:0.787559+0.0015086	test-auc:0.77672+0.00203401
[892]	train-auc:0.78758+0.00150507	test-auc:0.776724+0.00204286
[893]	train-auc:0.787593+0.0015063	test-auc:0.776735+0.00203756
[894]	train-auc:0.787612+0.00151232	test-auc:0.776744+0.00204225
[895]	train-auc:0.787625+0.00151648	test-auc:0.776742+0.00204365
[896]	train-auc:0.787635+0.00151751	test-auc:0.77675+0.00204375
[897]	train-auc:0.787648+0.00152628	test-auc:0.776759+0.00204277
[898]	train-auc:0.787661+0.00152275	test-auc:0.776772+0.00204421
[899]	train-auc:0.787673+0.00151978	test-auc:0.776775+0.00204268
[900]	train-auc:0.787685+0.00151785	test-auc:0.776788+0.00204115
[901]	train-auc:0.787697+0.00151264	test-auc:0.776799+0.0020493
[902]	train-auc:0.787714+0.00150806	test-auc:0.776812+0.00205214
[903]	train-auc:0.787729+0.00151035	test-auc:0.776815+0.00205886
[904]	train-auc:0.787741+0.00150882	test-auc:0.776819+0.00206002
[905]	train-auc:0.787762+0.00150906	test-auc:0.776821+0.00204688
[906]	train-auc:0.787779+0.00150288	test-auc:0.776835+0.00206106
[907]	train-auc:0.78779+0.00150445	test-auc:0.776844+0.00206157
[908]	train-auc:0.787804+0.00150524	test-auc:0.776848+0.00207057
[909]	train-auc:0.787816+0.00151102	test-auc:0.776846+0.00207208
[910]	train-auc:0.787825+0.00151203	test-auc:0.776857+0.00207354
[911]	train-auc:0.787838+0.00151584	test-auc:0.776863+0.00207629
[912]	train-auc:0.78785+0.00151134	test-auc:0.776861+0.00206863
[913]	train-auc:0.787869+0.00150715	test-auc:0.776875+0.00207262
[914]	train-auc:0.787884+0.00150635	test-auc:0.776893+0.00207151
[915]	train-auc:0.787897+0.00149817	test-auc:0.776904+0.00207808
[916]	train-auc:0.787911+0.00150102	test-auc:0.77692+0.00207541
[917]	train-auc:0.787926+0.00150092	test-auc:0.776924+0.00207913
[918]	train-auc:0.787939+0.0015041	test-auc:0.776933+0.00207847
[919]	train-auc:0.787954+0.0014999	test-auc:0.776943+0.00208576
[920]	train-auc:0.787964+0.00149435	test-auc:0.776954+0.00209267
[921]	train-auc:0.787976+0.00149632	test-auc:0.776962+0.0020834
[922]	train-auc:0.787991+0.00148956	test-auc:0.77697+0.00208185
[923]	train-auc:0.788006+0.00149046	test-auc:0.776975+0.00208589
[924]	train-auc:0.788019+0.00149824	test-auc:0.776973+0.00208813
[925]	train-auc:0.788039+0.00150828	test-auc:0.776979+0.00208469
[926]	train-auc:0.788055+0.00151045	test-auc:0.776975+0.00208859
[927]	train-auc:0.788072+0.001507	test-auc:0.776989+0.00208094
[928]	train-auc:0.788085+0.00150406	test-auc:0.776994+0.00208281
[929]	train-auc:0.788107+0.0014992	test-auc:0.777003+0.00207673
[930]	train-auc:0.788117+0.00150436	test-auc:0.777003+0.0020711
[931]	train-auc:0.788137+0.00150929	test-auc:0.777004+0.00207989
[932]	train-auc:0.788151+0.0015011	test-auc:0.777013+0.00208769
[933]	train-auc:0.788165+0.00150895	test-auc:0.777019+0.00209256
[934]	train-auc:0.788176+0.0015106	test-auc:0.777028+0.00208949
[935]	train-auc:0.788188+0.00150232	test-auc:0.777041+0.00209856
[936]	train-auc:0.788203+0.00149927	test-auc:0.777051+0.00210287
[937]	train-auc:0.788217+0.0015007	test-auc:0.77706+0.00209627
[938]	train-auc:0.788229+0.00149697	test-auc:0.77707+0.00210321
[939]	train-auc:0.788238+0.00148594	test-auc:0.777074+0.00211143
[940]	train-auc:0.788252+0.00148159	test-auc:0.777073+0.00210587
[941]	train-auc:0.788262+0.00148086	test-auc:0.77708+0.00209242
[942]	train-auc:0.788269+0.00148082	test-auc:0.777095+0.0020953
[943]	train-auc:0.788273+0.00147573	test-auc:0.777095+0.00209485
[944]	train-auc:0.788287+0.00146935	test-auc:0.777112+0.00210697
[945]	train-auc:0.788299+0.00146686	test-auc:0.777122+0.00211104
[946]	train-auc:0.788303+0.00146772	test-auc:0.777129+0.00210855
[947]	train-auc:0.788317+0.00146863	test-auc:0.777139+0.00210245
[948]	train-auc:0.78833+0.00147222	test-auc:0.777137+0.00209992
[949]	train-auc:0.788342+0.00147571	test-auc:0.777147+0.00209699
[950]	train-auc:0.788354+0.00148141	test-auc:0.777147+0.00210452
[951]	train-auc:0.788369+0.00148687	test-auc:0.777145+0.00210904
[952]	train-auc:0.788376+0.00148647	test-auc:0.777149+0.00210829
[953]	train-auc:0.788386+0.00147815	test-auc:0.777151+0.00211731
[954]	train-auc:0.788405+0.00147782	test-auc:0.77715+0.00211731
[955]	train-auc:0.78841+0.00148051	test-auc:0.777158+0.00212613
[956]	train-auc:0.78842+0.00148881	test-auc:0.77716+0.00212007
[957]	train-auc:0.788431+0.00148859	test-auc:0.777164+0.00212574
[958]	train-auc:0.788445+0.00148903	test-auc:0.777166+0.00212855
[959]	train-auc:0.78846+0.00149064	test-auc:0.777167+0.00213382
[960]	train-auc:0.788467+0.00148493	test-auc:0.777171+0.00213897
[961]	train-auc:0.788479+0.00148466	test-auc:0.777178+0.00213672
[962]	train-auc:0.788492+0.00148869	test-auc:0.777185+0.00212769
[963]	train-auc:0.788502+0.00148604	test-auc:0.777201+0.00212393
[964]	train-auc:0.788506+0.00148655	test-auc:0.777202+0.00212125
[965]	train-auc:0.788519+0.00148893	test-auc:0.777203+0.00211972
[966]	train-auc:0.78853+0.00148787	test-auc:0.77721+0.00211758
[967]	train-auc:0.788548+0.00148723	test-auc:0.777227+0.00211841
[968]	train-auc:0.788562+0.0014879	test-auc:0.777232+0.0021159
[969]	train-auc:0.788571+0.00148426	test-auc:0.77723+0.00211351
[970]	train-auc:0.788587+0.00148166	test-auc:0.77725+0.00211641
[971]	train-auc:0.788603+0.00147506	test-auc:0.777257+0.00212843
[972]	train-auc:0.788619+0.00147759	test-auc:0.777271+0.00213547
[973]	train-auc:0.788627+0.00148188	test-auc:0.777273+0.00212651
[974]	train-auc:0.78864+0.00148394	test-auc:0.777276+0.00212531
[975]	train-auc:0.788656+0.00148013	test-auc:0.777284+0.00213613
[976]	train-auc:0.788665+0.00147832	test-auc:0.777288+0.00213617
[977]	train-auc:0.788679+0.00147585	test-auc:0.777284+0.00213069
[978]	train-auc:0.78869+0.00147407	test-auc:0.77729+0.00213279
[979]	train-auc:0.788706+0.00147488	test-auc:0.7773+0.00213727
[980]	train-auc:0.78872+0.00147706	test-auc:0.777316+0.00212584
[981]	train-auc:0.788735+0.00147587	test-auc:0.777311+0.00212156
[982]	train-auc:0.788747+0.00147858	test-auc:0.777317+0.0021245
[983]	train-auc:0.78876+0.0014782	test-auc:0.777308+0.00212337
[984]	train-auc:0.788779+0.00147625	test-auc:0.777314+0.00211445
[985]	train-auc:0.788785+0.00147723	test-auc:0.777323+0.00210454
[986]	train-auc:0.788804+0.00147445	test-auc:0.777331+0.00210518
[987]	train-auc:0.788821+0.00147775	test-auc:0.777336+0.00209788
[988]	train-auc:0.788828+0.00147903	test-auc:0.777339+0.00209613
[989]	train-auc:0.788843+0.00147506	test-auc:0.777341+0.00209262
[990]	train-auc:0.788849+0.00147447	test-auc:0.777346+0.00209679
[991]	train-auc:0.788862+0.00148311	test-auc:0.777353+0.00209888
[992]	train-auc:0.788877+0.0014867	test-auc:0.777363+0.00209024
[993]	train-auc:0.788886+0.00148279	test-auc:0.777359+0.00208748
[994]	train-auc:0.788893+0.00148451	test-auc:0.777357+0.00208912
[995]	train-auc:0.788908+0.00148645	test-auc:0.777371+0.00209425
[996]	train-auc:0.788924+0.00149655	test-auc:0.777371+0.00209456
[997]	train-auc:0.788931+0.00149174	test-auc:0.777371+0.0020927
[998]	train-auc:0.788943+0.00149038	test-auc:0.777375+0.00209401
[999]	train-auc:0.788952+0.00148523	test-auc:0.777382+0.00209929
[1000]	train-auc:0.788964+0.00148661	test-auc:0.77739+0.00209147
[1001]	train-auc:0.788971+0.00148792	test-auc:0.777393+0.00209892
[1002]	train-auc:0.788984+0.00148143	test-auc:0.777398+0.00210641
[1003]	train-auc:0.788992+0.00148474	test-auc:0.777394+0.00210285
[1004]	train-auc:0.789009+0.00148896	test-auc:0.777408+0.00211014
[1005]	train-auc:0.789022+0.00148337	test-auc:0.777411+0.00211111
[1006]	train-auc:0.789035+0.00148165	test-auc:0.777409+0.00210797
[1007]	train-auc:0.789046+0.00148249	test-auc:0.777413+0.00210707
[1008]	train-auc:0.78906+0.00147848	test-auc:0.777414+0.00211915
[1009]	train-auc:0.789075+0.00148265	test-auc:0.777421+0.00211979
[1010]	train-auc:0.789091+0.00148153	test-auc:0.777429+0.00210951
[1011]	train-auc:0.789106+0.00148003	test-auc:0.777435+0.00210753
[1012]	train-auc:0.78912+0.00148389	test-auc:0.777432+0.00210686
[1013]	train-auc:0.789129+0.00148573	test-auc:0.777438+0.0021076
[1014]	train-auc:0.78914+0.00148981	test-auc:0.777438+0.0021108
[1015]	train-auc:0.78915+0.00149733	test-auc:0.777453+0.00211393
[1016]	train-auc:0.789163+0.00149294	test-auc:0.777454+0.00212367
[1017]	train-auc:0.789175+0.00149579	test-auc:0.777458+0.002128
[1018]	train-auc:0.789178+0.00149643	test-auc:0.777461+0.00212998
[1019]	train-auc:0.789185+0.00149991	test-auc:0.777462+0.0021288
[1020]	train-auc:0.789196+0.00149472	test-auc:0.777472+0.00212589
[1021]	train-auc:0.789208+0.00149826	test-auc:0.777471+0.00212542
[1022]	train-auc:0.789223+0.00149898	test-auc:0.777489+0.00212891
[1023]	train-auc:0.789233+0.00149525	test-auc:0.777499+0.002136
[1024]	train-auc:0.789244+0.00150216	test-auc:0.777506+0.00212936
[1025]	train-auc:0.789259+0.00150536	test-auc:0.777517+0.0021256
[1026]	train-auc:0.789268+0.00149904	test-auc:0.77752+0.00213437
[1027]	train-auc:0.789282+0.00150071	test-auc:0.777524+0.00212899
[1028]	train-auc:0.789296+0.00150225	test-auc:0.777518+0.0021352
[1029]	train-auc:0.789308+0.00150259	test-auc:0.777513+0.00213358
[1030]	train-auc:0.789328+0.00150209	test-auc:0.777521+0.00213443
[1031]	train-auc:0.789339+0.00149704	test-auc:0.777518+0.00213106
[1032]	train-auc:0.789351+0.0014995	test-auc:0.777517+0.00212828
[1033]	train-auc:0.789367+0.00149882	test-auc:0.777515+0.00212042
[1034]	train-auc:0.789378+0.0015007	test-auc:0.777525+0.00212053
[1035]	train-auc:0.789393+0.00149929	test-auc:0.777525+0.00210852
[1036]	train-auc:0.789407+0.00149129	test-auc:0.777538+0.00212419
[1037]	train-auc:0.789418+0.00149341	test-auc:0.777547+0.00211972
[1038]	train-auc:0.789431+0.00148896	test-auc:0.777553+0.00212679
[1039]	train-auc:0.789437+0.00148988	test-auc:0.777561+0.00212457
[1040]	train-auc:0.789445+0.00149045	test-auc:0.777569+0.00213454
[1041]	train-auc:0.78946+0.00149269	test-auc:0.777565+0.00213482
[1042]	train-auc:0.789473+0.00149644	test-auc:0.777579+0.00213326
[1043]	train-auc:0.789486+0.00149453	test-auc:0.77758+0.00213316
[1044]	train-auc:0.789498+0.00149495	test-auc:0.777584+0.00213763
[1045]	train-auc:0.789508+0.00149035	test-auc:0.777588+0.00213549
[1046]	train-auc:0.789519+0.00148995	test-auc:0.777593+0.00213823
[1047]	train-auc:0.789529+0.00149377	test-auc:0.7776+0.00214038
[1048]	train-auc:0.789542+0.0014898	test-auc:0.777601+0.0021358
[1049]	train-auc:0.789548+0.00149175	test-auc:0.777604+0.0021316
[1050]	train-auc:0.789562+0.00149334	test-auc:0.777607+0.0021339
[1051]	train-auc:0.789575+0.00149127	test-auc:0.777613+0.00214057
[1052]	train-auc:0.789586+0.00148868	test-auc:0.777621+0.00213036
[1053]	train-auc:0.789593+0.00148678	test-auc:0.777628+0.00213007
[1054]	train-auc:0.789603+0.00148416	test-auc:0.777631+0.0021358
[1055]	train-auc:0.789618+0.00148307	test-auc:0.777641+0.00214064
[1056]	train-auc:0.789632+0.00148397	test-auc:0.777644+0.0021413
[1057]	train-auc:0.789644+0.00149088	test-auc:0.777646+0.00214251
[1058]	train-auc:0.789652+0.00149344	test-auc:0.777648+0.00213799
[1059]	train-auc:0.789659+0.00148607	test-auc:0.777649+0.00214278
[1060]	train-auc:0.789675+0.00148727	test-auc:0.777663+0.00214604
[1061]	train-auc:0.789689+0.00148871	test-auc:0.777664+0.00215194
[1062]	train-auc:0.789704+0.00148164	test-auc:0.777674+0.00215859
[1063]	train-auc:0.789712+0.00148147	test-auc:0.777681+0.00216006
[1064]	train-auc:0.78973+0.00147569	test-auc:0.777698+0.00217084
[1065]	train-auc:0.789737+0.00148052	test-auc:0.777697+0.00217515
[1066]	train-auc:0.789746+0.00148217	test-auc:0.777696+0.00217604
[1067]	train-auc:0.789753+0.00147954	test-auc:0.777691+0.00217574
[1068]	train-auc:0.789765+0.00148007	test-auc:0.777698+0.00217763
[1069]	train-auc:0.789775+0.0014736	test-auc:0.777695+0.00217191
[1070]	train-auc:0.789784+0.00147438	test-auc:0.777698+0.0021772
[1071]	train-auc:0.789796+0.00147045	test-auc:0.777702+0.00217641
[1072]	train-auc:0.789809+0.00146638	test-auc:0.777714+0.00217937
[1073]	train-auc:0.789822+0.00146779	test-auc:0.777712+0.00218567
[1074]	train-auc:0.789834+0.00146972	test-auc:0.777721+0.00219239
[1075]	train-auc:0.789839+0.00146875	test-auc:0.777726+0.00218559
[1076]	train-auc:0.789853+0.00146397	test-auc:0.777725+0.00218786
[1077]	train-auc:0.789869+0.00146607	test-auc:0.777737+0.00219158
[1078]	train-auc:0.789884+0.00146979	test-auc:0.777743+0.00218202
[1079]	train-auc:0.789898+0.0014736	test-auc:0.77775+0.00218529
[1080]	train-auc:0.789909+0.00147029	test-auc:0.777761+0.00218181
[1081]	train-auc:0.789919+0.00147142	test-auc:0.77777+0.00217849
[1082]	train-auc:0.789934+0.00147176	test-auc:0.777763+0.00218824
[1083]	train-auc:0.789944+0.00147579	test-auc:0.777775+0.00218371
[1084]	train-auc:0.789956+0.00147734	test-auc:0.777784+0.00218431
[1085]	train-auc:0.789966+0.00147831	test-auc:0.777796+0.00218618
[1086]	train-auc:0.78998+0.00147386	test-auc:0.777799+0.00218551
[1087]	train-auc:0.789987+0.00146877	test-auc:0.777807+0.00219011
[1088]	train-auc:0.789997+0.00146992	test-auc:0.777809+0.00218815
[1089]	train-auc:0.790009+0.00147336	test-auc:0.777817+0.00218204
[1090]	train-auc:0.790026+0.00147128	test-auc:0.777817+0.00218646
[1091]	train-auc:0.790038+0.00147266	test-auc:0.777817+0.00218383
[1092]	train-auc:0.790048+0.00147544	test-auc:0.777818+0.00218002
[1093]	train-auc:0.79006+0.00147501	test-auc:0.777815+0.00218267
[1094]	train-auc:0.790072+0.001479	test-auc:0.777819+0.00218498
[1095]	train-auc:0.790084+0.00147948	test-auc:0.777824+0.00217684
[1096]	train-auc:0.790098+0.00147335	test-auc:0.777834+0.00218113
[1097]	train-auc:0.790112+0.00147483	test-auc:0.77784+0.00218389
[1098]	train-auc:0.790124+0.00146912	test-auc:0.77785+0.00219618
[1099]	train-auc:0.79014+0.00146955	test-auc:0.777858+0.00219569
[1100]	train-auc:0.79015+0.00147132	test-auc:0.777855+0.00220246
[1101]	train-auc:0.79016+0.00147498	test-auc:0.777863+0.00220989
[1102]	train-auc:0.79018+0.00147285	test-auc:0.777868+0.00221771
[1103]	train-auc:0.790197+0.00147506	test-auc:0.777874+0.00221225
[1104]	train-auc:0.790203+0.00147147	test-auc:0.777883+0.00221217
[1105]	train-auc:0.790219+0.00147111	test-auc:0.777884+0.00220929
[1106]	train-auc:0.79023+0.00146276	test-auc:0.77789+0.00221213
[1107]	train-auc:0.79024+0.00146505	test-auc:0.777886+0.00221268
[1108]	train-auc:0.790246+0.00146359	test-auc:0.777889+0.0022136
[1109]	train-auc:0.790256+0.00146717	test-auc:0.777888+0.00220574
[1110]	train-auc:0.790267+0.00147227	test-auc:0.777896+0.0021989
[1111]	train-auc:0.790275+0.00147228	test-auc:0.777901+0.00220812
[1112]	train-auc:0.790287+0.00147505	test-auc:0.777904+0.00220201
[1113]	train-auc:0.790296+0.0014731	test-auc:0.777909+0.0022016
[1114]	train-auc:0.7903+0.0014693	test-auc:0.777916+0.0022105
[1115]	train-auc:0.790314+0.00146657	test-auc:0.777931+0.00221865
[1116]	train-auc:0.790318+0.001467	test-auc:0.777935+0.00220784
[1117]	train-auc:0.790332+0.00146286	test-auc:0.777944+0.0022173
[1118]	train-auc:0.790337+0.00145839	test-auc:0.77794+0.00221485
[1119]	train-auc:0.790352+0.00146481	test-auc:0.777935+0.0022186
[1120]	train-auc:0.79037+0.00146302	test-auc:0.777933+0.0022189
[1121]	train-auc:0.790381+0.00146461	test-auc:0.77794+0.00222034
[1122]	train-auc:0.790393+0.00146127	test-auc:0.777938+0.00222417
[1123]	train-auc:0.790406+0.00146025	test-auc:0.777943+0.00222644
[1124]	train-auc:0.790415+0.00146081	test-auc:0.777949+0.00223494
[1125]	train-auc:0.790424+0.00146168	test-auc:0.777951+0.00222877
[1126]	train-auc:0.790438+0.00146544	test-auc:0.777949+0.0022323
[1127]	train-auc:0.790447+0.00147336	test-auc:0.777949+0.00222961
[1128]	train-auc:0.790459+0.00146764	test-auc:0.777957+0.00223975
[1129]	train-auc:0.790468+0.00146353	test-auc:0.777964+0.00224666
[1130]	train-auc:0.790475+0.00146043	test-auc:0.777963+0.00225314
[1131]	train-auc:0.790488+0.00146338	test-auc:0.777968+0.00224673
[1132]	train-auc:0.790499+0.00146644	test-auc:0.777969+0.00225581
[1133]	train-auc:0.790511+0.00147106	test-auc:0.777984+0.00225158
[1134]	train-auc:0.79052+0.00147208	test-auc:0.777988+0.00224409
[1135]	train-auc:0.790533+0.00146687	test-auc:0.777989+0.00224661
[1136]	train-auc:0.790541+0.00146209	test-auc:0.777991+0.0022456
[1137]	train-auc:0.790552+0.00145868	test-auc:0.777993+0.00224398
[1138]	train-auc:0.790563+0.00146033	test-auc:0.777994+0.0022392
[1139]	train-auc:0.79057+0.00146407	test-auc:0.777991+0.00224327
[1140]	train-auc:0.790578+0.00146859	test-auc:0.777995+0.00224046
[1141]	train-auc:0.790588+0.00146636	test-auc:0.778001+0.00224447
[1142]	train-auc:0.790599+0.00146863	test-auc:0.777998+0.00223914
[1143]	train-auc:0.79061+0.00147247	test-auc:0.777991+0.00223886
[1144]	train-auc:0.79062+0.00147324	test-auc:0.777985+0.00224454
[1145]	train-auc:0.790631+0.00147136	test-auc:0.777995+0.0022431
[1146]	train-auc:0.790647+0.00147211	test-auc:0.778+0.00224312
[1147]	train-auc:0.79066+0.00147386	test-auc:0.778+0.00223798
[1148]	train-auc:0.790663+0.00147092	test-auc:0.777996+0.00224239
[1149]	train-auc:0.790674+0.0014714	test-auc:0.778001+0.00225176
[1150]	train-auc:0.790685+0.00147098	test-auc:0.778013+0.00224919
[1151]	train-auc:0.790695+0.00147117	test-auc:0.77801+0.0022511
[1152]	train-auc:0.790705+0.0014709	test-auc:0.778014+0.00225582
[1153]	train-auc:0.790722+0.00147314	test-auc:0.77802+0.00225306
[1154]	train-auc:0.790732+0.00147211	test-auc:0.778024+0.00225648
[1155]	train-auc:0.790745+0.00147291	test-auc:0.778022+0.00225826
[1156]	train-auc:0.790759+0.00146436	test-auc:0.778025+0.00226451
[1157]	train-auc:0.790771+0.00146538	test-auc:0.77803+0.00226417
[1158]	train-auc:0.790783+0.00146741	test-auc:0.778028+0.00226791
[1159]	train-auc:0.790794+0.00146978	test-auc:0.778037+0.00227018
[1160]	train-auc:0.790807+0.00146898	test-auc:0.778045+0.00227602
[1161]	train-auc:0.79082+0.0014727	test-auc:0.778045+0.00226943
[1162]	train-auc:0.790828+0.00147389	test-auc:0.778049+0.00227342
[1163]	train-auc:0.790841+0.00147219	test-auc:0.778051+0.00227756
[1164]	train-auc:0.790854+0.00146915	test-auc:0.77807+0.00228419
[1165]	train-auc:0.790864+0.00146921	test-auc:0.778074+0.00227492
[1166]	train-auc:0.790872+0.00147557	test-auc:0.778074+0.00227503
[1167]	train-auc:0.790881+0.00148229	test-auc:0.778072+0.00227365
[1168]	train-auc:0.79089+0.00148674	test-auc:0.778073+0.00228142
[1169]	train-auc:0.7909+0.0014886	test-auc:0.778081+0.00227977
[1170]	train-auc:0.790913+0.00149495	test-auc:0.778088+0.00227554
[1171]	train-auc:0.790921+0.00149385	test-auc:0.778094+0.00226694
[1172]	train-auc:0.790932+0.00149639	test-auc:0.778107+0.0022727
[1173]	train-auc:0.790941+0.00149582	test-auc:0.778106+0.00227457
[1174]	train-auc:0.790949+0.00150015	test-auc:0.77811+0.00227486
[1175]	train-auc:0.790954+0.00150214	test-auc:0.778103+0.00226939
[1176]	train-auc:0.790966+0.00150525	test-auc:0.778103+0.00226842
[1177]	train-auc:0.790975+0.0015004	test-auc:0.778098+0.00227364
[1178]	train-auc:0.790979+0.00150126	test-auc:0.778106+0.0022807
[1179]	train-auc:0.790994+0.00150462	test-auc:0.778112+0.00228438
[1180]	train-auc:0.791011+0.0015095	test-auc:0.778114+0.00228139
[1181]	train-auc:0.791022+0.00150974	test-auc:0.778115+0.00229283
[1182]	train-auc:0.791033+0.00151136	test-auc:0.778118+0.00228992
[1183]	train-auc:0.79104+0.00150733	test-auc:0.778116+0.00229037
[1184]	train-auc:0.791051+0.00150202	test-auc:0.77812+0.0022896
[1185]	train-auc:0.791064+0.00149893	test-auc:0.778137+0.00230373
[1186]	train-auc:0.791071+0.00149284	test-auc:0.778144+0.00231063
[1187]	train-auc:0.79108+0.00149189	test-auc:0.778145+0.00231469
[1188]	train-auc:0.791091+0.00149381	test-auc:0.778149+0.00231196
[1189]	train-auc:0.791102+0.00149372	test-auc:0.778155+0.00230995
[1190]	train-auc:0.791111+0.00149268	test-auc:0.778162+0.0023131
[1191]	train-auc:0.791122+0.00149289	test-auc:0.778163+0.00231247
[1192]	train-auc:0.791131+0.00148959	test-auc:0.778164+0.00230951
[1193]	train-auc:0.791144+0.00149114	test-auc:0.778173+0.00231625
[1194]	train-auc:0.791154+0.00148905	test-auc:0.778178+0.00230932
[1195]	train-auc:0.791168+0.0014871	test-auc:0.778182+0.00231367
[1196]	train-auc:0.791178+0.00148188	test-auc:0.778201+0.00232976
[1197]	train-auc:0.791186+0.00147722	test-auc:0.778202+0.00232481
[1198]	train-auc:0.791194+0.00147231	test-auc:0.778201+0.00232802
[1199]	train-auc:0.791202+0.00147042	test-auc:0.778207+0.00232703
[1200]	train-auc:0.791214+0.0014678	test-auc:0.778205+0.00233878
[1201]	train-auc:0.791223+0.00146565	test-auc:0.778202+0.00233707
[1202]	train-auc:0.791237+0.00145839	test-auc:0.778208+0.00233687
[1203]	train-auc:0.791247+0.00145685	test-auc:0.778214+0.00233424
[1204]	train-auc:0.791258+0.00144936	test-auc:0.778217+0.0023331
[1205]	train-auc:0.791261+0.00145342	test-auc:0.778216+0.00232832
[1206]	train-auc:0.791276+0.00145327	test-auc:0.778217+0.00232733
[1207]	train-auc:0.791286+0.00145453	test-auc:0.778217+0.00232167
[1208]	train-auc:0.791296+0.00145323	test-auc:0.778217+0.00232039
[1209]	train-auc:0.791306+0.00144742	test-auc:0.778219+0.00231446
[1210]	train-auc:0.791316+0.00144231	test-auc:0.778223+0.0023224
[1211]	train-auc:0.791325+0.00144066	test-auc:0.778227+0.00232161
[1212]	train-auc:0.791331+0.00143871	test-auc:0.778226+0.00232173
[1213]	train-auc:0.79134+0.00144225	test-auc:0.77823+0.00232714
[1214]	train-auc:0.791348+0.0014433	test-auc:0.77823+0.00232686
[1215]	train-auc:0.791362+0.00143993	test-auc:0.778234+0.00232576
[1216]	train-auc:0.791371+0.00143747	test-auc:0.778232+0.00232378
[1217]	train-auc:0.791379+0.00143531	test-auc:0.778229+0.00232102
[1218]	train-auc:0.791392+0.00144047	test-auc:0.778238+0.00232141
[1219]	train-auc:0.7914+0.00143942	test-auc:0.778233+0.00232396
[1220]	train-auc:0.791411+0.00144173	test-auc:0.778243+0.00232931
[1221]	train-auc:0.791416+0.00144503	test-auc:0.778249+0.00232366
[1222]	train-auc:0.791424+0.00144017	test-auc:0.778257+0.00232998
[1223]	train-auc:0.791434+0.00143938	test-auc:0.778259+0.00233304
[1224]	train-auc:0.791444+0.00143855	test-auc:0.778258+0.00233333
[1225]	train-auc:0.79146+0.00143762	test-auc:0.77826+0.00233056
[1226]	train-auc:0.79147+0.00143831	test-auc:0.778265+0.00233264
[1227]	train-auc:0.791484+0.00143527	test-auc:0.778271+0.00233854
[1228]	train-auc:0.791497+0.00143831	test-auc:0.778271+0.00233974
[1229]	train-auc:0.791509+0.00143676	test-auc:0.778265+0.00233849
[1230]	train-auc:0.791514+0.00143979	test-auc:0.778268+0.0023353
[1231]	train-auc:0.791522+0.00144147	test-auc:0.778267+0.00233588
[1232]	train-auc:0.791531+0.00143885	test-auc:0.778268+0.00234323
[1233]	train-auc:0.791542+0.0014333	test-auc:0.778273+0.00234243
[1234]	train-auc:0.791552+0.00142856	test-auc:0.778284+0.00234696
[1235]	train-auc:0.791562+0.00143312	test-auc:0.778282+0.00234985
[1236]	train-auc:0.791575+0.00142997	test-auc:0.778287+0.00234287
[1237]	train-auc:0.791588+0.00142773	test-auc:0.778292+0.00234221
[1238]	train-auc:0.7916+0.00142706	test-auc:0.778298+0.00233453
[1239]	train-auc:0.791609+0.00142621	test-auc:0.778305+0.00233301
[1240]	train-auc:0.791619+0.00142522	test-auc:0.778306+0.00232612
[1241]	train-auc:0.791629+0.00142579	test-auc:0.778307+0.00232233
[1242]	train-auc:0.791642+0.00141786	test-auc:0.778312+0.00232593
[1243]	train-auc:0.791651+0.00141688	test-auc:0.778313+0.00233527
[1244]	train-auc:0.791663+0.00141806	test-auc:0.778315+0.00234038
[1245]	train-auc:0.791667+0.00141823	test-auc:0.778323+0.00233753
[1246]	train-auc:0.791679+0.00141722	test-auc:0.778323+0.00234091
[1247]	train-auc:0.791686+0.0014215	test-auc:0.77833+0.00233815
[1248]	train-auc:0.791695+0.00142356	test-auc:0.778334+0.0023413
[1249]	train-auc:0.791708+0.00142037	test-auc:0.778333+0.00233986
[1250]	train-auc:0.791718+0.00142818	test-auc:0.778328+0.00234112
[1251]	train-auc:0.791726+0.00142974	test-auc:0.778331+0.00234279
[1252]	train-auc:0.79174+0.00143326	test-auc:0.778327+0.00234308
[1253]	train-auc:0.791753+0.00143346	test-auc:0.778329+0.0023438
[1254]	train-auc:0.791758+0.00142945	test-auc:0.778336+0.00235624
[1255]	train-auc:0.79177+0.0014273	test-auc:0.778335+0.0023563
[1256]	train-auc:0.791777+0.00142689	test-auc:0.778337+0.00235295
[1257]	train-auc:0.791786+0.00142622	test-auc:0.778339+0.00235114
[1258]	train-auc:0.791799+0.00142477	test-auc:0.778339+0.00235809
[1259]	train-auc:0.791806+0.00142294	test-auc:0.778347+0.00235513
[1260]	train-auc:0.791816+0.00142465	test-auc:0.778352+0.00235488
[1261]	train-auc:0.791822+0.0014254	test-auc:0.778348+0.00235157
[1262]	train-auc:0.791831+0.00142503	test-auc:0.778351+0.0023524
[1263]	train-auc:0.791842+0.0014298	test-auc:0.778346+0.00234967
[1264]	train-auc:0.791858+0.00142877	test-auc:0.77836+0.0023541
[1265]	train-auc:0.791864+0.00142928	test-auc:0.778355+0.00235192
[1266]	train-auc:0.791875+0.00142588	test-auc:0.778363+0.00235668
[1267]	train-auc:0.791889+0.00142339	test-auc:0.778368+0.00236075
[1268]	train-auc:0.791903+0.00142575	test-auc:0.778369+0.00235925
[1269]	train-auc:0.791914+0.00142542	test-auc:0.778374+0.00235646
[1270]	train-auc:0.791923+0.00142627	test-auc:0.77838+0.00235935
[1271]	train-auc:0.791934+0.00142431	test-auc:0.778389+0.00235879
[1272]	train-auc:0.791941+0.00142227	test-auc:0.778391+0.00236206
[1273]	train-auc:0.791946+0.00141956	test-auc:0.778393+0.00236077
[1274]	train-auc:0.791956+0.00142318	test-auc:0.778391+0.00235107
[1275]	train-auc:0.791967+0.00142788	test-auc:0.778392+0.00234791
[1276]	train-auc:0.791975+0.00142668	test-auc:0.778391+0.00234909
[1277]	train-auc:0.791984+0.00142504	test-auc:0.778392+0.002341
[1278]	train-auc:0.79199+0.0014283	test-auc:0.77839+0.00234868
[1279]	train-auc:0.792+0.00143061	test-auc:0.778387+0.00234901
[1280]	train-auc:0.792013+0.00142847	test-auc:0.778396+0.00235378
[1281]	train-auc:0.792025+0.00143213	test-auc:0.7784+0.00234439
[1282]	train-auc:0.792033+0.00143305	test-auc:0.778398+0.00234779
[1283]	train-auc:0.792042+0.0014334	test-auc:0.778406+0.0023527
[1284]	train-auc:0.792056+0.00142933	test-auc:0.778409+0.00234769
[1285]	train-auc:0.792068+0.00142358	test-auc:0.778414+0.00234794
[1286]	train-auc:0.792077+0.00142111	test-auc:0.778413+0.00235264
[1287]	train-auc:0.792084+0.00141733	test-auc:0.778415+0.00234745
[1288]	train-auc:0.792094+0.00141059	test-auc:0.778421+0.00235424
[1289]	train-auc:0.792101+0.00141157	test-auc:0.778431+0.00235101
[1290]	train-auc:0.792111+0.00140521	test-auc:0.778439+0.0023617
[1291]	train-auc:0.792121+0.00141045	test-auc:0.778443+0.00235826
[1292]	train-auc:0.792132+0.00140672	test-auc:0.778438+0.0023615
[1293]	train-auc:0.792146+0.00140413	test-auc:0.778441+0.0023552
[1294]	train-auc:0.792156+0.0014042	test-auc:0.77845+0.00235673
[1295]	train-auc:0.792167+0.00140235	test-auc:0.778457+0.0023529
[1296]	train-auc:0.792175+0.00140058	test-auc:0.778462+0.00235765
[1297]	train-auc:0.79218+0.00139907	test-auc:0.77846+0.00235546
[1298]	train-auc:0.792191+0.00140041	test-auc:0.77846+0.00235705
[1299]	train-auc:0.792203+0.00140041	test-auc:0.778456+0.0023587
[1300]	train-auc:0.792205+0.00140228	test-auc:0.778458+0.00235967
[1301]	train-auc:0.792219+0.00140149	test-auc:0.778448+0.0023658
[1302]	train-auc:0.792233+0.0014067	test-auc:0.778445+0.0023638
[1303]	train-auc:0.79224+0.00140814	test-auc:0.778442+0.00236265
[1304]	train-auc:0.792253+0.00141127	test-auc:0.77844+0.00236411
[1305]	train-auc:0.792262+0.00140891	test-auc:0.778446+0.00237619
[1306]	train-auc:0.792271+0.00140601	test-auc:0.778448+0.00237611
[1307]	train-auc:0.792282+0.00140299	test-auc:0.778458+0.00237961
[1308]	train-auc:0.792291+0.00140485	test-auc:0.778455+0.00238542
[1309]	train-auc:0.792303+0.00140958	test-auc:0.778454+0.00237982
[1310]	train-auc:0.792313+0.00141172	test-auc:0.778454+0.00237878
[1311]	train-auc:0.792325+0.00140829	test-auc:0.778464+0.00238352
[1312]	train-auc:0.792332+0.00141504	test-auc:0.778468+0.0023872
[1313]	train-auc:0.792345+0.00141504	test-auc:0.778477+0.00238863
[1314]	train-auc:0.792354+0.00141444	test-auc:0.778481+0.00239028
[1315]	train-auc:0.792363+0.00140813	test-auc:0.778479+0.00239794
[1316]	train-auc:0.792375+0.00141025	test-auc:0.778474+0.00239476
[1317]	train-auc:0.792385+0.00140695	test-auc:0.778469+0.00238913
[1318]	train-auc:0.792394+0.00140527	test-auc:0.778474+0.00239093
[1319]	train-auc:0.792399+0.00140359	test-auc:0.778482+0.00239336
[1320]	train-auc:0.792406+0.00140444	test-auc:0.778482+0.00239265
[1321]	train-auc:0.792416+0.00140503	test-auc:0.778477+0.00239966
[1322]	train-auc:0.792425+0.00140733	test-auc:0.778474+0.00240068
[1323]	train-auc:0.792428+0.00140588	test-auc:0.778475+0.0024014
[1324]	train-auc:0.792435+0.00140737	test-auc:0.778477+0.00240075
[1325]	train-auc:0.792445+0.00140651	test-auc:0.778484+0.00240259
[1326]	train-auc:0.792451+0.00140256	test-auc:0.778492+0.00239787
[1327]	train-auc:0.792459+0.00140248	test-auc:0.778493+0.002398
[1328]	train-auc:0.792475+0.00139996	test-auc:0.778503+0.00240583
[1329]	train-auc:0.792488+0.00139683	test-auc:0.7785+0.00240244
[1330]	train-auc:0.792494+0.00139733	test-auc:0.778497+0.00240028
[1331]	train-auc:0.792499+0.00139674	test-auc:0.778495+0.00239582
[1332]	train-auc:0.792507+0.00139394	test-auc:0.778499+0.00238906
[1333]	train-auc:0.792519+0.00139349	test-auc:0.778508+0.00239375
[1334]	train-auc:0.792527+0.00139167	test-auc:0.77851+0.00239681
[1335]	train-auc:0.79254+0.00138907	test-auc:0.778517+0.00239719
[1336]	train-auc:0.792551+0.00139299	test-auc:0.77852+0.00240024
[1337]	train-auc:0.792568+0.00139192	test-auc:0.778535+0.00240923
[1338]	train-auc:0.792579+0.00139514	test-auc:0.778542+0.0024091
[1339]	train-auc:0.79259+0.00139904	test-auc:0.77855+0.0024115
[1340]	train-auc:0.792599+0.0013987	test-auc:0.778549+0.00240544
[1341]	train-auc:0.792606+0.00139995	test-auc:0.778548+0.00239937
[1342]	train-auc:0.792615+0.0013976	test-auc:0.778548+0.00239996
[1343]	train-auc:0.79263+0.00140532	test-auc:0.778557+0.00239782
[1344]	train-auc:0.792641+0.00140525	test-auc:0.778558+0.00240726
[1345]	train-auc:0.792649+0.00140783	test-auc:0.778563+0.00240615
[1346]	train-auc:0.79266+0.00140321	test-auc:0.778563+0.00240746
[1347]	train-auc:0.792667+0.00139873	test-auc:0.778572+0.00241863
[1348]	train-auc:0.792673+0.00139247	test-auc:0.778571+0.00241783
[1349]	train-auc:0.792684+0.00138674	test-auc:0.778566+0.00241787
[1350]	train-auc:0.792694+0.00138883	test-auc:0.778564+0.00241703
[1351]	train-auc:0.792703+0.0013869	test-auc:0.778566+0.00241443
[1352]	train-auc:0.792712+0.00138602	test-auc:0.778564+0.00241755
[1353]	train-auc:0.792722+0.0013864	test-auc:0.778566+0.00241489
[1354]	train-auc:0.792728+0.00138834	test-auc:0.778566+0.00241566
[1355]	train-auc:0.792739+0.00138732	test-auc:0.778572+0.00241813
[1356]	train-auc:0.792745+0.00138783	test-auc:0.778571+0.00242023
[1357]	train-auc:0.792754+0.00138663	test-auc:0.778574+0.00242802
[1358]	train-auc:0.792764+0.00138619	test-auc:0.778575+0.00241802
[1359]	train-auc:0.79277+0.00139194	test-auc:0.778574+0.00241976
[1360]	train-auc:0.792774+0.00139443	test-auc:0.778567+0.00241981
[1361]	train-auc:0.792781+0.00139384	test-auc:0.778564+0.00241657
[1362]	train-auc:0.792794+0.00138975	test-auc:0.778566+0.00241654
[1363]	train-auc:0.792806+0.00138552	test-auc:0.77857+0.00242694
[1364]	train-auc:0.792818+0.0013816	test-auc:0.778571+0.00243367
[1365]	train-auc:0.792828+0.00138269	test-auc:0.778579+0.00242989
[1366]	train-auc:0.792838+0.0013821	test-auc:0.778579+0.00241594
[1367]	train-auc:0.792851+0.00138024	test-auc:0.778581+0.00241276
[1368]	train-auc:0.79286+0.00137748	test-auc:0.778577+0.002417
[1369]	train-auc:0.792871+0.00138105	test-auc:0.778579+0.0024174
[1370]	train-auc:0.792878+0.0013804	test-auc:0.778581+0.00241298
[1371]	train-auc:0.792885+0.00137841	test-auc:0.778575+0.0024175
[1372]	train-auc:0.792893+0.00137929	test-auc:0.778575+0.00241923
[1373]	train-auc:0.792904+0.0013853	test-auc:0.778572+0.00242063
[1374]	train-auc:0.792908+0.00138086	test-auc:0.77858+0.00242218
[1375]	train-auc:0.79292+0.00138021	test-auc:0.778578+0.0024233
[1376]	train-auc:0.792931+0.00138374	test-auc:0.778581+0.00242523
[1377]	train-auc:0.79294+0.00138681	test-auc:0.778577+0.00242608
[1378]	train-auc:0.792954+0.0013868	test-auc:0.778591+0.00243378
[1379]	train-auc:0.792966+0.00138598	test-auc:0.778592+0.00244555
[1380]	train-auc:0.792979+0.00138947	test-auc:0.778595+0.00244702
[1381]	train-auc:0.792988+0.00138878	test-auc:0.778596+0.0024503
[1382]	train-auc:0.792995+0.00139553	test-auc:0.778593+0.00245436
[1383]	train-auc:0.793008+0.0013911	test-auc:0.778599+0.00246884
[1384]	train-auc:0.793018+0.0013943	test-auc:0.778601+0.0024694
[1385]	train-auc:0.793026+0.00139673	test-auc:0.77861+0.00246703
[1386]	train-auc:0.793035+0.00139612	test-auc:0.778607+0.0024659
[1387]	train-auc:0.79304+0.00139582	test-auc:0.778609+0.00246578
[1388]	train-auc:0.793053+0.00139819	test-auc:0.778609+0.00246185
[1389]	train-auc:0.793063+0.0013982	test-auc:0.778602+0.00245987
[1390]	train-auc:0.793072+0.00139546	test-auc:0.778607+0.00245661
[1391]	train-auc:0.793081+0.00139345	test-auc:0.778613+0.00246004
[1392]	train-auc:0.793091+0.00139606	test-auc:0.778618+0.00245482
[1393]	train-auc:0.793101+0.00139531	test-auc:0.77862+0.00245205
[1394]	train-auc:0.793114+0.00139349	test-auc:0.77862+0.00244628
[1395]	train-auc:0.793124+0.00139175	test-auc:0.778623+0.00245016
[1396]	train-auc:0.793138+0.00139321	test-auc:0.778619+0.00244697
[1397]	train-auc:0.79314+0.00139354	test-auc:0.778618+0.0024433
[1398]	train-auc:0.793148+0.00139105	test-auc:0.77861+0.00244667
[1399]	train-auc:0.793154+0.00138873	test-auc:0.778615+0.0024511
[1400]	train-auc:0.793162+0.0013887	test-auc:0.77862+0.0024496
[1401]	train-auc:0.793172+0.00138899	test-auc:0.77862+0.00245408
[1402]	train-auc:0.793184+0.00138651	test-auc:0.778628+0.00247228
[1403]	train-auc:0.79319+0.00138612	test-auc:0.778634+0.00247454
[1404]	train-auc:0.7932+0.00138403	test-auc:0.778633+0.00247826
[1405]	train-auc:0.79321+0.00138081	test-auc:0.778643+0.00248503
[1406]	train-auc:0.79322+0.00138301	test-auc:0.778642+0.00248081
[1407]	train-auc:0.793229+0.00138005	test-auc:0.778648+0.00248212
[1408]	train-auc:0.793238+0.00137384	test-auc:0.778648+0.002475
[1409]	train-auc:0.793252+0.0013696	test-auc:0.778649+0.00247731
[1410]	train-auc:0.793265+0.00136924	test-auc:0.778645+0.00247596
[1411]	train-auc:0.793271+0.0013707	test-auc:0.778651+0.0024769
[1412]	train-auc:0.793281+0.00137254	test-auc:0.778647+0.00248201
[1413]	train-auc:0.793287+0.00137039	test-auc:0.778655+0.00249204
[1414]	train-auc:0.793295+0.00136749	test-auc:0.778658+0.0024893
[1415]	train-auc:0.793308+0.00136764	test-auc:0.77866+0.00248655
[1416]	train-auc:0.793319+0.00136471	test-auc:0.77867+0.00248958
[1417]	train-auc:0.793332+0.00136677	test-auc:0.778682+0.00249194
[1418]	train-auc:0.793339+0.00136668	test-auc:0.778686+0.00249115
[1419]	train-auc:0.793347+0.0013626	test-auc:0.778689+0.00249776
[1420]	train-auc:0.793355+0.00136194	test-auc:0.778694+0.00250814
[1421]	train-auc:0.793364+0.00136022	test-auc:0.778692+0.00250708
[1422]	train-auc:0.793374+0.00135794	test-auc:0.778698+0.00251152
[1423]	train-auc:0.79338+0.00135354	test-auc:0.778701+0.00250857
[1424]	train-auc:0.793387+0.00135343	test-auc:0.7787+0.00250476
[1425]	train-auc:0.793396+0.00135191	test-auc:0.778695+0.0025065
[1426]	train-auc:0.793407+0.00135	test-auc:0.778691+0.00250688
[1427]	train-auc:0.793413+0.00135432	test-auc:0.778693+0.0025014
[1428]	train-auc:0.793426+0.00135356	test-auc:0.778693+0.00251092
[1429]	train-auc:0.793431+0.00135913	test-auc:0.778688+0.00251356
[1430]	train-auc:0.793438+0.00136341	test-auc:0.778695+0.00251404
[1431]	train-auc:0.793447+0.00135764	test-auc:0.778698+0.00251615
[1432]	train-auc:0.793457+0.00135295	test-auc:0.778704+0.00251424
[1433]	train-auc:0.793465+0.00135142	test-auc:0.778709+0.00251487
[1434]	train-auc:0.793476+0.00135136	test-auc:0.778709+0.00251346
[1435]	train-auc:0.793483+0.00135482	test-auc:0.778707+0.00251206
[1436]	train-auc:0.793493+0.00135364	test-auc:0.778708+0.00251243
[1437]	train-auc:0.793502+0.00135494	test-auc:0.778706+0.00251093
[1438]	train-auc:0.793507+0.0013579	test-auc:0.778708+0.00250965
[1439]	train-auc:0.793513+0.00135638	test-auc:0.778712+0.00250681
[1440]	train-auc:0.793518+0.00135981	test-auc:0.77872+0.0025055
[1441]	train-auc:0.793524+0.00136125	test-auc:0.778722+0.00250524
[1442]	train-auc:0.793531+0.00136127	test-auc:0.778721+0.00250688
[1443]	train-auc:0.793534+0.00135941	test-auc:0.77872+0.00250752
[1444]	train-auc:0.793545+0.00136104	test-auc:0.778726+0.00250738
[1445]	train-auc:0.793554+0.00136619	test-auc:0.778731+0.0025063
[1446]	train-auc:0.793569+0.00136673	test-auc:0.77873+0.00251389
[1447]	train-auc:0.793579+0.00136869	test-auc:0.778738+0.00251753
[1448]	train-auc:0.793592+0.00136728	test-auc:0.778743+0.00252431
[1449]	train-auc:0.793604+0.00136767	test-auc:0.778744+0.00252575
[1450]	train-auc:0.793615+0.00136837	test-auc:0.778746+0.00252907
[1451]	train-auc:0.793619+0.00136908	test-auc:0.778744+0.00253209
[1452]	train-auc:0.793627+0.00136718	test-auc:0.778748+0.00252951
[1453]	train-auc:0.793638+0.00136841	test-auc:0.778743+0.0025267
[1454]	train-auc:0.793647+0.00136874	test-auc:0.778748+0.00252575
[1455]	train-auc:0.793652+0.00136796	test-auc:0.778747+0.00252106
[1456]	train-auc:0.793661+0.00136833	test-auc:0.77874+0.00252252
[1457]	train-auc:0.793675+0.0013696	test-auc:0.778739+0.00252548
[1458]	train-auc:0.793686+0.00136855	test-auc:0.778741+0.00252496
[1459]	train-auc:0.793695+0.0013714	test-auc:0.778738+0.00252626
[1460]	train-auc:0.793705+0.00137069	test-auc:0.77874+0.00252463
[1461]	train-auc:0.793717+0.00137407	test-auc:0.778742+0.00253756
[1462]	train-auc:0.793732+0.00137318	test-auc:0.778741+0.00254213
[1463]	train-auc:0.793742+0.00137268	test-auc:0.778745+0.00253597
[1464]	train-auc:0.79375+0.00137497	test-auc:0.778749+0.00253366
[1465]	train-auc:0.79376+0.00137674	test-auc:0.778757+0.00252747
[1466]	train-auc:0.793767+0.00138013	test-auc:0.778766+0.00252117
[1467]	train-auc:0.793777+0.00138325	test-auc:0.778772+0.00251798
[1468]	train-auc:0.793789+0.00138371	test-auc:0.778778+0.00252575
[1469]	train-auc:0.7938+0.0013838	test-auc:0.778782+0.00252071
[1470]	train-auc:0.793808+0.00138178	test-auc:0.778788+0.00252648
[1471]	train-auc:0.79382+0.00138318	test-auc:0.778784+0.00253477
[1472]	train-auc:0.79383+0.00138211	test-auc:0.778786+0.00253715
[1473]	train-auc:0.793838+0.00138602	test-auc:0.778787+0.00253656
[1474]	train-auc:0.793852+0.0013839	test-auc:0.778789+0.00254647
[1475]	train-auc:0.793857+0.00138621	test-auc:0.778791+0.00254636
[1476]	train-auc:0.793869+0.00138639	test-auc:0.778792+0.00255151
[1477]	train-auc:0.793878+0.00138825	test-auc:0.778799+0.00254862
[1478]	train-auc:0.793889+0.0013898	test-auc:0.778805+0.00254836
[1479]	train-auc:0.793901+0.00139173	test-auc:0.778816+0.00254556
[1480]	train-auc:0.793914+0.00138769	test-auc:0.778817+0.00254641
[1481]	train-auc:0.79392+0.00138856	test-auc:0.77882+0.00254922
[1482]	train-auc:0.793931+0.00139362	test-auc:0.778815+0.00255743
[1483]	train-auc:0.793937+0.00139273	test-auc:0.778821+0.00255418
[1484]	train-auc:0.793948+0.00139039	test-auc:0.778815+0.00255516
[1485]	train-auc:0.793962+0.00139045	test-auc:0.778824+0.00255755
[1486]	train-auc:0.793974+0.00138976	test-auc:0.778828+0.00255958
[1487]	train-auc:0.793984+0.00139517	test-auc:0.77883+0.00256482
[1488]	train-auc:0.793993+0.00139659	test-auc:0.778836+0.00256428
[1489]	train-auc:0.794001+0.00139741	test-auc:0.778836+0.00256513
[1490]	train-auc:0.794012+0.00140098	test-auc:0.778839+0.00256679
[1491]	train-auc:0.79402+0.0013979	test-auc:0.778838+0.00256527
[1492]	train-auc:0.794028+0.00139844	test-auc:0.77884+0.00256684
[1493]	train-auc:0.794036+0.00140274	test-auc:0.778837+0.00256628
[1494]	train-auc:0.794045+0.00140323	test-auc:0.778837+0.00256638
[1495]	train-auc:0.794051+0.00140079	test-auc:0.778841+0.00256288
[1496]	train-auc:0.794059+0.00140223	test-auc:0.77884+0.00256467
[1497]	train-auc:0.79407+0.00140361	test-auc:0.778836+0.0025651
[1498]	train-auc:0.794083+0.00140628	test-auc:0.778832+0.00256387
[1499]	train-auc:0.79409+0.00140188	test-auc:0.77883+0.00256445
[1500]	train-auc:0.7941+0.00140319	test-auc:0.778832+0.00257132
[1501]	train-auc:0.794111+0.00140258	test-auc:0.778845+0.00257566
[1502]	train-auc:0.794121+0.00140003	test-auc:0.77885+0.00258695
[1503]	train-auc:0.794129+0.00140093	test-auc:0.77885+0.00258878
[1504]	train-auc:0.794145+0.00140366	test-auc:0.778852+0.00258125
[1505]	train-auc:0.794148+0.00140357	test-auc:0.778854+0.00258312
[1506]	train-auc:0.79416+0.00140026	test-auc:0.778856+0.00258522
[1507]	train-auc:0.794162+0.0013997	test-auc:0.778861+0.00258661
[1508]	train-auc:0.794175+0.00139934	test-auc:0.778858+0.00259243
[1509]	train-auc:0.794182+0.00139329	test-auc:0.778863+0.00259089
[1510]	train-auc:0.794195+0.0013937	test-auc:0.778863+0.00259295
[1511]	train-auc:0.794208+0.0013918	test-auc:0.778868+0.00259567
[1512]	train-auc:0.79421+0.00139557	test-auc:0.778872+0.00259676
[1513]	train-auc:0.794218+0.00139615	test-auc:0.778876+0.00259422
[1514]	train-auc:0.794224+0.00139472	test-auc:0.778875+0.00258885
[1515]	train-auc:0.79423+0.00139393	test-auc:0.778874+0.00259153
[1516]	train-auc:0.794235+0.0013935	test-auc:0.778877+0.00259315
[1517]	train-auc:0.794242+0.00139673	test-auc:0.778879+0.00258769
[1518]	train-auc:0.794253+0.00139919	test-auc:0.778877+0.00259261
[1519]	train-auc:0.794262+0.00139704	test-auc:0.778875+0.00259056
[1520]	train-auc:0.794266+0.00139778	test-auc:0.77888+0.00259009
[1521]	train-auc:0.794271+0.00139858	test-auc:0.778882+0.00258768
[1522]	train-auc:0.79428+0.00139951	test-auc:0.778891+0.00259205
[1523]	train-auc:0.794289+0.0014012	test-auc:0.778887+0.00258638
[1524]	train-auc:0.794302+0.00139479	test-auc:0.778892+0.00258866
[1525]	train-auc:0.79431+0.00139193	test-auc:0.778895+0.00259041
[1526]	train-auc:0.794319+0.0013905	test-auc:0.778895+0.00259212
[1527]	train-auc:0.794326+0.00138674	test-auc:0.778901+0.00258823
[1528]	train-auc:0.794336+0.0013817	test-auc:0.778903+0.00258824
[1529]	train-auc:0.794341+0.00138376	test-auc:0.778903+0.00257823
[1530]	train-auc:0.794349+0.00137905	test-auc:0.778908+0.0025864
[1531]	train-auc:0.794357+0.00137375	test-auc:0.778907+0.00258797
[1532]	train-auc:0.794366+0.0013754	test-auc:0.778907+0.00258738
[1533]	train-auc:0.794373+0.00137313	test-auc:0.778913+0.00258479
[1534]	train-auc:0.794383+0.00137205	test-auc:0.778913+0.00258568
[1535]	train-auc:0.794386+0.00137359	test-auc:0.77891+0.00258253
[1536]	train-auc:0.794394+0.00136791	test-auc:0.77891+0.00259042
[1537]	train-auc:0.794401+0.00136907	test-auc:0.778917+0.0025886
[1538]	train-auc:0.794411+0.00136639	test-auc:0.778913+0.00258968
[1539]	train-auc:0.79442+0.00136639	test-auc:0.778916+0.00257824
[1540]	train-auc:0.794428+0.00136382	test-auc:0.778915+0.00257263
[1541]	train-auc:0.794436+0.00136313	test-auc:0.778912+0.00257341
[1542]	train-auc:0.79445+0.00136501	test-auc:0.778915+0.00257446
[1543]	train-auc:0.794459+0.00136693	test-auc:0.77892+0.00257742
[1544]	train-auc:0.794467+0.00137462	test-auc:0.778916+0.00258148
[1545]	train-auc:0.794476+0.00136985	test-auc:0.778917+0.0025752
[1546]	train-auc:0.794483+0.00137052	test-auc:0.778912+0.00256663
[1547]	train-auc:0.794491+0.00136827	test-auc:0.778917+0.00256231
[1548]	train-auc:0.794504+0.00136003	test-auc:0.778916+0.00256335
[1549]	train-auc:0.794513+0.00136423	test-auc:0.778927+0.00256072
[1550]	train-auc:0.794523+0.00136579	test-auc:0.778924+0.00255714
[1551]	train-auc:0.794528+0.00136288	test-auc:0.778923+0.00255616
[1552]	train-auc:0.794535+0.00135986	test-auc:0.778922+0.00255861
[1553]	train-auc:0.794538+0.00136005	test-auc:0.778931+0.00255873
[1554]	train-auc:0.794549+0.00135986	test-auc:0.778932+0.00255619
[1555]	train-auc:0.794561+0.00135586	test-auc:0.778933+0.00255347
[1556]	train-auc:0.794569+0.00135635	test-auc:0.778929+0.00254985
[1557]	train-auc:0.79458+0.00136266	test-auc:0.778937+0.0025475
[1558]	train-auc:0.794587+0.00135924	test-auc:0.778939+0.00255122
[1559]	train-auc:0.794601+0.00136478	test-auc:0.778936+0.00255284
[1560]	train-auc:0.794609+0.00136462	test-auc:0.778939+0.00255548
[1561]	train-auc:0.794617+0.00136126	test-auc:0.778939+0.00255729
[1562]	train-auc:0.794628+0.00135948	test-auc:0.77894+0.00255973
[1563]	train-auc:0.794638+0.00136457	test-auc:0.778938+0.00255791
[1564]	train-auc:0.794648+0.00136894	test-auc:0.778938+0.00255596
[1565]	train-auc:0.794658+0.00136775	test-auc:0.778947+0.00256128
[1566]	train-auc:0.794663+0.00137059	test-auc:0.778945+0.00256607
[1567]	train-auc:0.794672+0.00137504	test-auc:0.77894+0.00256891
[1568]	train-auc:0.794682+0.00137789	test-auc:0.778946+0.00257008
[1569]	train-auc:0.79469+0.00138086	test-auc:0.778955+0.00256646
[1570]	train-auc:0.794697+0.00138378	test-auc:0.778959+0.00256758
[1571]	train-auc:0.794709+0.00137697	test-auc:0.778962+0.00256384
[1572]	train-auc:0.794717+0.00137376	test-auc:0.778968+0.00256917
[1573]	train-auc:0.794722+0.00137164	test-auc:0.778963+0.00257039
[1574]	train-auc:0.794731+0.00137558	test-auc:0.778972+0.00257419
[1575]	train-auc:0.79474+0.00137669	test-auc:0.778977+0.00257018
[1576]	train-auc:0.794751+0.00137208	test-auc:0.778972+0.00256612
[1577]	train-auc:0.794756+0.00137113	test-auc:0.778969+0.00256995
[1578]	train-auc:0.794767+0.00137144	test-auc:0.778972+0.0025756
[1579]	train-auc:0.79478+0.00137044	test-auc:0.778969+0.00257497
[1580]	train-auc:0.794789+0.00136617	test-auc:0.778979+0.00258073
[1581]	train-auc:0.7948+0.00136732	test-auc:0.778977+0.00257574
[1582]	train-auc:0.794813+0.00136825	test-auc:0.778982+0.00258394
[1583]	train-auc:0.79482+0.00136921	test-auc:0.778987+0.002587
[1584]	train-auc:0.794835+0.00136919	test-auc:0.778989+0.00258387
[1585]	train-auc:0.794841+0.0013642	test-auc:0.779001+0.00259598
[1586]	train-auc:0.794846+0.00136447	test-auc:0.779+0.00260023
[1587]	train-auc:0.794854+0.00136348	test-auc:0.779002+0.00259779
[1588]	train-auc:0.794862+0.00136529	test-auc:0.778995+0.00259917
[1589]	train-auc:0.794865+0.0013658	test-auc:0.778992+0.00259773
[1590]	train-auc:0.794877+0.00136744	test-auc:0.778992+0.00259665
[1591]	train-auc:0.794882+0.00136987	test-auc:0.778994+0.00260016
[1592]	train-auc:0.794892+0.00137097	test-auc:0.778993+0.00260044
[1593]	train-auc:0.794904+0.00136995	test-auc:0.778999+0.00260117
[1594]	train-auc:0.794916+0.00137215	test-auc:0.778998+0.00260074
[1595]	train-auc:0.794924+0.00137561	test-auc:0.778999+0.0026054
[1596]	train-auc:0.794933+0.00137286	test-auc:0.778995+0.00260392
[1597]	train-auc:0.794945+0.00137843	test-auc:0.778993+0.00260582
[1598]	train-auc:0.794956+0.0013794	test-auc:0.778998+0.00260567
[1599]	train-auc:0.794964+0.00137805	test-auc:0.778999+0.00260345
[1600]	train-auc:0.79497+0.00137489	test-auc:0.779006+0.00260366
[1601]	train-auc:0.794981+0.00137848	test-auc:0.779011+0.00260184
[1602]	train-auc:0.794993+0.00137603	test-auc:0.779008+0.00260407
[1603]	train-auc:0.794999+0.00137655	test-auc:0.779012+0.0026046
[1604]	train-auc:0.79501+0.00137337	test-auc:0.779011+0.00260972
[1605]	train-auc:0.795019+0.00137171	test-auc:0.77901+0.00260715
[1606]	train-auc:0.795031+0.00137494	test-auc:0.779012+0.00261098
[1607]	train-auc:0.795033+0.00137298	test-auc:0.779015+0.00261643
[1608]	train-auc:0.795039+0.0013756	test-auc:0.779013+0.0026152
[1609]	train-auc:0.795046+0.00137498	test-auc:0.779012+0.00261158
[1610]	train-auc:0.795053+0.00137488	test-auc:0.779016+0.00261325
[1611]	train-auc:0.79506+0.00137648	test-auc:0.779018+0.00261615
[1612]	train-auc:0.795066+0.00137526	test-auc:0.779022+0.00260772
[1613]	train-auc:0.795075+0.00137558	test-auc:0.779027+0.00260868
[1614]	train-auc:0.795087+0.00137053	test-auc:0.779032+0.00261701
[1615]	train-auc:0.795098+0.00136963	test-auc:0.779031+0.00261527
[1616]	train-auc:0.795102+0.00137069	test-auc:0.779032+0.00261675
[1617]	train-auc:0.795113+0.0013678	test-auc:0.779038+0.00262558
[1618]	train-auc:0.795122+0.00136936	test-auc:0.779044+0.0026242
[1619]	train-auc:0.795128+0.00137158	test-auc:0.779045+0.00262762
[1620]	train-auc:0.795136+0.00137345	test-auc:0.779044+0.00263343
[1621]	train-auc:0.795143+0.00137785	test-auc:0.779042+0.00263384
[1622]	train-auc:0.795151+0.00137909	test-auc:0.779038+0.00263504
[1623]	train-auc:0.79516+0.00137952	test-auc:0.779042+0.0026358
[1624]	train-auc:0.795171+0.00138362	test-auc:0.779046+0.00263561
[1625]	train-auc:0.795182+0.00138062	test-auc:0.779057+0.00264423
[1626]	train-auc:0.795191+0.00137991	test-auc:0.779054+0.00264361
[1627]	train-auc:0.795201+0.00137814	test-auc:0.779063+0.00264223
[1628]	train-auc:0.795215+0.00137667	test-auc:0.779066+0.00264736
[1629]	train-auc:0.795222+0.00137777	test-auc:0.779062+0.00264527
[1630]	train-auc:0.795236+0.00137463	test-auc:0.779056+0.00263904
[1631]	train-auc:0.795241+0.00137519	test-auc:0.77906+0.00262815
[1632]	train-auc:0.795255+0.00137734	test-auc:0.779059+0.00262432
[1633]	train-auc:0.795263+0.00137839	test-auc:0.779055+0.00262433
[1634]	train-auc:0.795267+0.00137945	test-auc:0.77906+0.00261878
[1635]	train-auc:0.795275+0.00137928	test-auc:0.779056+0.00262057
[1636]	train-auc:0.795282+0.00137831	test-auc:0.779058+0.00262228
[1637]	train-auc:0.795293+0.0013792	test-auc:0.779054+0.00262024
[1638]	train-auc:0.795301+0.0013785	test-auc:0.779056+0.0026198
[1639]	train-auc:0.795309+0.0013826	test-auc:0.779053+0.00262707
[1640]	train-auc:0.795318+0.00138527	test-auc:0.779047+0.00262525
[1641]	train-auc:0.795329+0.00138322	test-auc:0.77905+0.00263309
[1642]	train-auc:0.795338+0.00138421	test-auc:0.779047+0.00263167
[1643]	train-auc:0.795343+0.00138411	test-auc:0.779043+0.00262898
[1644]	train-auc:0.795351+0.0013848	test-auc:0.77904+0.00262461
[1645]	train-auc:0.79536+0.00138538	test-auc:0.77904+0.0026243
[1646]	train-auc:0.795368+0.00138654	test-auc:0.77904+0.00263479
[1647]	train-auc:0.795373+0.00139018	test-auc:0.779032+0.00264123
[1648]	train-auc:0.795383+0.00138686	test-auc:0.779043+0.00263973
[1649]	train-auc:0.795391+0.00138594	test-auc:0.779043+0.00264336
[1650]	train-auc:0.795396+0.00138875	test-auc:0.779044+0.00264482
[1651]	train-auc:0.795404+0.0013905	test-auc:0.77904+0.00264134
[1652]	train-auc:0.795413+0.00138848	test-auc:0.779036+0.00264605
[1653]	train-auc:0.795417+0.0013893	test-auc:0.779028+0.00264485
[1654]	train-auc:0.795426+0.00139107	test-auc:0.779027+0.00264433
[1655]	train-auc:0.795433+0.00138868	test-auc:0.779032+0.00264092
[1656]	train-auc:0.795444+0.00139013	test-auc:0.77903+0.00264194
[1657]	train-auc:0.795452+0.0013916	test-auc:0.779033+0.00263275
[1658]	train-auc:0.795458+0.00139367	test-auc:0.779032+0.00264124
[1659]	train-auc:0.795467+0.00139535	test-auc:0.779031+0.00263891
[1660]	train-auc:0.795475+0.00139708	test-auc:0.779035+0.00263722
[1661]	train-auc:0.795482+0.00139945	test-auc:0.77904+0.00263162
[1662]	train-auc:0.795492+0.00139971	test-auc:0.779047+0.00263364
[1663]	train-auc:0.795501+0.00140244	test-auc:0.779041+0.0026378
[1664]	train-auc:0.79551+0.00140674	test-auc:0.779042+0.00264725
[1665]	train-auc:0.795518+0.00140498	test-auc:0.779044+0.002651
[1666]	train-auc:0.795532+0.00140594	test-auc:0.779046+0.00264605
[1667]	train-auc:0.79554+0.00140161	test-auc:0.779051+0.00265379
[1668]	train-auc:0.795547+0.00140594	test-auc:0.779056+0.00265186
[1669]	train-auc:0.795555+0.00141071	test-auc:0.779058+0.00265362
[1670]	train-auc:0.795569+0.00141467	test-auc:0.779053+0.0026496
[1671]	train-auc:0.795584+0.00141773	test-auc:0.779058+0.0026509
[1672]	train-auc:0.795589+0.00141926	test-auc:0.779061+0.0026489
[1673]	train-auc:0.795598+0.00142032	test-auc:0.779066+0.00265495
[1674]	train-auc:0.795603+0.00142012	test-auc:0.779063+0.00265387
[1675]	train-auc:0.795615+0.00141983	test-auc:0.779065+0.00265316
[1676]	train-auc:0.795623+0.00141555	test-auc:0.779069+0.00265059
[1677]	train-auc:0.795631+0.0014219	test-auc:0.77906+0.00265476
[1678]	train-auc:0.79564+0.00142575	test-auc:0.779069+0.00265239
[1679]	train-auc:0.79565+0.00143104	test-auc:0.779068+0.00264203
[1680]	train-auc:0.795659+0.00143342	test-auc:0.779077+0.00263978
[1681]	train-auc:0.795664+0.00143322	test-auc:0.779083+0.00264574
[1682]	train-auc:0.795673+0.00143155	test-auc:0.779085+0.00265315
[1683]	train-auc:0.795676+0.00143356	test-auc:0.779085+0.00265551
[1684]	train-auc:0.795684+0.00143228	test-auc:0.779083+0.00265643
[1685]	train-auc:0.795694+0.00143166	test-auc:0.779087+0.00266235
[1686]	train-auc:0.7957+0.00142748	test-auc:0.779085+0.00265759
[1687]	train-auc:0.795709+0.00142948	test-auc:0.779087+0.00266634
[1688]	train-auc:0.795715+0.00142765	test-auc:0.779091+0.00266335
[1689]	train-auc:0.795721+0.00142664	test-auc:0.779089+0.00265929
[1690]	train-auc:0.795731+0.0014258	test-auc:0.779088+0.00266029
[1691]	train-auc:0.795737+0.00143051	test-auc:0.779083+0.00266272
[1692]	train-auc:0.795745+0.00142949	test-auc:0.779086+0.00266171
[1693]	train-auc:0.795751+0.00142476	test-auc:0.77909+0.00265951
[1694]	train-auc:0.79576+0.00142563	test-auc:0.779093+0.00266083
[1695]	train-auc:0.795771+0.00142946	test-auc:0.7791+0.0026547
[1696]	train-auc:0.795776+0.00142828	test-auc:0.779102+0.002662
[1697]	train-auc:0.795784+0.00142997	test-auc:0.779098+0.00266226
[1698]	train-auc:0.795786+0.00143195	test-auc:0.779101+0.00266675
[1699]	train-auc:0.795794+0.00142756	test-auc:0.7791+0.00266503
[1700]	train-auc:0.795802+0.0014293	test-auc:0.779094+0.00266848
[1701]	train-auc:0.795811+0.00142844	test-auc:0.779093+0.00266538
[1702]	train-auc:0.795819+0.001432	test-auc:0.779093+0.00266511
[1703]	train-auc:0.795823+0.00143433	test-auc:0.779093+0.00266739
[1704]	train-auc:0.795836+0.00143272	test-auc:0.779088+0.00266433
[1705]	train-auc:0.795843+0.0014357	test-auc:0.779089+0.00266465
[1706]	train-auc:0.795855+0.00143664	test-auc:0.779091+0.00266942
[1707]	train-auc:0.795861+0.00143756	test-auc:0.779097+0.00266466
[1708]	train-auc:0.795873+0.00143599	test-auc:0.779104+0.00267031
[1709]	train-auc:0.795883+0.00143274	test-auc:0.779097+0.00267161
[1710]	train-auc:0.795889+0.00143522	test-auc:0.779099+0.00267192
[1711]	train-auc:0.795901+0.00142929	test-auc:0.7791+0.00266644
[1712]	train-auc:0.795906+0.00143133	test-auc:0.7791+0.00266991
[1713]	train-auc:0.795914+0.0014344	test-auc:0.779101+0.00267467
[1714]	train-auc:0.795924+0.00143547	test-auc:0.779102+0.00267306
[1715]	train-auc:0.795928+0.00143334	test-auc:0.779106+0.00267182
[1716]	train-auc:0.795936+0.00143262	test-auc:0.779102+0.00266597
[1717]	train-auc:0.795942+0.00142928	test-auc:0.779107+0.00266934
[1718]	train-auc:0.795951+0.00142549	test-auc:0.779099+0.00267099
[1719]	train-auc:0.795963+0.00142571	test-auc:0.779092+0.00266905
[1720]	train-auc:0.795968+0.00142601	test-auc:0.779091+0.00266847
[1721]	train-auc:0.795977+0.00142613	test-auc:0.779094+0.00266235
[1722]	train-auc:0.795983+0.00142714	test-auc:0.779097+0.00265703
[1723]	train-auc:0.795992+0.00142828	test-auc:0.779099+0.0026565
[1724]	train-auc:0.796002+0.00143092	test-auc:0.779094+0.00266282
[1725]	train-auc:0.796008+0.00142516	test-auc:0.779097+0.0026603
[1726]	train-auc:0.796011+0.00142485	test-auc:0.7791+0.00266026
[1727]	train-auc:0.796019+0.00142774	test-auc:0.779096+0.00266595
[1728]	train-auc:0.796024+0.00142584	test-auc:0.779093+0.0026602
[1729]	train-auc:0.796032+0.00142357	test-auc:0.779097+0.00267044
[1730]	train-auc:0.796041+0.00142517	test-auc:0.779104+0.00266567
[1731]	train-auc:0.79605+0.00141937	test-auc:0.779109+0.00266414
[1732]	train-auc:0.796058+0.00142083	test-auc:0.779105+0.00266169
[1733]	train-auc:0.796068+0.0014198	test-auc:0.779109+0.00266148
[1734]	train-auc:0.796074+0.00142166	test-auc:0.779109+0.00265793
[1735]	train-auc:0.796083+0.00142143	test-auc:0.779109+0.00266304
[1736]	train-auc:0.796089+0.00141631	test-auc:0.779119+0.00266501
[1737]	train-auc:0.796097+0.00141547	test-auc:0.779115+0.0026607
[1738]	train-auc:0.796104+0.0014238	test-auc:0.779108+0.00266569
[1739]	train-auc:0.796111+0.00142186	test-auc:0.779111+0.0026664
[1740]	train-auc:0.79612+0.00142168	test-auc:0.779112+0.00267327
[1741]	train-auc:0.79613+0.00142099	test-auc:0.779113+0.00268049
[1742]	train-auc:0.796133+0.00141938	test-auc:0.779116+0.0026817
[1743]	train-auc:0.796139+0.0014192	test-auc:0.779119+0.00269476
[1744]	train-auc:0.796144+0.00141787	test-auc:0.779118+0.00269695
[1745]	train-auc:0.796158+0.00142043	test-auc:0.779118+0.00269193
[1746]	train-auc:0.796163+0.00141839	test-auc:0.779118+0.00269326
[1747]	train-auc:0.796169+0.00141708	test-auc:0.779117+0.00269306
[1748]	train-auc:0.796178+0.00141873	test-auc:0.779121+0.00269541
[1749]	train-auc:0.796185+0.00141721	test-auc:0.77913+0.002698
[1750]	train-auc:0.796196+0.00141889	test-auc:0.779131+0.00270534
[1751]	train-auc:0.796203+0.00141851	test-auc:0.779129+0.00270332
[1752]	train-auc:0.796211+0.00141553	test-auc:0.779131+0.00271119
[1753]	train-auc:0.796217+0.00141651	test-auc:0.779129+0.00271412
[1754]	train-auc:0.796222+0.00141949	test-auc:0.779121+0.00271338
[1755]	train-auc:0.796233+0.00141727	test-auc:0.779121+0.00271164
[1756]	train-auc:0.796242+0.00141565	test-auc:0.779128+0.00270816
[1757]	train-auc:0.796248+0.00141594	test-auc:0.779137+0.00269774
[1758]	train-auc:0.796254+0.0014215	test-auc:0.779138+0.00269717
[1759]	train-auc:0.796264+0.00141924	test-auc:0.77914+0.00269723
[1760]	train-auc:0.796276+0.00141906	test-auc:0.779147+0.00269837
[1761]	train-auc:0.796284+0.00141778	test-auc:0.779149+0.00269651
[1762]	train-auc:0.796293+0.00141697	test-auc:0.779151+0.00269951
[1763]	train-auc:0.796301+0.00141315	test-auc:0.779154+0.00269981
[1764]	train-auc:0.796309+0.0014126	test-auc:0.779158+0.00270677
[1765]	train-auc:0.796309+0.00141302	test-auc:0.779155+0.00270923
[1766]	train-auc:0.796318+0.00141741	test-auc:0.779162+0.00270578
[1767]	train-auc:0.796327+0.00141729	test-auc:0.779162+0.00270745
[1768]	train-auc:0.796332+0.00141327	test-auc:0.779162+0.00270921
[1769]	train-auc:0.796339+0.00141573	test-auc:0.779159+0.00271234
[1770]	train-auc:0.796346+0.0014145	test-auc:0.779157+0.00271129
[1771]	train-auc:0.796361+0.00141638	test-auc:0.77916+0.00271603
[1772]	train-auc:0.79637+0.00142071	test-auc:0.779156+0.00272229
[1773]	train-auc:0.796378+0.00142187	test-auc:0.779155+0.00272238
[1774]	train-auc:0.796391+0.00142161	test-auc:0.77915+0.00273552
[1775]	train-auc:0.796399+0.00142487	test-auc:0.779149+0.00273286
[1776]	train-auc:0.796408+0.0014239	test-auc:0.779144+0.00273228
[1777]	train-auc:0.796419+0.00142248	test-auc:0.779149+0.00273555
[1778]	train-auc:0.796434+0.00142075	test-auc:0.779152+0.00274192
[1779]	train-auc:0.796445+0.00141845	test-auc:0.779152+0.00274328
[1780]	train-auc:0.796451+0.00141688	test-auc:0.77915+0.00274039
[1781]	train-auc:0.796458+0.00141473	test-auc:0.77915+0.00274037
[1782]	train-auc:0.796471+0.00141516	test-auc:0.779151+0.00275337
[1783]	train-auc:0.796478+0.00141569	test-auc:0.779148+0.00274713
[1784]	train-auc:0.796481+0.00141279	test-auc:0.779149+0.00275003
[1785]	train-auc:0.79649+0.00141192	test-auc:0.779148+0.0027505
[1786]	train-auc:0.796498+0.00141536	test-auc:0.779147+0.00274718
[1787]	train-auc:0.796511+0.00141852	test-auc:0.779143+0.00275097
[1788]	train-auc:0.796523+0.0014175	test-auc:0.779149+0.00275493
[1789]	train-auc:0.796535+0.00141662	test-auc:0.77915+0.00275233
[1790]	train-auc:0.796542+0.00141637	test-auc:0.77915+0.00274968
[1791]	train-auc:0.796552+0.00141446	test-auc:0.77915+0.00275607
[1792]	train-auc:0.79656+0.00141621	test-auc:0.77915+0.00275732
[1793]	train-auc:0.796571+0.00141374	test-auc:0.779151+0.0027571
[1794]	train-auc:0.79658+0.00141818	test-auc:0.779151+0.00275785
[1795]	train-auc:0.796591+0.00141907	test-auc:0.779153+0.00275931
[1796]	train-auc:0.796598+0.00141641	test-auc:0.779153+0.00276136
[1797]	train-auc:0.796606+0.00142008	test-auc:0.779154+0.00276456
[1798]	train-auc:0.796613+0.00142257	test-auc:0.779163+0.0027679
[1799]	train-auc:0.796618+0.00142289	test-auc:0.779165+0.00276728
[1800]	train-auc:0.796625+0.00142131	test-auc:0.779162+0.00276887
[1801]	train-auc:0.79663+0.00142599	test-auc:0.779157+0.0027713
[1802]	train-auc:0.796637+0.00142461	test-auc:0.779157+0.00277529
[1803]	train-auc:0.79665+0.00142312	test-auc:0.779162+0.00276994
[1804]	train-auc:0.796655+0.00142444	test-auc:0.779169+0.00276517
[1805]	train-auc:0.796664+0.00142787	test-auc:0.779172+0.0027623
[1806]	train-auc:0.796668+0.00142952	test-auc:0.779172+0.00276389
[1807]	train-auc:0.796676+0.00142788	test-auc:0.779169+0.00276426
[1808]	train-auc:0.796682+0.00142524	test-auc:0.779171+0.00275849
[1809]	train-auc:0.796692+0.00142686	test-auc:0.779185+0.00276112
[1810]	train-auc:0.796702+0.00142672	test-auc:0.779186+0.00276693
[1811]	train-auc:0.796714+0.00142724	test-auc:0.77919+0.0027657
[1812]	train-auc:0.796722+0.00142873	test-auc:0.779193+0.00277062
[1813]	train-auc:0.79673+0.00142771	test-auc:0.779191+0.00276886
[1814]	train-auc:0.796742+0.00142805	test-auc:0.779193+0.00277187
[1815]	train-auc:0.796749+0.00142539	test-auc:0.779195+0.00277679
[1816]	train-auc:0.796754+0.00142483	test-auc:0.779193+0.00278762
[1817]	train-auc:0.796761+0.00142823	test-auc:0.779187+0.0027929
[1818]	train-auc:0.796772+0.00142704	test-auc:0.779188+0.00279791
[1819]	train-auc:0.796774+0.00142981	test-auc:0.779188+0.00279717
[1820]	train-auc:0.796784+0.00143407	test-auc:0.779196+0.0027901
[1821]	train-auc:0.79679+0.00143448	test-auc:0.779195+0.00278793
[1822]	train-auc:0.796797+0.00143465	test-auc:0.779202+0.00278672
[1823]	train-auc:0.796808+0.00143591	test-auc:0.779204+0.00278714
[1824]	train-auc:0.796814+0.00143404	test-auc:0.779208+0.00278739
[1825]	train-auc:0.796824+0.00143527	test-auc:0.77921+0.00278743
[1826]	train-auc:0.796834+0.00143472	test-auc:0.779216+0.00279054
[1827]	train-auc:0.796842+0.00143554	test-auc:0.779218+0.00279269
[1828]	train-auc:0.79685+0.00143482	test-auc:0.779212+0.00278792
[1829]	train-auc:0.796858+0.00143949	test-auc:0.779216+0.00278638
[1830]	train-auc:0.796867+0.00143875	test-auc:0.779218+0.00278402
[1831]	train-auc:0.796877+0.00143702	test-auc:0.779219+0.00278575
[1832]	train-auc:0.796883+0.00143449	test-auc:0.779221+0.00278814
[1833]	train-auc:0.79689+0.00143341	test-auc:0.77922+0.00279053
[1834]	train-auc:0.796897+0.00143357	test-auc:0.779218+0.00278905
[1835]	train-auc:0.796904+0.001436	test-auc:0.779218+0.00279374
[1836]	train-auc:0.796914+0.0014359	test-auc:0.779215+0.00279612
[1837]	train-auc:0.796923+0.00143264	test-auc:0.779219+0.00279882
[1838]	train-auc:0.796934+0.00143708	test-auc:0.77922+0.00280582
[1839]	train-auc:0.796945+0.00143658	test-auc:0.779224+0.00280397
[1840]	train-auc:0.796956+0.00143755	test-auc:0.77923+0.00279988
[1841]	train-auc:0.79696+0.00144016	test-auc:0.779227+0.00281147
[1842]	train-auc:0.796971+0.00143822	test-auc:0.779227+0.00280973
[1843]	train-auc:0.796975+0.00143568	test-auc:0.779222+0.002806
[1844]	train-auc:0.796985+0.00143826	test-auc:0.779228+0.00280631
[1845]	train-auc:0.796995+0.00144379	test-auc:0.779227+0.00281062
[1846]	train-auc:0.797001+0.00144019	test-auc:0.779228+0.00281591
[1847]	train-auc:0.797003+0.00144182	test-auc:0.779234+0.0028148
[1848]	train-auc:0.797006+0.00144421	test-auc:0.779233+0.00282013
[1849]	train-auc:0.797014+0.00144271	test-auc:0.779234+0.00281958
[1850]	train-auc:0.797024+0.00144168	test-auc:0.779238+0.00281318
[1851]	train-auc:0.797031+0.0014433	test-auc:0.779231+0.00282116
[1852]	train-auc:0.79704+0.00143873	test-auc:0.779235+0.00282744
[1853]	train-auc:0.797046+0.00143943	test-auc:0.779236+0.00282526
[1854]	train-auc:0.797055+0.00144384	test-auc:0.779231+0.00282857
[1855]	train-auc:0.797062+0.00144831	test-auc:0.779235+0.00282879
[1856]	train-auc:0.797069+0.00145016	test-auc:0.779233+0.00283028
[1857]	train-auc:0.797074+0.00145014	test-auc:0.779231+0.00282754
[1858]	train-auc:0.797086+0.00144811	test-auc:0.779235+0.00282892
[1859]	train-auc:0.797094+0.00145029	test-auc:0.779241+0.0028344
[1860]	train-auc:0.797101+0.00144868	test-auc:0.779244+0.00282857
[1861]	train-auc:0.797105+0.0014481	test-auc:0.779242+0.00283261
[1862]	train-auc:0.797111+0.00144631	test-auc:0.779243+0.00283026
[1863]	train-auc:0.797115+0.00144503	test-auc:0.779249+0.00283174
[1864]	train-auc:0.797123+0.00144641	test-auc:0.779248+0.0028319
[1865]	train-auc:0.797134+0.00144292	test-auc:0.779255+0.00284036
[1866]	train-auc:0.797142+0.00144025	test-auc:0.779259+0.00284171
[1867]	train-auc:0.797149+0.00143986	test-auc:0.779261+0.0028365
[1868]	train-auc:0.797161+0.00144407	test-auc:0.779265+0.00284203
[1869]	train-auc:0.797174+0.00144647	test-auc:0.779276+0.0028295
[1870]	train-auc:0.797181+0.00144878	test-auc:0.779275+0.00283394
[1871]	train-auc:0.797193+0.00144656	test-auc:0.779278+0.00284168
[1872]	train-auc:0.797202+0.00144413	test-auc:0.779282+0.00284127
[1873]	train-auc:0.797209+0.00144096	test-auc:0.779279+0.00284028
[1874]	train-auc:0.797221+0.00144143	test-auc:0.779283+0.00284464
[1875]	train-auc:0.79723+0.0014414	test-auc:0.779283+0.00284611
[1876]	train-auc:0.797239+0.00143859	test-auc:0.779278+0.00285091
[1877]	train-auc:0.797244+0.00143972	test-auc:0.779283+0.00285183
[1878]	train-auc:0.797249+0.0014357	test-auc:0.779285+0.00285686
[1879]	train-auc:0.797258+0.00143701	test-auc:0.779286+0.00285913
[1880]	train-auc:0.797265+0.00143963	test-auc:0.779284+0.00285926
[1881]	train-auc:0.797276+0.00143937	test-auc:0.779295+0.0028605
[1882]	train-auc:0.79728+0.00144033	test-auc:0.779293+0.00286368
[1883]	train-auc:0.797288+0.00143981	test-auc:0.77929+0.00286298
[1884]	train-auc:0.797295+0.00144256	test-auc:0.779285+0.00286588
[1885]	train-auc:0.797302+0.00143966	test-auc:0.779288+0.00286048
[1886]	train-auc:0.797312+0.00144122	test-auc:0.779283+0.00286091
[1887]	train-auc:0.797319+0.00144418	test-auc:0.779277+0.00286219
[1888]	train-auc:0.797326+0.00144113	test-auc:0.779279+0.00286675
[1889]	train-auc:0.797335+0.00144275	test-auc:0.779282+0.00286998
[1890]	train-auc:0.797343+0.00144498	test-auc:0.779283+0.00286727
[1891]	train-auc:0.797353+0.00144382	test-auc:0.779282+0.00287794
[1892]	train-auc:0.797361+0.00144508	test-auc:0.779283+0.00288056
[1893]	train-auc:0.797366+0.0014421	test-auc:0.779284+0.00287368
[1894]	train-auc:0.797373+0.00144269	test-auc:0.779291+0.00286848
[1895]	train-auc:0.797379+0.00143947	test-auc:0.779292+0.00286983
[1896]	train-auc:0.797387+0.00144144	test-auc:0.779298+0.00287442
[1897]	train-auc:0.797396+0.00143878	test-auc:0.779304+0.00287226
[1898]	train-auc:0.797405+0.00143574	test-auc:0.779301+0.00286767
[1899]	train-auc:0.797415+0.00143845	test-auc:0.779302+0.00287014
[1900]	train-auc:0.797424+0.00143309	test-auc:0.779309+0.00286179
[1901]	train-auc:0.797433+0.00143216	test-auc:0.779308+0.00286266
[1902]	train-auc:0.79744+0.00143034	test-auc:0.779307+0.00286729
[1903]	train-auc:0.797449+0.00142997	test-auc:0.779306+0.00286317
[1904]	train-auc:0.797457+0.00143226	test-auc:0.77931+0.00286244
[1905]	train-auc:0.797468+0.00142942	test-auc:0.779317+0.00286857
[1906]	train-auc:0.797472+0.00143307	test-auc:0.779318+0.00286792
[1907]	train-auc:0.79748+0.00142736	test-auc:0.779322+0.00287476
[1908]	train-auc:0.797487+0.0014258	test-auc:0.779327+0.00287721
[1909]	train-auc:0.797494+0.00142538	test-auc:0.779327+0.00287269
[1910]	train-auc:0.7975+0.00142673	test-auc:0.779328+0.00287006
[1911]	train-auc:0.797507+0.00142829	test-auc:0.779325+0.00287269
[1912]	train-auc:0.79752+0.00142623	test-auc:0.779323+0.00287324
[1913]	train-auc:0.797526+0.00142435	test-auc:0.779322+0.00287203
[1914]	train-auc:0.79753+0.00142492	test-auc:0.779324+0.00287504
[1915]	train-auc:0.797538+0.00142541	test-auc:0.779319+0.00287335
[1916]	train-auc:0.797552+0.00142505	test-auc:0.779311+0.00287824
[1917]	train-auc:0.797562+0.00142809	test-auc:0.779306+0.00288413
[1918]	train-auc:0.797573+0.00142513	test-auc:0.779303+0.00288492
[1919]	train-auc:0.79758+0.00142846	test-auc:0.779305+0.00288043
[1920]	train-auc:0.797588+0.00142364	test-auc:0.7793+0.00288134
[1921]	train-auc:0.797598+0.00143052	test-auc:0.779304+0.00288332
[1922]	train-auc:0.797606+0.00143035	test-auc:0.779301+0.00288858
[1923]	train-auc:0.797614+0.00142442	test-auc:0.779306+0.0028885
[1924]	train-auc:0.79762+0.00142291	test-auc:0.779305+0.00289078
[1925]	train-auc:0.797629+0.00142456	test-auc:0.779302+0.00288547
[1926]	train-auc:0.797635+0.00142161	test-auc:0.779308+0.00288689
[1927]	train-auc:0.797646+0.00142015	test-auc:0.779308+0.00288775
[1928]	train-auc:0.79765+0.00141953	test-auc:0.77931+0.0028866
[1929]	train-auc:0.797659+0.00141982	test-auc:0.779312+0.00288673
[1930]	train-auc:0.797671+0.00141959	test-auc:0.779316+0.00288558
[1931]	train-auc:0.797681+0.00142471	test-auc:0.779315+0.00288449
[1932]	train-auc:0.797691+0.00142595	test-auc:0.779317+0.00288978
[1933]	train-auc:0.797704+0.00143193	test-auc:0.779317+0.0028976
[1934]	train-auc:0.797718+0.0014323	test-auc:0.779317+0.00288958
[1935]	train-auc:0.797724+0.0014337	test-auc:0.779319+0.00289427
[1936]	train-auc:0.797731+0.00143233	test-auc:0.779323+0.00289241
[1937]	train-auc:0.797736+0.00142979	test-auc:0.779325+0.00289727
[1938]	train-auc:0.797741+0.00143016	test-auc:0.779325+0.00289781
[1939]	train-auc:0.797746+0.00143182	test-auc:0.779321+0.00290277
[1940]	train-auc:0.797752+0.00143051	test-auc:0.779317+0.00290272
[1941]	train-auc:0.797759+0.00142942	test-auc:0.779319+0.00290404
[1942]	train-auc:0.797766+0.00142567	test-auc:0.779315+0.00290935
[1943]	train-auc:0.797776+0.0014266	test-auc:0.779315+0.00291146
[1944]	train-auc:0.797785+0.00142641	test-auc:0.779319+0.00291021
[1945]	train-auc:0.797789+0.00142835	test-auc:0.779329+0.00290193
[1946]	train-auc:0.797795+0.00142768	test-auc:0.779336+0.00290158
[1947]	train-auc:0.797806+0.00142942	test-auc:0.77933+0.00289981
[1948]	train-auc:0.797816+0.00142817	test-auc:0.779336+0.00290862
[1949]	train-auc:0.797822+0.00142651	test-auc:0.779328+0.00290868
[1950]	train-auc:0.797827+0.00142794	test-auc:0.779328+0.00290637
[1951]	train-auc:0.797834+0.00142976	test-auc:0.779328+0.00290622
[1952]	train-auc:0.797838+0.00143283	test-auc:0.779331+0.00291156
[1953]	train-auc:0.797843+0.00143244	test-auc:0.779335+0.0029163
[1954]	train-auc:0.797848+0.00143603	test-auc:0.779337+0.00291299
[1955]	train-auc:0.797855+0.00143618	test-auc:0.779339+0.002917
[1956]	train-auc:0.797861+0.00144259	test-auc:0.779337+0.00291554
[1957]	train-auc:0.797863+0.00144425	test-auc:0.779339+0.00291777
[1958]	train-auc:0.797869+0.00144288	test-auc:0.779341+0.00292065
[1959]	train-auc:0.797869+0.00144567	test-auc:0.779337+0.00292021
[1960]	train-auc:0.797879+0.00144495	test-auc:0.77934+0.00292276
[1961]	train-auc:0.797886+0.00144738	test-auc:0.779343+0.00292317
[1962]	train-auc:0.797889+0.0014484	test-auc:0.779345+0.0029233
[1963]	train-auc:0.797897+0.00144698	test-auc:0.779346+0.00292799
[1964]	train-auc:0.797903+0.00144646	test-auc:0.779347+0.00293491
[1965]	train-auc:0.797913+0.00144512	test-auc:0.779351+0.00293706
[1966]	train-auc:0.797924+0.00144467	test-auc:0.779352+0.00293562
[1967]	train-auc:0.797937+0.00144307	test-auc:0.779354+0.00294577
[1968]	train-auc:0.797945+0.0014474	test-auc:0.779351+0.00293971
[1969]	train-auc:0.797951+0.00144912	test-auc:0.779347+0.00294047
[1970]	train-auc:0.797964+0.00144974	test-auc:0.779341+0.00294352
[1971]	train-auc:0.797973+0.00144355	test-auc:0.77934+0.00295067
[1972]	train-auc:0.797981+0.00144504	test-auc:0.77935+0.00295251
[1973]	train-auc:0.797993+0.00144696	test-auc:0.779351+0.0029522
[1974]	train-auc:0.798001+0.00144597	test-auc:0.779351+0.00295189
[1975]	train-auc:0.79801+0.00144856	test-auc:0.779355+0.00294962
[1976]	train-auc:0.798016+0.00144565	test-auc:0.779362+0.00295468
[1977]	train-auc:0.79802+0.00144862	test-auc:0.779358+0.00295125
[1978]	train-auc:0.79803+0.00144584	test-auc:0.779359+0.00295821
[1979]	train-auc:0.798036+0.00144503	test-auc:0.77936+0.00295986
[1980]	train-auc:0.798045+0.00144874	test-auc:0.779356+0.0029562
[1981]	train-auc:0.79805+0.00145186	test-auc:0.77936+0.00295487
[1982]	train-auc:0.798057+0.00145224	test-auc:0.779363+0.00295236
[1983]	train-auc:0.798062+0.00145526	test-auc:0.779356+0.00295299
[1984]	train-auc:0.798069+0.00145514	test-auc:0.779359+0.00295928
[1985]	train-auc:0.798077+0.00145785	test-auc:0.779359+0.00296065
[1986]	train-auc:0.798087+0.00145706	test-auc:0.779358+0.00297019
[1987]	train-auc:0.798096+0.00146236	test-auc:0.779362+0.00296579
[1988]	train-auc:0.798103+0.00146081	test-auc:0.779363+0.00296566
[1989]	train-auc:0.79811+0.00146513	test-auc:0.779363+0.00297059
[1990]	train-auc:0.798115+0.00146384	test-auc:0.779356+0.00296655
[1991]	train-auc:0.798125+0.00146806	test-auc:0.779361+0.0029674
[1992]	train-auc:0.798134+0.0014694	test-auc:0.779361+0.00297167
[1993]	train-auc:0.798141+0.00146739	test-auc:0.77936+0.00297295
[1994]	train-auc:0.79815+0.00146622	test-auc:0.779361+0.00297262
[1995]	train-auc:0.79816+0.00146595	test-auc:0.779356+0.00298088
[1996]	train-auc:0.798165+0.00146723	test-auc:0.779352+0.00298451
[1997]	train-auc:0.798169+0.00146817	test-auc:0.779353+0.00298393
[1998]	train-auc:0.798178+0.00147191	test-auc:0.77935+0.00298492
[1999]	train-auc:0.798186+0.00147355	test-auc:0.77935+0.00297961
[2000]	train-auc:0.798195+0.00147374	test-auc:0.779352+0.00297868
[2001]	train-auc:0.798202+0.00147619	test-auc:0.779352+0.00297774
[2002]	train-auc:0.798212+0.00147173	test-auc:0.779351+0.00297807
[2003]	train-auc:0.798217+0.00147047	test-auc:0.779342+0.00297536
[2004]	train-auc:0.798224+0.00146843	test-auc:0.779346+0.00297187
[2005]	train-auc:0.798234+0.0014675	test-auc:0.77935+0.00297618
[2006]	train-auc:0.798242+0.00146552	test-auc:0.779353+0.00297067
[2007]	train-auc:0.798254+0.00146752	test-auc:0.779344+0.00297501
[2008]	train-auc:0.798263+0.00146575	test-auc:0.779339+0.00297249
[2009]	train-auc:0.798269+0.00146104	test-auc:0.779343+0.00297756
[2010]	train-auc:0.79828+0.00145674	test-auc:0.779351+0.00298474
[2011]	train-auc:0.798288+0.00145955	test-auc:0.77936+0.00297947
[2012]	train-auc:0.798296+0.00145914	test-auc:0.779361+0.00297831
[2013]	train-auc:0.798304+0.00146196	test-auc:0.779359+0.00297579
[2014]	train-auc:0.798309+0.00145799	test-auc:0.779367+0.00296705
[2015]	train-auc:0.798318+0.00146328	test-auc:0.779367+0.00297004
[2016]	train-auc:0.798325+0.00146406	test-auc:0.779366+0.00297185
[2017]	train-auc:0.798332+0.00146561	test-auc:0.779362+0.00297165
[2018]	train-auc:0.798337+0.00146455	test-auc:0.779357+0.00296504
[2019]	train-auc:0.798341+0.00146681	test-auc:0.779358+0.00296476
[2020]	train-auc:0.798343+0.00146623	test-auc:0.779352+0.00296626
[2021]	train-auc:0.798349+0.00146871	test-auc:0.779347+0.00297101
[2022]	train-auc:0.798357+0.0014686	test-auc:0.779346+0.00296919
[2023]	train-auc:0.798362+0.00146793	test-auc:0.779349+0.00296795
[2024]	train-auc:0.798369+0.00146489	test-auc:0.77935+0.00296829
[2025]	train-auc:0.798382+0.00146533	test-auc:0.779352+0.00296722
[2026]	train-auc:0.798393+0.00146696	test-auc:0.779352+0.00296482
[2027]	train-auc:0.7984+0.00146632	test-auc:0.779347+0.00296282
[2028]	train-auc:0.798407+0.00146454	test-auc:0.779352+0.00295785
[2029]	train-auc:0.79841+0.00146659	test-auc:0.779355+0.00295619
[2030]	train-auc:0.79842+0.00146831	test-auc:0.77935+0.00295813
[2031]	train-auc:0.798426+0.00146653	test-auc:0.779354+0.002958
[2032]	train-auc:0.79843+0.00146615	test-auc:0.779354+0.00295563
[2033]	train-auc:0.79844+0.00146784	test-auc:0.779362+0.00295169
[2034]	train-auc:0.798452+0.00147159	test-auc:0.779365+0.0029503
[2035]	train-auc:0.798461+0.00147212	test-auc:0.779362+0.00295824
[2036]	train-auc:0.798472+0.00147286	test-auc:0.779356+0.00295812
[2037]	train-auc:0.798477+0.00146913	test-auc:0.779357+0.00295667
[2038]	train-auc:0.798485+0.00146917	test-auc:0.779357+0.00296185
[2039]	train-auc:0.798497+0.00146949	test-auc:0.779363+0.00297481
[2040]	train-auc:0.798501+0.00147334	test-auc:0.779365+0.00297459
[2041]	train-auc:0.798506+0.00147335	test-auc:0.779373+0.00297194
[2042]	train-auc:0.798513+0.00147529	test-auc:0.779372+0.00297085
[2043]	train-auc:0.798523+0.00147843	test-auc:0.779373+0.0029699
[2044]	train-auc:0.798526+0.00147722	test-auc:0.779369+0.00296919
[2045]	train-auc:0.798531+0.00148078	test-auc:0.779363+0.00297584
[2046]	train-auc:0.798539+0.00147534	test-auc:0.779364+0.00298265
[2047]	train-auc:0.798546+0.00147304	test-auc:0.779364+0.0029815
[2048]	train-auc:0.798558+0.00147621	test-auc:0.77936+0.00298311
[2049]	train-auc:0.798563+0.00148015	test-auc:0.779354+0.00298321
[2050]	train-auc:0.79857+0.00148057	test-auc:0.779365+0.00298269
[2051]	train-auc:0.798581+0.00147784	test-auc:0.779362+0.00298996
[2052]	train-auc:0.79859+0.00147569	test-auc:0.779362+0.00299593
[2053]	train-auc:0.798597+0.00148144	test-auc:0.779359+0.00300287
[2054]	train-auc:0.798603+0.00147955	test-auc:0.779359+0.0030085
[2055]	train-auc:0.79861+0.00148348	test-auc:0.77936+0.00300964
[2056]	train-auc:0.798616+0.00148646	test-auc:0.779359+0.00301166
[2057]	train-auc:0.798626+0.00148354	test-auc:0.779358+0.00301562
[2058]	train-auc:0.798634+0.00148569	test-auc:0.779356+0.00302016
[2059]	train-auc:0.798643+0.0014859	test-auc:0.779355+0.0030197
[2060]	train-auc:0.798649+0.00148662	test-auc:0.779358+0.0030209
[2061]	train-auc:0.798658+0.00148485	test-auc:0.779357+0.00302829
[2062]	train-auc:0.798669+0.00148763	test-auc:0.779357+0.00303253
[2063]	train-auc:0.798679+0.00148862	test-auc:0.779355+0.00303215
[2064]	train-auc:0.798687+0.00148439	test-auc:0.779354+0.00302787
[2065]	train-auc:0.798691+0.00148615	test-auc:0.779355+0.00302326
[2066]	train-auc:0.798699+0.00148623	test-auc:0.779348+0.00301933
[2067]	train-auc:0.798709+0.00148604	test-auc:0.779348+0.00302899
[2068]	train-auc:0.798713+0.00148519	test-auc:0.779348+0.00302739
[2069]	train-auc:0.79872+0.00148261	test-auc:0.779343+0.0030298
[2070]	train-auc:0.798724+0.00148389	test-auc:0.779347+0.00303145
[2071]	train-auc:0.798731+0.0014825	test-auc:0.779348+0.00303264
[2072]	train-auc:0.79874+0.00148069	test-auc:0.779352+0.00303233
[2073]	train-auc:0.79875+0.00147933	test-auc:0.779354+0.00303393
[2074]	train-auc:0.798756+0.00148242	test-auc:0.779353+0.00304596
[2075]	train-auc:0.798766+0.00147806	test-auc:0.779353+0.00304959
[2076]	train-auc:0.798772+0.00147437	test-auc:0.779362+0.00304823
[2077]	train-auc:0.798785+0.00147846	test-auc:0.779364+0.00305761
[2078]	train-auc:0.798794+0.0014804	test-auc:0.779363+0.00306209
[2079]	train-auc:0.798804+0.00147661	test-auc:0.779363+0.00305797
[2080]	train-auc:0.798807+0.0014757	test-auc:0.779361+0.00305534
[2081]	train-auc:0.798814+0.00147476	test-auc:0.779363+0.00306472
[2082]	train-auc:0.798825+0.0014792	test-auc:0.779363+0.0030717
[2083]	train-auc:0.798832+0.00147774	test-auc:0.779373+0.00306726
[2084]	train-auc:0.798842+0.00147887	test-auc:0.779369+0.00306687
[2085]	train-auc:0.798846+0.00147616	test-auc:0.779369+0.00306878
[2086]	train-auc:0.798854+0.0014738	test-auc:0.779375+0.00307105
[2087]	train-auc:0.798863+0.00147276	test-auc:0.779376+0.00307468
[2088]	train-auc:0.79887+0.0014677	test-auc:0.779381+0.00308373
[2089]	train-auc:0.798881+0.00147111	test-auc:0.779383+0.00308841
[2090]	train-auc:0.798887+0.00147564	test-auc:0.779382+0.00309476
[2091]	train-auc:0.798895+0.00147685	test-auc:0.779383+0.0030922
[2092]	train-auc:0.798903+0.00147631	test-auc:0.779389+0.00309485
[2093]	train-auc:0.798911+0.00147782	test-auc:0.779388+0.00310147
[2094]	train-auc:0.798916+0.0014797	test-auc:0.779383+0.00310398
[2095]	train-auc:0.798924+0.0014783	test-auc:0.779385+0.00310393
[2096]	train-auc:0.798933+0.00148133	test-auc:0.77938+0.00310155
[2097]	train-auc:0.79894+0.00147962	test-auc:0.779378+0.00310671
[2098]	train-auc:0.79895+0.0014811	test-auc:0.779381+0.00310758
[2099]	train-auc:0.798961+0.00147852	test-auc:0.779384+0.00311184
[2100]	train-auc:0.798965+0.00147851	test-auc:0.779379+0.00311367
[2101]	train-auc:0.798973+0.00148026	test-auc:0.779384+0.00311472
[2102]	train-auc:0.798979+0.00147622	test-auc:0.779386+0.00311315
[2103]	train-auc:0.79899+0.00147292	test-auc:0.779392+0.00311225
[2104]	train-auc:0.798999+0.00147304	test-auc:0.779396+0.00311368
[2105]	train-auc:0.799002+0.00147323	test-auc:0.779399+0.00311971
[2106]	train-auc:0.799009+0.00147675	test-auc:0.779392+0.00311834
[2107]	train-auc:0.799019+0.00147929	test-auc:0.779398+0.00312438
[2108]	train-auc:0.79903+0.00148025	test-auc:0.779392+0.00312122
[2109]	train-auc:0.799036+0.00147817	test-auc:0.779393+0.00312844
[2110]	train-auc:0.799039+0.00148095	test-auc:0.779392+0.00312586
[2111]	train-auc:0.799048+0.00148059	test-auc:0.779394+0.00313279
[2112]	train-auc:0.799055+0.00147675	test-auc:0.779391+0.00313841
[2113]	train-auc:0.799063+0.00147565	test-auc:0.779386+0.00313903
[2114]	train-auc:0.799073+0.00147295	test-auc:0.779383+0.0031359
[2115]	train-auc:0.79908+0.00147597	test-auc:0.779385+0.00313413
[2116]	train-auc:0.79909+0.00147634	test-auc:0.779376+0.00313147
[2117]	train-auc:0.799101+0.0014786	test-auc:0.779376+0.00313409
[2118]	train-auc:0.79911+0.00147723	test-auc:0.779379+0.00313465
[2119]	train-auc:0.799119+0.00147767	test-auc:0.779386+0.00312491
[2120]	train-auc:0.799126+0.00147444	test-auc:0.779386+0.00312069
[2121]	train-auc:0.799139+0.00147868	test-auc:0.779388+0.00311648
[2122]	train-auc:0.799148+0.00147391	test-auc:0.779393+0.00311773
[2123]	train-auc:0.799155+0.00147312	test-auc:0.779391+0.00311525
[2124]	train-auc:0.799163+0.00147264	test-auc:0.779392+0.00311538
[2125]	train-auc:0.79917+0.0014714	test-auc:0.779391+0.00311708
[2126]	train-auc:0.799178+0.00147205	test-auc:0.779393+0.00311517
[2127]	train-auc:0.799183+0.0014705	test-auc:0.779394+0.00311543
[2128]	train-auc:0.799193+0.00147624	test-auc:0.779394+0.00311826
[2129]	train-auc:0.7992+0.00147776	test-auc:0.779391+0.00312244
[2130]	train-auc:0.799208+0.00147848	test-auc:0.779392+0.00312721
[2131]	train-auc:0.799211+0.00147977	test-auc:0.779398+0.00312487
[2132]	train-auc:0.799223+0.00147998	test-auc:0.779395+0.00312634
[2133]	train-auc:0.799228+0.00147948	test-auc:0.779388+0.00312833
[2134]	train-auc:0.799239+0.00148208	test-auc:0.779388+0.00313157
[2135]	train-auc:0.799247+0.00148439	test-auc:0.779386+0.00313054
[2136]	train-auc:0.799255+0.00148488	test-auc:0.779386+0.00312565
[2137]	train-auc:0.799262+0.00148824	test-auc:0.779385+0.00312477
[2138]	train-auc:0.799268+0.00149226	test-auc:0.779386+0.0031259
[2139]	train-auc:0.799273+0.001493	test-auc:0.779387+0.00312387
[2140]	train-auc:0.799284+0.00149461	test-auc:0.779389+0.00312582
[2141]	train-auc:0.799295+0.00149233	test-auc:0.779389+0.00313285
[2142]	train-auc:0.7993+0.00149351	test-auc:0.779391+0.00313323
[2143]	train-auc:0.799308+0.00149212	test-auc:0.779396+0.00313701
[2144]	train-auc:0.799316+0.00149287	test-auc:0.779399+0.0031449
[2145]	train-auc:0.799318+0.00149167	test-auc:0.779403+0.00314492
[2146]	train-auc:0.799328+0.00149648	test-auc:0.779403+0.00314453
[2147]	train-auc:0.799331+0.00149578	test-auc:0.779401+0.00314758
[2148]	train-auc:0.799339+0.00149876	test-auc:0.779399+0.00315035
[2149]	train-auc:0.799345+0.00149665	test-auc:0.7794+0.00315113
[2150]	train-auc:0.799352+0.00149729	test-auc:0.779395+0.00315223
[2151]	train-auc:0.799359+0.00150306	test-auc:0.779396+0.0031476
[2152]	train-auc:0.799369+0.00150175	test-auc:0.779398+0.00314652
[2153]	train-auc:0.799377+0.00150203	test-auc:0.77939+0.00314455
[2154]	train-auc:0.799384+0.00149601	test-auc:0.779396+0.0031509
[2155]	train-auc:0.799391+0.00149965	test-auc:0.779401+0.00315069
[2156]	train-auc:0.799399+0.00150159	test-auc:0.779395+0.00315445
[2157]	train-auc:0.799407+0.0015007	test-auc:0.779397+0.00315572
[2158]	train-auc:0.799417+0.0014995	test-auc:0.779395+0.00315624
[2159]	train-auc:0.799425+0.00149964	test-auc:0.779395+0.00315778
[2160]	train-auc:0.799433+0.00149547	test-auc:0.779392+0.00315945
[2161]	train-auc:0.799445+0.00149815	test-auc:0.77939+0.00316145
[2162]	train-auc:0.799449+0.00149699	test-auc:0.779392+0.00316071
[2163]	train-auc:0.799457+0.00149664	test-auc:0.779388+0.00316586
[2164]	train-auc:0.799464+0.00149662	test-auc:0.779385+0.00315972
[2165]	train-auc:0.799469+0.00149521	test-auc:0.779389+0.00315941
[2166]	train-auc:0.79948+0.00149464	test-auc:0.779385+0.00315639
[2167]	train-auc:0.799489+0.00149486	test-auc:0.779384+0.00316362
[2168]	train-auc:0.799497+0.00149414	test-auc:0.779377+0.00316309
[2169]	train-auc:0.799508+0.00149376	test-auc:0.779386+0.00315857
[2170]	train-auc:0.799516+0.00149868	test-auc:0.779385+0.00316087
[2171]	train-auc:0.799523+0.00149969	test-auc:0.779386+0.00316681
[2172]	train-auc:0.799527+0.0015001	test-auc:0.779387+0.00316469
[2173]	train-auc:0.799537+0.00149765	test-auc:0.779386+0.00316891
[2174]	train-auc:0.799549+0.00150254	test-auc:0.779384+0.0031724
[2175]	train-auc:0.799558+0.00150248	test-auc:0.779382+0.00317488
[2176]	train-auc:0.799568+0.00150038	test-auc:0.779381+0.00317381
[2177]	train-auc:0.799581+0.00149935	test-auc:0.779386+0.00317194
[2178]	train-auc:0.799587+0.00149377	test-auc:0.779387+0.00316013
[2179]	train-auc:0.799594+0.00149447	test-auc:0.779383+0.00315763
[2180]	train-auc:0.799603+0.00149266	test-auc:0.779387+0.00315926
[2181]	train-auc:0.799611+0.00149059	test-auc:0.779385+0.00315999
[2182]	train-auc:0.799616+0.00149231	test-auc:0.779379+0.00316244
[2183]	train-auc:0.799622+0.00149265	test-auc:0.779384+0.00316136
[2184]	train-auc:0.799629+0.00149371	test-auc:0.779387+0.00316583
[2185]	train-auc:0.799638+0.00149607	test-auc:0.77939+0.00316757
[2186]	train-auc:0.799642+0.00149666	test-auc:0.779385+0.00316838
[2187]	train-auc:0.799644+0.00149713	test-auc:0.77938+0.0031728
[2188]	train-auc:0.799651+0.00149999	test-auc:0.779385+0.00316837
[2189]	train-auc:0.799658+0.00149696	test-auc:0.779381+0.00316674
[2190]	train-auc:0.799663+0.00149404	test-auc:0.779381+0.00317488
[2191]	train-auc:0.799674+0.00149154	test-auc:0.779382+0.0031738
[2192]	train-auc:0.799682+0.00149062	test-auc:0.779382+0.00317627
[2193]	train-auc:0.799689+0.00149478	test-auc:0.779384+0.00317706
[2194]	train-auc:0.799694+0.00149448	test-auc:0.779383+0.00317386
[2195]	train-auc:0.799698+0.00149475	test-auc:0.779374+0.00317194

Model Report
 - train_acc : 0.8218
 - train auc score: 0.796587
 - text_acc : 0.8327
 - text auc score: 0.805928